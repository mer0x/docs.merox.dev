{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/","title":"About","text":""},{"location":"about/#robert-melcher","title":"Robert Melcher","text":"<p>Welcome! I am a seasoned System Administrator and Cybersecurity Engineer, with a profound passion for Linux, Networking, and Security. My career has been a journey through challenging roles, where I've developed a blend of technical prowess and strategic insight.</p>"},{"location":"about/#professional-journey","title":"Professional Journey","text":"<p>My experience spans across esteemed organizations such as Hella, Atos, and Netex, where I've embraced roles that pushed the boundaries of IT infrastructure, cybersecurity, and system administration. Highlights of my career include:</p> <ul> <li>Spearheading cybersecurity initiatives, enhancing system security, and resilience.</li> <li>Leading server administration tasks, optimizing performance, and ensuring reliability.</li> <li>Developing and implementing IT strategies that align with organizational goals.</li> </ul>"},{"location":"about/#core-competencies","title":"Core Competencies","text":"<ul> <li>Linux Systems: In-depth knowledge and hands-on experience with Linux-based environments.</li> <li>Networking: Expertise in designing and maintaining robust network infrastructures.</li> <li>Security: A strong foundation in cybersecurity principles and practices.</li> </ul>"},{"location":"about/#philosophies-and-passions","title":"Philosophies and Passions","text":"<p>My professional path is driven by a relentless pursuit of knowledge and excellence. I am continually exploring new technologies and methodologies to stay at the forefront of the IT field. My passion for sharing knowledge and mentoring others in the community is a testament to my commitment to the broader tech ecosystem.</p> <p>For a deeper insight into my professional background and experiences, feel free to connect.</p> <p>Tip</p> <p>For more information about me, check out my personal website: www.robertmelcher.ro.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/03/01/argocd-installation-on-kubernetes/","title":"ArgoCD installation on kubernetes","text":"<p>Install and configure ArgoCD</p> <ol> <li>Create a new namespace argocd and deploy ArgoCD with the web UI included. <pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre></li> <li>Log in to the ArgoCD web interface</li> </ol> <p>Log in to the ArgoCD web interface https://[your-dns-record/] by using the default username admin and the password, collected by the following command.</p> <pre><code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre>"},{"location":"blog/2024/03/06/deploying-a-kubernetes-based-media-server-a-comprehensive-guide/","title":"Deploying a Kubernetes-Based Media Server: A Comprehensive Guide","text":"MEDIA STACK K8S <p>For a long time, I've been on the hunt for a comprehensive and well-crafted tutorial to deploy a media server on my Kubernetes cluster. This media server stack includes Jellyfin, Radarr, Sonarr, Jackett, and qBittorrent. Let's briefly dive into what each component brings to our setup</p> <p>Example</p> <p>Jellyfin: An open-source media system that provides a way to manage and stream your media library across various devices. Radarr: A movie collection manager for Usenet and BitTorrent users. It automates the process of searching for movies, downloading, and managing your movie library. Sonarr: Similar to Radarr but for TV shows. It keeps track of your series, downloads new episodes, and manages your collection with ease. Jackett: Acts as a proxy server, translating queries from other apps (like Sonarr or Radarr) into queries that can be understood by a wide array of torrent search engines. qBittorrent: A powerful BitTorrent client that handles your downloads. Paired with Jackett, it streamlines finding and downloading media content.  Gluetun: A lightweight, open-source VPN client for Docker environments, supporting multiple VPN providers to secure and manage internet connections across containerized applications. It ensures privacy and seamless network security with easy configuration and integration.</p> <p>The configuration for these applications is hosted on Longhorn storage, ensuring resilience and ease of management, while the media (movies, shows, books, etc.) is stored on a Synology NAS DS223. The NAS location is utilized as a Persistent Volume (PV) through NFS 4.1 by Kubernetes.</p> <p>In this tutorial, you'll find the Kubernetes configuration for each necessary component to set up, install, and secure each service used by the media server.</p>"},{"location":"blog/2024/03/06/deploying-a-kubernetes-based-media-server-a-comprehensive-guide/#0-if-you-use-synology-nas-this-is-the-rule-i-created-for-my-nfs-share-which-will-be-mounted-on-kubernetes-side","title":"0. If you use Synology NAS, this is the rule I created for my NFS share which will be mounted on kubernetes side.","text":"<p>Let's start step by step.</p>"},{"location":"blog/2024/03/06/deploying-a-kubernetes-based-media-server-a-comprehensive-guide/#1configuring-pvc-and-pv-for-nfs-share","title":"1.Configuring PVC and PV for NFS Share","text":"<p>1.1) Media  Create nfs-media-pv-and-pvc.yaml:</p> <p><pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: jellyfin-videos\nspec:\n  capacity:\n    storage: 400Gi\n  accessModes:\n    - ReadWriteOnce\n  nfs:\n    path: /volume1/server/k3s/media\n    server: storage.merox.cloud\n  persistentVolumeReclaimPolicy: Retain\n  mountOptions:\n    - hard\n    - nfsvers=3\n  storageClassName: \"\"\n# Persistent Volume spec including capacity, access modes, NFS path, and server details follow\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: jellyfin-videos\n  namespace: media\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 400Gi\n  volumeName: jellyfin-videos\n  storageClassName: \"\"\n# Persistent Volume Claim spec including access modes, resources requests, and storage class name follow\n</code></pre> Apply with:</p> <pre><code>kubectl apply -f nfs-media-pv-and-pvc.yaml\n</code></pre> <p>1.2) Download  Create nfs-download-pv-and-pvc.yaml:</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: qbitt-download\nspec:\n  capacity:\n    storage: 400Gi\n  accessModes:\n    - ReadWriteOnce\n  nfs:\n    path: /volume1/server/k3s/media/download\n    server: storage.merox.cloud\n  persistentVolumeReclaimPolicy: Retain\n  mountOptions:\n    - hard\n    - nfsvers=3\n  storageClassName: \"\"\n# Persistent Volume spec including capacity, access modes, NFS path, and server details follow\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: qbitt-download\n  namespace: media\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 400Gi\n  volumeName: qbitt-download\n  storageClassName: \"\"\n# Persistent Volume Claim spec including access modes, resources requests, and storage class name follow\n</code></pre> <p>Apply with: <pre><code>kubectl apply -f nfs-download-pv-and-pvc.yaml\n</code></pre></p>"},{"location":"blog/2024/03/06/deploying-a-kubernetes-based-media-server-a-comprehensive-guide/#2configuring-longhorn-pvc-for-each-application","title":"2.Configuring Longhorn PVC for Each Application","text":"<p>Create app-config-pvc.yaml:</p> <p><pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: app # radarr for example\n  namespace: media\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 5Gi\n# Persistent Volume Claim spec including access modes, storage class name, and resources requests follow\n</code></pre> Apply with: <pre><code>kubectl apply -f app-config-pvc.yaml\n</code></pre></p> <p>Warning</p> <p>This type of configuration needs to be generated for each application: Jellyfin, Sonarr, Radarr, Jackett, qBittorrent.</p>"},{"location":"blog/2024/03/06/deploying-a-kubernetes-based-media-server-a-comprehensive-guide/#3deploying-each-application","title":"3.Deploying each application","text":"<p>3.1) Jellyfin:  Jellyfin serves as our media streaming platform, providing access to movies, TV shows, and other media across various devices. Here's how to deploy it</p> <p>Create specific yaml for each file, for example: radarr-deployment.yaml  Apply with <pre><code>kubectl apply -f radarr-deployment.yaml\n</code></pre></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jellyfin\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jellyfin\n  template:\n    metadata:\n      labels:\n        app: jellyfin\n    spec:\n      containers:\n      - name: jellyfin\n        image: jellyfin/jellyfin\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: videos\n          mountPath: /data/videos\n        ports:\n        - containerPort: 8096\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: jellyfin-config\n      - name: videos\n        persistentVolumeClaim:\n          claimName: jellyfin-videos\n</code></pre> <p>3.2) Sonarr:  Sonarr automates TV show downloads, managing our series collection efficiently.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sonarr\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sonarr\n  template:\n    metadata:\n      labels:\n        app: sonarr\n    spec:\n      containers:\n      - name: sonarr\n        image: linuxserver/sonarr\n        env:\n        - name: PUID\n          value: \"1057\"\n        - name: PGID\n          value: \"1056\"\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: videos\n          mountPath: /tv\n        - name: downloads\n          mountPath: /downloads\n        ports:\n        - containerPort: 8989\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: sonarr-config\n      - name: videos\n        persistentVolumeClaim:\n          claimName: jellyfin-videos\n      - name: downloads\n        persistentVolumeClaim:\n          claimName: qbitt-download \n</code></pre> <p>3.3) Radarr:  Radarr works like Sonarr but focuses on movies, keeping our film library organized and up-to-date.</p> <p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: radarr\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: radarr\n  template:\n    metadata:\n      labels:\n        app: radarr\n    spec:\n      containers:\n      - name: radarr\n        image: linuxserver/radarr\n        env:\n        - name: PUID\n          value: \"1057\"  \n        - name: PGID\n          value: \"1056\"  \n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: videos\n          mountPath: /movies\n        - name: downloads\n          mountPath: /downloads\n        ports:\n        - containerPort: 7878\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: radarr-config\n      - name: videos\n        persistentVolumeClaim:\n          claimName: jellyfin-videos\n      - name: downloads\n        persistentVolumeClaim:\n          claimName: qbitt-download \n</code></pre> 3.4) Jackett:  Jackett acts as a bridge between torrent search engines and our media management tools, enhancing their capabilities. <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jackett\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jackett\n  template:\n    metadata:\n      labels:\n        app: jackett\n    spec:\n      containers:\n      - name: jackett\n        image: linuxserver/jackett\n        env:\n        - name: PUID\n          value: \"1057\" \n        - name: PGID\n          value: \"1056\" \n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 9117\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: jackett-config\n</code></pre> 3.5) qBittorrent <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: qbittorrent\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: qbittorrent\n  template:\n    metadata:\n      labels:\n        app: qbittorrent\n    spec:\n      containers:\n      - name: qbittorrent\n        image: linuxserver/qbittorrent\n        resources:\n          limits:\n            memory: \"2Gi\"\n          requests:\n            memory: \"512Mi\"\n        env:\n        - name: PUID\n          value: \"1057\" \n        - name: PGID\n          value: \"1056\"  \n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: downloads\n          mountPath: /downloads\n        ports:\n        - containerPort: 8080\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: qbitt-config\n      - name: downloads\n        persistentVolumeClaim:\n          claimName: qbitt-download  \n</code></pre></p> <p>3.6) qBittorrent with Gluetun <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: qbittorrent\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: qbittorrent\n  template:\n    metadata:\n      labels:\n        app: qbittorrent\n    spec:\n      containers:\n        - name: qbittorrent\n          image: linuxserver/qbittorrent\n          resources:\n            limits:\n              memory: \"2Gi\"\n            requests:\n              memory: \"512Mi\"\n          env:\n           - name: PUID\n             value: \"1057\"\n           - name: PGID\n             value: \"1056\"\n          volumeMounts:\n            - name: config\n              mountPath: /config\n            - name: downloads\n              mountPath: /downloads\n          ports:\n            - containerPort: 8080\n\n        - name: gluetun\n          image: qmcgaw/gluetun\n          env:\n            - name: VPNSP\n              value: \"protonvpn\"\n            - name: OPENVPN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: protonvpn-secrets\n                  key: PROTONVPN_USER\n            - name: OPENVPN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: protonvpn-secrets\n                  key: PROTONVPN_PASSWORD\n            - name: COUNTRY\n              value: \"Germany\" \n          securityContext:\n            capabilities:\n              add:\n                - NET_ADMIN\n          volumeMounts:\n            - name: gluetun-config\n              mountPath: /gluetun\n\n      volumes:\n        - name: config\n          persistentVolumeClaim:\n            claimName: qbitt-config\n        - name: downloads\n          persistentVolumeClaim:\n            claimName: qbitt-download\n        - name: gluetun-config\n          persistentVolumeClaim:\n            claimName: gluetun-config\n</code></pre></p> <p>Example</p> <p>I've chosen to use ProtonVPN due to their security policy and because they do not collect/store data, but also because of the speeds and diverse settings, all at a very good price</p>"},{"location":"blog/2024/03/06/deploying-a-kubernetes-based-media-server-a-comprehensive-guide/#4creating-clusterip-services","title":"4.Creating ClusterIP Services","text":"<p>For our media server applications to communicate efficiently within the Kubernetes cluster without exposing them directly to the external network, we utilize ClusterIP services.</p> <p>4.1) To set this up, we create a app-service.yaml for each application (taking Radarr as an example here):</p> <p>create app-service.yaml <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: app #radarr for example \n  namespace: media\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 7878\n  selector:\n    app: app #radarr for example \n</code></pre> <pre><code>kubectl apply -f app-service.yaml\n</code></pre></p>"},{"location":"blog/2024/03/06/deploying-a-kubernetes-based-media-server-a-comprehensive-guide/#5creating-middleware-for-traefik-in-the-media-namespace","title":"5.Creating middleware for Traefik in the media namespace","text":"<p>For enhanced security and to ensure smooth functioning with Traefik, we define middleware:</p> <ul> <li>The middleware, named <code>default-headers-media</code>, is configured in the <code>media</code> namespace.</li> <li>It sets various security headers, including XSS protection and options to prevent MIME sniffing, among others.</li> </ul> <p>Create default-headers-media.yaml</p> <pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: default-headers-media\n  namespace: media\nspec:\n  headers:\n    browserXssFilter: true\n    contentTypeNosniff: true\n    forceSTSHeader: true\n    stsIncludeSubdomains: true\n    stsPreload: true\n    stsSeconds: 15552000\n    customFrameOptionsValue: SAMEORIGIN\n    customRequestHeaders:\n      X-Forwarded-Proto: https\n</code></pre> <p>Apply with: <pre><code>kubectl apply -f default-headers-media.yaml\n</code></pre></p>"},{"location":"blog/2024/03/06/deploying-a-kubernetes-based-media-server-a-comprehensive-guide/#6creating-ingress-route-for-each-application","title":"6.Creating Ingress Route for Each Application","text":"<p>To expose each application securely, we create IngressRoutes using Traefik:</p> <ul> <li>An IngressRoute for the application (such as Radarr) is defined, which uses the <code>traefik-external</code> ingress class.</li> <li>It listens on the <code>websecure</code> entry point and routes traffic based on the host (<code>movies.merox.cloud</code> in this example, replace with your domain).</li> <li>The middleware <code>default-headers-media</code> is applied to enhance security.</li> <li>TLS configuration is included, referencing a secret that contains the SSL/TLS certificate.</li> </ul> <p>Create app-ingress-route.yaml</p> <pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: app #radarr for example \n  namespace: media\n  annotations:\n    kubernetes.io/ingress.class: traefik-external\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`movies.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: app #radarr for example \n          port: 80\n    - match: Host(`movies.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: app #radarr for example \n          port: 80\n      middlewares:\n        - name: default-headers-media\n  tls:\n    secretName: mycert-tls # change to your cert name\n</code></pre> <p>Apply with: <pre><code>kubectl apply -f app-ingress-route.yaml\n</code></pre></p> <p>Danger</p> <p>Don't forget: You must create the host declared in your IngressRoute in your DNS server(s).</p>"},{"location":"blog/2024/03/06/deploying-a-kubernetes-based-media-server-a-comprehensive-guide/#qa","title":"Q&amp;A","text":"<p>Question</p> <p>Q: Why use a ClusterIP service?</p> <p>Answer</p> <p>A: Because we will be using Traefik as an ingress controller to expose it to the local network/internet with SSL/TLS certificates.</p> <p>Question</p> <p>Q: Can I download all manifest files from anywhere?</p> <p>Answer</p> <p>A: SURE! The link is at the end of this page :)</p> <p>This concludes the necessary steps and configurations to deploy a resilient media server in a Kubernetes cluster successfully.</p> <p>Quick link: All manifest files</p>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/","title":"Tailscale site-to-site pfSense - Linux","text":"Tailscale between pfSense and Linux <p>I've decided to implement monitoring for my homelab through a cloud virtual machine (VM) (I've opted for Hetzner, but more on that in a future post).</p> <p>To enhance the security of this setup, I've chosen to establish the cloud VM from Hetzner as the single entry point to my infrastructure. For this purpose, I've opted to use Tailscale for tunneling, not only for client-to-site but also for site-to-site connectivity.</p> <p>Info</p> <p>Informations provided by tailscale: \"Use site-to-site layer 3 (L3) networking to connect two subnets on your Tailscale network with each other. The two subnets are each required to provide a subnet router but their devices do not need to install Tailscale. This scenario applies to Linux subnet routers only.\"</p> <p>Warning</p> <p>This scenario will not work on subnets with overlapping CIDR ranges, nor with 4via6 subnet routing.</p> <p>In my case, there are two private subnets without any connectivity between them.  Subnet 1 - Homelab:  10.57.57.0/24 Subnet 2 - Cloudlab: 192.168.57.0/24</p> <p>IP addresses of the routers for each subnet:  Subnet 1 -&gt; 10.57.57.1 ( pfSense ) Subnet 2 -&gt; 192.168.57.254 ( Linux VM )</p>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#setting-up-tailscale-site-to-site-on-pfsense-subnet-1","title":"Setting up Tailscale site-to-site on pfSense (Subnet 1)","text":"<p>Let's dive into the configuration. Due to pfSense being based on FreeBSD and Tailscale not offering as much support for pfSense as for other platforms, this configuration is a bit trickier. But let's see how it looks.</p>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#1-install-tailscale-on-pfsense","title":"1) Install tailscale on pfSense:","text":"<p>Navigate to Package Manager: Go to System &gt; Package Manager in the pfSense web interface.</p> <p>Install Package:  Click on the \"Available Packages\" tab. Search for tailscale and click \"Install\".</p>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#12-configure-tailscale-on-pfsense","title":"1.2) Configure tailscale on pfSense:","text":"<p>Navigate to Tailscale: VPN -&gt; Tailscale</p>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#121-authentication","title":"1.2.1) Authentication:","text":"<ul> <li>Copy auth-key from https://login.tailscale.com/admin/settings/keys</li> <li>Generate Auth keys</li> </ul>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#122-settings","title":"1.2.2) Settings:","text":"<ul> <li>Check: \"Enable tailscale\" </li> <li>Listen port: leave it as it is</li> <li>Check: Accept Subnet Routes</li> <li>Optional check: Advertise Exit Node</li> <li>Advertised Routes: 10.57.57.0/24 </li> </ul>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#123-tricky-part-outbound-nat-rules","title":"1.2.3) Tricky part: Outbound NAT Rules","text":"<p>Navigate to Firewall-&gt; NAT-&gt; Outbound</p>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#make-sure-outbound-nat-mode-is-configured-to-be-configured-as","title":"Make sure Outbound NAT Mode is configured to be configured as:","text":"<p>Hybrid Outbound NAT </p>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#124-create-next-manual-mapping","title":"1.2.4) Create next manual mapping:","text":"<ul> <li>Interface: Tailscale</li> <li>Address Family: IPV4+IPV6</li> <li>Protocol: Any</li> <li>Source Network or Alias: 10.57.57.0/24</li> <li>Destination: Any</li> </ul> <p>This part is broken from last update ( 23.09.1 ) so NAT Alias is missing.  Workaround: * Translation section:     * Address: Network or Alias put the tailscale ip address 100.xx.xx.xx/32 This is how should look like: </p>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#configure-tailscale-site-to-site-on-linux-vm-subnet-2","title":"Configure tailscale site-to-site on Linux VM (Subnet 2)","text":""},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#2-install-tailscale-and-activate-routing","title":"2) Install tailscale and activate routing:","text":"<pre><code>    curl -sSL https://tailscale.com/install.sh | sh #Install tailscale\n    echo 'net.ipv4.ip_forward = 1' | sudo tee -a /etc/sysctl.conf #Activate routing for IPv4\n    echo 'net.ipv6.conf.all.forwarding = 1' | sudo tee -a /etc/sysctl.conf #Activate routing for IPv6\n    sudo sysctl -p /etc/sysctl.conf # Apply routing configuration at kernel level\n</code></pre>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#21-on-the-19216857254-device-advertise-routes-for-19216857024","title":"2.1) On the 192.168.57.254 device, advertise routes for 192.168.57.0/24:","text":"<pre><code>    tailscale up --advertise-routes=192.168.57.0/24 --snat-subnet-routes=false --accept-routes\n</code></pre> <p>Command explained: --advertise-routes: Exposes the physical subnet routes to your entire Tailscale network. --snat-subnet-routes=false: Disables source NAT. In normal operations, a subnet device will see the traffic originating from the subnet router. This simplifies routing, but does not allow traversing multiple networks. By disabling source NAT, the end machine sees the LAN IP address of the originating machine as the source. --accept-routes: Accepts the advertised route of the other subnet router, as well as any other nodes that are subnet routers.</p>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#enable-subnet-routes-from-the-admin-console","title":"Enable subnet routes from the admin console","text":"<p>Info</p> <p>This step is not required if using autoApprovers.</p> <p>Open the Machines page of the admin console, and locate the devices that you configured as subnet routers. You can look for the Subnets badge in the machines list, or use the property:subnet filter to see all devices advertising subnet routes. For each device that you need to approve, click the ellipsis icon menu at the end of the table, and select Edit route settings. In the Edit route settings panel, approve the device.</p> <p>Success</p> <p>The Tailscale side of the routing is complete.</p>"},{"location":"blog/2024/03/27/tailscale-site-to-site-pfsense---linux/#credits","title":"Credits","text":"<ul> <li> Tailscale Seamless networking for secure connections.</li> <li> Christian McDonald  YouTube Channel</li> </ul>"},{"location":"fundamentals/intro/","title":"Fundamentals","text":"<p>Welcome to the Fundamentals section, a personal exploration through the foundational aspects of IT infrastructure. This space is born from my hands-on experiences and observations, aimed at those who share a curiosity or passion for technology. While my insights may not cover every nuance, they serve as a starting point for discussion and discovery.</p>"},{"location":"fundamentals/intro/#navigate-through-basics","title":"Navigate Through Basics","text":"<p>Join me as we venture into the building blocks of IT infrastructure:</p>"},{"location":"fundamentals/intro/#personal-observations","title":"Personal Observations","text":"<p>Gain access to my thoughts on core components like servers, networks, and storage solutions, drawn from real-life scenarios.</p>"},{"location":"fundamentals/intro/#starting-points","title":"Starting Points","text":"<p>Explore my take on beginning your journey in IT, offering a blend of advice, anecdotal evidence, and personal reflections.</p>"},{"location":"fundamentals/intro/#fundamental-concepts","title":"Fundamental Concepts","text":"<p>Dive into discussions that aim to decode the essential principles underlying IT operations, based on my experiences.</p>"},{"location":"fundamentals/intro/#why-share-these-fundamentals","title":"Why Share These Fundamentals?","text":"<ul> <li> <p>Perspective: Offer a unique viewpoint on the basics of IT, emphasizing practical insights over comprehensive education.</p> </li> <li> <p>Community Dialogue: Encourage conversation and exchange of ideas within the tech community, fostering a space for collective learning.</p> </li> <li> <p>Personal Growth: Document my journey and reflections, inviting others to explore their own path in the vast domain of technology.</p> </li> </ul> <p>Embark on this exploratory journey with me, as we delve into the fundamentals of IT infrastructure through a personal lens.</p>"},{"location":"fundamentals/backup/clonezilla/","title":"Clonezilla: Backup Guide","text":""},{"location":"fundamentals/backup/clonezilla/#overview","title":"Overview","text":"<p>Clonezilla is a versatile tool for disk and partition imaging/cloning, perfect for backing up any disks. This guide covers how to use Clonezilla for backups using both NFS shares and local storage options, ensuring your data's safety and system's rapid recovery in various scenarios.For example, I use clonezilla to clone my Proxmox cluster installation disks.</p>"},{"location":"fundamentals/backup/clonezilla/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Clonezilla Live on a bootable USB drive : Downlaod link</li> <li>Configured and accessible NFS server (for NFS share backups)</li> <li>Sufficient storage space on your backup destination</li> </ul>"},{"location":"fundamentals/backup/clonezilla/#booting-clonezilla","title":"Booting Clonezilla","text":"<ol> <li>Insert the Clonezilla Live USB into the system.</li> <li>Reboot and select the USB drive as the boot device.</li> <li>Follow the on-screen instructions to load Clonezilla.</li> </ol>"},{"location":"fundamentals/backup/clonezilla/#backup-using-nfs-share","title":"Backup using NFS Share","text":""},{"location":"fundamentals/backup/clonezilla/#step-1-select-backup-mode","title":"Step 1: Select Backup Mode","text":"<ul> <li>Choose \"device-image\" for disk or partition image backups.</li> </ul>"},{"location":"fundamentals/backup/clonezilla/#step-2-storage-selection","title":"Step 2: Storage Selection","text":"<ul> <li>Select \"NFS server\" to utilize an NFS share as your backup destination.</li> </ul>"},{"location":"fundamentals/backup/clonezilla/#step-3-configure-nfs-share","title":"Step 3: Configure NFS Share","text":"<ul> <li>Input your NFS server's IP and the shared folder path for storing backups.</li> </ul>"},{"location":"fundamentals/backup/clonezilla/#step-4-select-source-disk","title":"Step 4: Select Source Disk","text":"<ul> <li>Choose the disk within your Proxmox cluster to back up.</li> </ul>"},{"location":"fundamentals/backup/clonezilla/#step-5-start-backup","title":"Step 5: Start Backup","text":"<ul> <li>Follow prompts to initiate the backup process to the NFS share.</li> </ul>"},{"location":"fundamentals/backup/clonezilla/#backup-to-local-storage","title":"Backup to Local Storage","text":""},{"location":"fundamentals/backup/clonezilla/#step-1-backup-mode","title":"Step 1: Backup Mode","text":"<ul> <li>Opt for \"device-image\" for creating image backups.</li> </ul>"},{"location":"fundamentals/backup/clonezilla/#step-2-choose-storage","title":"Step 2: Choose Storage","text":"<ul> <li>Select \"local_dev\" for using local storage as the backup destination.</li> </ul>"},{"location":"fundamentals/backup/clonezilla/#step-3-connect-storage-device","title":"Step 3: Connect Storage Device","text":"<ul> <li>Ensure your external storage device is connected and recognized by Clonezilla.</li> </ul>"},{"location":"fundamentals/backup/clonezilla/#step-4-select-source-disk_1","title":"Step 4: Select Source Disk","text":"<ul> <li>Identify and select the Proxmox cluster disk for backup.</li> </ul>"},{"location":"fundamentals/backup/clonezilla/#step-5-initiate-backup","title":"Step 5: Initiate Backup","text":"<ul> <li>Proceed with on-screen instructions to begin backup to local storage.</li> </ul>"},{"location":"fundamentals/backup/clonezilla/#restoring-from-backup","title":"Restoring from Backup","text":"<ol> <li>Boot from the Clonezilla Live USB.</li> <li>Follow similar steps to the backup process but choose \"restore.\"</li> <li>Select your backup image and follow the on-screen steps to complete restoration.</li> </ol>"},{"location":"fundamentals/backup/clonezilla/#testing-and-validation","title":"Testing and Validation","text":"<p>Ensure to test your Proxmox cluster post-backup or restoration to validate functionality and data integrity.</p>"},{"location":"fundamentals/database/influxdb/basics/","title":"Basics","text":""},{"location":"fundamentals/database/influxdb/basics/#influxdb-cheatsheet","title":"InfluxDB Cheatsheet","text":""},{"location":"fundamentals/database/influxdb/basics/#connect-to-influxdb-using-the-commandline","title":"Connect to InfluxDB using the commandline:","text":"<pre><code>    $ influx\n</code></pre>"},{"location":"fundamentals/database/influxdb/basics/#create-a-database-foo","title":"Create a database foo:","text":"<pre><code>    CREATE DATABASE foo\n</code></pre>"},{"location":"fundamentals/database/influxdb/basics/#list-the-databases","title":"List the databases:","text":"<pre><code>    SHOW DATABASES\n</code></pre>"},{"location":"fundamentals/database/influxdb/basics/#select-the-new-created-database","title":"Select the new created database:","text":"<pre><code>    USE foo\n</code></pre>"},{"location":"fundamentals/database/influxdb/basics/#list-measurements","title":"List measurements","text":"<pre><code>    SHOW MEASUREMENTS\n</code></pre>"},{"location":"fundamentals/database/influxdb/basics/#show-measurements-for-name-mars","title":"Show measurements for name: mars","text":"<pre><code>    SELECT * FROM mars\n</code></pre>"},{"location":"fundamentals/database/influxdb/basics/#drop-mars-measurements","title":"Drop mars measurements","text":"<pre><code>    DROP MEASUREMENT mars\n</code></pre>"},{"location":"fundamentals/database/influxdb/basics/#show-field-keys","title":"Show field keys","text":"<pre><code>    SHOW FIELD KEYS FROM \"mars-A6\"\n</code></pre>"},{"location":"fundamentals/database/influxdb/basics/#get-power-records-from-measurement-with-tag-and-time-range","title":"Get power records from measurement with tag and time range","text":"<pre><code>    SELECT \"power\" FROM \"drilling\" WHERE (\"module_id\"='rover') AND time &gt;= now() - 9h\n</code></pre>"},{"location":"fundamentals/database/influxdb/basics/#show-series","title":"Show series","text":"<pre><code>    SHOW SERIES\n</code></pre>"},{"location":"fundamentals/database/influxdb/basics/#drop-all-series-for-tag","title":"Drop all series for tag","text":"<pre><code>    DROP SERIES FROM \"drilling\" WHERE (\"module_id\" = 'oppy')\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/","title":"Basics","text":""},{"location":"fundamentals/database/mysql/basics/#mysql-cheatsheet","title":"MySQL Cheatsheet","text":""},{"location":"fundamentals/database/mysql/basics/#create-database","title":"Create Database;","text":"<p><pre><code>create database dbname;\n</code></pre> or</p> <pre><code>create database `dbname` CHARACTER SET utf8 COLLATE utf8_general_ci;\n</code></pre> <p>or</p> <pre><code>create schema some_db default character set utf8mb4;\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#create-user","title":"Create User","text":"<pre><code>create user 'user'@'%' identified by 'some_pwd';\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#grant-privileges-to-user","title":"Grant Privileges to User","text":"<pre><code>grant all privileges on dbname.* to 'user'@'%';\nflush privileges;\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#update-users-password","title":"Update users password","text":"<pre><code>ALTER USER 'userName'@'%' IDENTIFIED BY 'Newpass';\nflush privileges;\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#show-users","title":"Show Users","text":"<pre><code>select user,host from mysql.user;\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#show-grants","title":"Show Grants","text":"<pre><code>show grants for 'some_user'@'%';\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#give-admin-rights","title":"Give Admin Rights","text":"<pre><code>GRANT ALL PRIVILEGES ON *.* TO 'some_user'@'%';\n</code></pre> <pre><code>GRANT ALL PRIVILEGES ON *.* TO 'some_user'@'localhost';\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#drop-database","title":"Drop database","text":"<pre><code>drop databse some_db;\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#show-process-list","title":"Show Process list","text":"<pre><code>show processlist;\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#max-connections","title":"max connections","text":"<pre><code>show variables like \"max_connections\";\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#increase-max-connections","title":"Increase max connections:","text":"<pre><code>set global max_connections = 200;\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#max-allowed-packets","title":"max allowed packets","text":"<p>See max allowed packets value:</p> <pre><code>SHOW VARIABLES LIKE 'max_allowed_packet';\n</code></pre>"},{"location":"fundamentals/database/mysql/basics/#change-max-allowed-packets-value","title":"Change max allowed packets value:","text":"<pre><code>SET GLOBAL max_allowed_packet=16777216;\n</code></pre>"},{"location":"fundamentals/networking/ccna/cheatsheet/","title":"Cheatsheet","text":""},{"location":"fundamentals/networking/ccna/cheatsheet/#packetlife","title":"Packetlife:","text":"<p>https://packetlife.net/library/cheat-sheets/</p>"},{"location":"fundamentals/networking/ccna/cheatsheet/#geeksforgeeks","title":"Geeksforgeeks","text":"<p>https://www.geeksforgeeks.org/ccna-cheatsheet/</p>"},{"location":"fundamentals/networking/cisco/install/firewall/","title":"Basic Cisco ASA Firewall Configuration Guide","text":"<p>This document outlines the steps for a basic configuration of the Cisco ASA firewall, covering initial setup, interface security levels, NAT configurations, and basic access control rules.</p>"},{"location":"fundamentals/networking/cisco/install/firewall/#initial-setup","title":"Initial Setup","text":""},{"location":"fundamentals/networking/cisco/install/firewall/#accessing-the-asa","title":"Accessing the ASA","text":"<ol> <li>Connect to the ASA via console cable and use a terminal emulator to access the ASA's command-line interface (CLI).</li> </ol>"},{"location":"fundamentals/networking/cisco/install/firewall/#entering-global-configuration-mode","title":"Entering Global Configuration Mode","text":"<p><pre><code>asa&gt; enable\nPassword: [Enter password here]\nasa# configure terminal\n</code></pre> Setting Hostname</p> <p><pre><code>asa(config)# hostname ASAfirewall\n</code></pre> Configuring Console and VTY Passwords</p> <p><pre><code>ASAfirewall(config)# aaa authentication serial console LOCAL\nASAfirewall(config)# username admin password PASSWORD privilege 15\n</code></pre> Setting the Enable Secret Password</p> <p><pre><code>ASAfirewall(config)# enable password EN_PASSWORD\n</code></pre> Configuring Interfaces Setting Interface Names and Security Levels</p> <p><pre><code>ASAfirewall(config)# interface GigabitEthernet0/0\nASAfirewall(config-if)# nameif outside\nASAfirewall(config-if)# security-level 0\nASAfirewall(config-if)# ip address dhcp setroute\nASAfirewall(config-if)# no shutdown\n\nASAfirewall(config)# interface GigabitEthernet0/1\nASAfirewall(config-if)# nameif inside\nASAfirewall(config-if)# security-level 100\nASAfirewall(config-if)# ip address 192.168.1.1 255.255.255.0\nASAfirewall(config-if)# no shutdown\n</code></pre> Configuring PAT (Port Address Translation)</p> <p><pre><code>ASAfirewall(config)# object network obj_any\nASAfirewall(config-network-object)# subnet 0.0.0.0 0.0.0.0\nASAfirewall(config-network-object)# nat (inside,outside) dynamic interface\n</code></pre> Configuring Access Rules Allowing Internal Users to Access the Internet</p> <p><pre><code>ASAfirewall(config)# access-list outside_access_in extended permit ip any any\nASAfirewall(config)# access-group outside_access_in in interface outside\n</code></pre> Allowing SSH Access to the ASA</p> <p><pre><code>ASAfirewall(config)# crypto key generate rsa modulus 2048\nASAfirewall(config)# aaa authentication ssh console LOCAL\nASAfirewall(config)# ssh 192.168.1.0 255.255.255.0 inside\nASAfirewall(config)# ssh timeout 60\n</code></pre> Saving the Configuration</p> <p><pre><code>ASAfirewall(config)# write memory\n</code></pre> Verifying Configuration</p> <p><pre><code>ASAfirewall# show running-config interface\nASAfirewall# show running-config object\nASAfirewall# show running-config access-list\n</code></pre> This guide provides a foundational approach to configuring the Cisco ASA firewall for basic network protection and connectivity. It covers setting up interface names and security levels, configuring PAT to allow internal network access to the internet, and setting up basic access rules. Adjustments might be needed based on your specific network requirements and ASA model.</p>"},{"location":"fundamentals/networking/cisco/install/ipsecgre/","title":"Configuring IPsec over GRE Tunnel on Cisco Devices","text":"<p>This guide outlines the steps to configure an IPsec over GRE tunnel on Cisco routers. This setup combines the advantages of GRE tunnels, such as the ability to encapsulate a wide variety of network layer protocols over a single point-to-point link, with the security features of IPsec.</p>"},{"location":"fundamentals/networking/cisco/install/ipsecgre/#prerequisites","title":"Prerequisites","text":"<ul> <li>Two Cisco routers with IP connectivity.</li> <li>IOS with crypto support.</li> <li>Knowledge of the network topology and IP addressing scheme.</li> </ul>"},{"location":"fundamentals/networking/cisco/install/ipsecgre/#configuration-overview","title":"Configuration Overview","text":"<ol> <li>Configure GRE Tunnel</li> <li>Configure IPsec</li> <li>Verify the Tunnel and IPsec Configuration</li> </ol>"},{"location":"fundamentals/networking/cisco/install/ipsecgre/#step-1-configure-gre-tunnel","title":"Step 1: Configure GRE Tunnel","text":""},{"location":"fundamentals/networking/cisco/install/ipsecgre/#router-a-configuration","title":"Router A Configuration","text":"<p>Replace <code>192.168.1.1</code> with the local tunnel IP and <code>192.168.2.1</code> with the remote tunnel IP.</p> <p><pre><code>interface Tunnel0\n ip address 192.168.1.1 255.255.255.0\n tunnel source &lt;RouterA_Outside_Interface&gt;\n tunnel destination &lt;RouterB_Public_IP&gt;\n tunnel mode gre ip\n</code></pre> Router B Configuration</p> <p>Replace 192.168.2.1 with the local tunnel IP and 192.168.1.1 with the remote tunnel IP.</p> <p><pre><code>interface Tunnel0\n ip address 192.168.2.1 255.255.255.0\n tunnel source &lt;RouterB_Outside_Interface&gt;\n tunnel destination &lt;RouterA_Public_IP&gt;\n tunnel mode gre ip\n</code></pre> Step 2: Configure IPsec Define ISAKMP Policy (on both routers)</p> <p>This policy defines the main mode parameters.</p> <p><pre><code>crypto isakmp policy 10\n encr aes 256\n authentication pre-share\n group 5\n</code></pre> Specify Pre-shared Key (on both routers)</p> <p>Replace YourPSK with your pre-shared key.</p> <p><pre><code>crypto isakmp key YourPSK address &lt;Peer_Public_IP&gt;\n</code></pre> Define IPsec Transform Set (on both routers)</p> <p>This set specifies the transform parameters for IPsec.</p> <p><pre><code>crypto ipsec transform-set MYSET esp-aes 256 esp-sha-hmac\n mode transport\n</code></pre> Define the Crypto Map (on both routers)</p> <p>This map ties the ISAKMP and IPsec configuration together and applies it to the interface.</p> <p><pre><code>crypto map MYMAP 10 ipsec-isakmp\n set peer &lt;Peer_Public_IP&gt;\n set transform-set MYSET\n match address 100\n</code></pre> Apply the Crypto Map to the Outside Interface (on both routers)</p> <p>Replace OutsideInterface with the actual interface name facing the peer.</p> <p><pre><code>interface &lt;OutsideInterface&gt;\n crypto map MYMAP\n</code></pre> Configure Access Control List (ACL) (on both routers)</p> <p>This ACL permits the GRE tunnel traffic to be encrypted by IPsec.</p> <p><pre><code>access-list 100 permit gre host &lt;Local_Public_IP&gt; host &lt;Peer_Public_IP&gt;\n</code></pre> Step 3: Verify the Tunnel and IPsec Configuration Verify GRE Tunnel Status</p> <p><pre><code>show interface Tunnel0\n</code></pre> Verify IPsec SA (Security Associations)</p> <p><pre><code>show crypto ipsec sa\n</code></pre> Verify ISAKMP SA</p> <p><pre><code>show crypto isakmp sa\n</code></pre> These commands help ensure that the GRE tunnel is up and that IPsec encryption is operational between the two endpoints.</p> <p>By following these steps, you will have successfully configured an IPsec over GRE tunnel on Cisco routers, providing a secure and encapsulated VPN tunnel for your network traffic.</p>"},{"location":"fundamentals/networking/cisco/install/router/","title":"Basic Cisco Router Configuration Guide","text":"<p>This tutorial covers the essentials of configuring a Cisco router for a CCNA-level routing and switching environment. It includes initial setup, interface configuration, and setting up routing protocols.</p>"},{"location":"fundamentals/networking/cisco/install/router/#accessing-the-router","title":"Accessing the Router","text":"<ol> <li> <p>Connect to the Router:    Use a console cable to connect your computer to the router's console port. Use terminal emulation software (like PuTTY or Tera Term) to access the router's command line interface (CLI).</p> </li> <li> <p>Enter Global Configuration Mode:    After accessing the CLI, enter the global configuration mode to make changes to the router's configuration:</p> </li> </ol> <pre><code>Router&gt; enable\nRouter# configure terminal\nRouter(config)#\n</code></pre>"},{"location":"fundamentals/networking/cisco/install/router/#basic-configuration","title":"Basic Configuration","text":"<ol> <li>Set Hostname:    Assign a hostname to the router for easy identification:</li> </ol> <pre><code>Router(config)# hostname MyRouter\nMyRouter(config)#\n</code></pre> <ol> <li> <p>Secure Access:    Configure a secret password to secure privileged EXEC mode access:</p> <pre><code>MyRouter(config)# enable secret mySecretPassword\n</code></pre> </li> </ol> <p>Optionally, set a password for console access:</p> <pre><code>MyRouter(config)# line console 0\nMyRouter(config-line)# password consolePassword\nMyRouter(config-line)# login\nMyRouter(config-line)# exit\n</code></pre> <ol> <li>Configure Interfaces:    Assign IP addresses to the router's interfaces and bring them up:</li> </ol> <pre><code>MyRouter(config)# interface GigabitEthernet0/0\nMyRouter(config-if)# ip address 192.168.1.1 255.255.255.0\nMyRouter(config-if)# no shutdown\nMyRouter(config-if)# exit\n</code></pre> <p>Repeat these steps for other interfaces as needed, adjusting the interface identifiers and IP addresses according to your network design.</p> <ol> <li>Save Configuration:    To save your configuration to the startup configuration file:</li> </ol> <pre><code>MyRouter(config)# exit\nMyRouter# copy running-config startup-config\n</code></pre>"},{"location":"fundamentals/networking/cisco/install/router/#setting-up-routing","title":"Setting Up Routing","text":"<ol> <li>Static Routing:    For a simple network, you can set up static routing by specifying a destination network and the next-hop address or exit interface:</li> </ol> <pre><code>MyRouter(config)# ip route 0.0.0.0 0.0.0.0 GigabitEthernet0/1\n</code></pre> <p>This command sets a default route, directing all unknown traffic to the next hop specified by the <code>GigabitEthernet0/1</code> interface.</p> <ol> <li>Dynamic Routing:</li> </ol> <p>EIGRP Configuration</p> <pre><code>Enable EIGRP:\n\nSpecify the EIGRP autonomous system number. This number must match across all routers in the EIGRP domain.\n</code></pre> <p><pre><code>RouterA(config)# router eigrp 1\n</code></pre> Advertise Networks:</p> <p>Use the network command to specify which networks to advertise via EIGRP.</p> <p><pre><code>RouterA(config-router)# network 192.168.1.0 0.0.0.255\nRouterA(config-router)# network 10.1.1.0 0.0.0.255\n</code></pre> Optional: Configure EIGRP for IPv6 (if required):</p> <p><pre><code>RouterA(config)# ipv6 router eigrp 1\nRouterA(config-rtr)# eigrp router-id 1.1.1.1\nRouterA(config-rtr)# no shutdown\nRouterA(config)# interface GigabitEthernet0/0\nRouterA(config-if)# ipv6 eigrp 1\n</code></pre> OSPF Configuration</p> <ul> <li> <p>Enable OSPF:</p> </li> <li> <p>Define the OSPF process ID and router ID.</p> </li> </ul> <p><pre><code>RouterA(config)# router ospf 1\nRouterA(config-router)# router-id 1.1.1.1\n</code></pre> Advertise Networks:</p> <p>Use the network command to advertise networks, specifying the area.</p> <p><pre><code>RouterA(config-router)# network 192.168.1.0 0.0.0.255 area 0\nRouterA(config-router)# network 10.1.1.0 0.0.0.255 area 0\n</code></pre> Optional: Configure OSPF for IPv6 (if required):</p> <p><pre><code>RouterA(config)# ipv6 router ospf 1\nRouterA(config-rtr)# router-id 1.1.1.1\nRouterA(config)# interface GigabitEthernet0/0\nRouterA(config-if)# ipv6 ospf 1 area 0\n</code></pre> Verifying Configuration</p> <p>After configuring the router, use various show commands to verify the setup and the operation of the dynamic routing protocols. For EIGRP:</p> <p><pre><code>RouterA# show ip eigrp neighbors\nRouterA# show ip eigrp topology\n</code></pre> For OSPF:</p> <pre><code>RouterA# show ip ospf neighbor\nRouterA# show ip route ospf\n</code></pre>"},{"location":"fundamentals/networking/cisco/install/router/#best-practices","title":"Best Practices","text":"<ul> <li>Security: Always change default passwords and consider implementing additional security features such as ACLs (Access Control Lists).</li> <li>Backup Configuration: Regularly backup your router's configuration to avoid data loss.</li> <li>Firmware Updates: Keep your router's firmware up to date to ensure you have the latest features and security patches.</li> </ul> <p>This guide provides a foundation for configuring a Cisco router in a CCNA routing and switching context. For more detailed configurations and advanced features, refer to Cisco's official documentation and CCNA study resources.</p>"},{"location":"fundamentals/networking/cisco/install/switch/","title":"Cisco Switch: Layer 2 and Layer 3","text":"<p>This document outlines the steps for basic configuration of Cisco switches, including setups for both Layer 2 and Layer 3 functionalities. It's designed to serve as a practical guide for configuring Cisco switches to support various network architectures and designs.</p>"},{"location":"fundamentals/networking/cisco/install/switch/#basic-switch-configuration","title":"Basic Switch Configuration","text":""},{"location":"fundamentals/networking/cisco/install/switch/#accessing-the-switch","title":"Accessing the Switch","text":"<ol> <li>Connect to the switch via a console cable and use a terminal emulator to access the switch's command-line interface (CLI).</li> </ol>"},{"location":"fundamentals/networking/cisco/install/switch/#entering-global-configuration-mode","title":"Entering Global Configuration Mode","text":"<p><pre><code>Switch&gt; enable\nSwitch# configure terminal\n</code></pre> Setting Hostname</p> <p><pre><code>Switch(config)# hostname SwitchA\n</code></pre> Configuring Console and VTY Passwords</p> <p><pre><code>SwitchA(config)# line console 0\nSwitchA(config-line)# password PASSWORD\nSwitchA(config-line)# login\nSwitchA(config-line)# exit\nSwitchA(config)# line vty 0 15\nSwitchA(config-line)# password PASSWORD\nSwitchA(config-line)# login\nSwitchA(config-line)# exit\n</code></pre> Setting the Enable Secret Password</p> <p><pre><code>SwitchA(config)# enable secret EN_PASSWORD\n</code></pre> Saving the Configuration</p> <p><pre><code>SwitchA(config)# exit\nSwitchA# copy running-config startup-config\n</code></pre> Layer 2 Switch Configuration Creating VLANs</p> <p><pre><code>SwitchA(config)# vlan 10\nSwitchA(config-vlan)# name Sales\nSwitchA(config-vlan)# exit\nSwitchA(config)# vlan 20\nSwitchA(config-vlan)# name Engineering\nSwitchA(config-vlan)# exit\n</code></pre> Assigning VLANs to Ports</p> <p><pre><code>SwitchA(config)# interface range fa0/1 - 2\nSwitchA(config-if-range)# switchport mode access\nSwitchA(config-if-range)# switchport access vlan 10\nSwitchA(config-if-range)# exit\nSwitchA(config)# interface range fa0/3 - 4\nSwitchA(config-if-range)# switchport mode access\nSwitchA(config-if-range)# switchport access vlan 20\nSwitchA(config-if-range)# exit\n</code></pre> Configuring Trunk Ports</p> <p><pre><code>SwitchA(config)# interface fa0/24\nSwitchA(config-if)# switchport mode trunk\nSwitchA(config-if)# switchport trunk allowed vlan 10,20\nSwitchA(config-if)# exit\n</code></pre> Layer 3 Switch Configuration Enabling IP Routing</p> <p><pre><code>SwitchA(config)# ip routing\n</code></pre> Creating SVIs for Inter-VLAN Routing</p> <p><pre><code>SwitchA(config)# interface vlan 10\nSwitchA(config-if)# ip address 192.168.10.1 255.255.255.0\nSwitchA(config-if)# no shutdown\nSwitchA(config-if)# exit\nSwitchA(config)# interface vlan 20\nSwitchA(config-if)# ip address 192.168.20.1 255.255.255.0\nSwitchA(config-if)# no shutdown\nSwitchA(config-if)# exit\n</code></pre> Configuring Static Routing (if necessary)</p> <p><pre><code>SwitchA(config)# ip route 0.0.0.0 0.0.0.0 &lt;next_hop_ip_address&gt;\n</code></pre> Configuring Routing Protocols (Optional)</p> <p>For example, enabling OSPF:</p> <p><pre><code>SwitchA(config)# router ospf 1\nSwitchA(config-router)# network 192.168.10.0 0.0.0.255 area 0\nSwitchA(config-router)# network 192.168.20.0 0.0.0.255 area 0\nSwitchA(config-router)# exit\n</code></pre> Verifying Configurations For Layer 2</p> <p><pre><code>SwitchA# show vlan brief\nSwitchA# show interfaces trunk\n</code></pre> For Layer 3</p> <p><pre><code>SwitchA# show ip interface brief\nSwitchA# show ip route\n</code></pre> By following these configuration steps, your Cisco switch will be equipped to handle both Layer 2 switching and Layer 3 routing functionalities. Adjust configurations based on your specific network requirements and design.</p>"},{"location":"fundamentals/networking/configs/DHCP/","title":"DHCP","text":"<p>Dynamic Host Configuration Protocol (DHCP), allows a device such as pfSense\u00ae software to dynamically allocate IP addresses to clients from a predefined pool of addresses. DHCP also sends configuration information to clients such as a gateway, DNS servers, domain name, and other useful settings.</p> <pre><code>[2.7.0-RELEASE][admin@fw.merox.cloud]/root: cat /var/dhcpd/etc/dhcpd.conf \n\noption domain-name \"merox.cloud\";\noption ldap-server code 95 = text;\noption domain-search-list code 119 = text;\noption arch code 93 = unsigned integer 16; # RFC4578\n\ndefault-lease-time 7200;\nmax-lease-time 86400;\nlog-facility local7;\none-lease-per-client true;\ndeny duplicates;\nupdate-conflict-detection false;\nauthoritative;\nclass \"s_lan\" {\n        match pick-first-value (option dhcp-client-identifier, hardware);\n}\nsubnet X.X.X.X.0 netmask 255.255.255.0 {\n        pool {\n                option domain-name-servers X.X.X.X.188,X.X.X.X.1;\n\n                range X.X.X.X.202 X.X.X.X.254;\n        }\n\n        option routers X.X.X.X.1;\n        option domain-name-servers X.X.X.X.188,X.X.X.X.1;\n        ping-check true;\n\n}\n\n\nsubclass \"s_lan\" 1:bc:24:11:f8:37:bd;\nhost s_lan_7 {\n        hardware ethernet bc:24:11:68:c2:c6;\n        fixed-address X.X.X.X.80;\n        option host-name \"k3s-admin\";\n\n}\nsubclass \"s_lan\" 1:bc:24:11:68:c2:c6;\nhost s_lan_8 {\n        hardware ethernet bc:24:11:e0:a7:02;\n        fixed-address X.X.X.X.81;\n        option host-name \"k3s-master-01\";\n\n}\n\nsubclass \"s_lan\" 1:90:09:d0:50:08:4b;\nclass \"s_opt1\" {\n        match pick-first-value (option dhcp-client-identifier, hardware);\n}\nsubnet X.X.X.Y.0 netmask 255.255.255.0 {\n        pool {\n                option domain-name-servers X.X.X.X.1;\n\n                range X.X.X.Y.100 X.X.X.Y.200;\n        }\n\n        option routers X.X.X.Y.1;\n        option domain-name-servers X.X.X.X.1;\n        ping-check true;\n\n}\nhost s_opt1_0 {\n        hardware ethernet 14:4f:8a:dc:13:ba;\n        option dhcp-client-identifier \"merox_lenovo_laptop\";\n        fixed-address X.X.X.Y.10;\n\n}\nsubclass \"s_opt1\" 1:14:4f:8a:dc:13:ba;\nsubclass \"s_opt1\" \"merox_lenovo_laptop\";\n\nsubclass \"s_opt1\" 1:c4:c1:7d:a5:13:8e;\nsubclass \"s_opt1\" \"iphone_merox\";\n</code></pre>"},{"location":"fundamentals/networking/configs/DNS/","title":"DNS","text":"<p>Unbound is a validating, recursive, caching DNS resolver. It is designed to be fast and lean and incorporates modern features based on open standards.</p> <pre><code>[2.7.0-RELEASE][merox@fw]/root: cat /var/unbound/unbound.conf\n##########################\n# Unbound Configuration\n##########################\n\n##\n# Server configuration\n##\nserver:\n\nchroot: /var/unbound\nusername: \"unbound\"\ndirectory: \"/var/unbound\"\npidfile: \"/var/run/unbound.pid\"\nuse-syslog: yes\nport: 53\nverbosity: 1\nhide-identity: yes\nhide-version: yes\nharden-glue: yes\ndo-ip4: yes\ndo-ip6: yes\ndo-udp: yes\ndo-tcp: yes\ndo-daemonize: yes\nmodule-config: \"iterator\"\nunwanted-reply-threshold: 0\nnum-queries-per-thread: 4096\njostle-timeout: 200\ninfra-keep-probing: yes\ninfra-host-ttl: 900\ninfra-cache-numhosts: 10000\noutgoing-num-tcp: 10\nincoming-num-tcp: 10\nedns-buffer-size: 1432\ncache-max-ttl: 86400\ncache-min-ttl: 0\nharden-dnssec-stripped: yes\nmsg-cache-size: 4m\nrrset-cache-size: 8m\n\nnum-threads: 2\nmsg-cache-slabs: 2\nrrset-cache-slabs: 2\ninfra-cache-slabs: 2\nkey-cache-slabs: 2\noutgoing-range: 4096\n#so-rcvbuf: 4m\n\nprefetch: no\nprefetch-key: no\nuse-caps-for-id: no\nserve-expired: no\naggressive-nsec: no\n# Statistics\n# Unbound Statistics\nstatistics-interval: 0\nextended-statistics: yes\nstatistics-cumulative: yes\n\n# TLS Configuration\ntls-cert-bundle: \"/etc/ssl/cert.pem\"\n\n# Interface IP addresses to bind to\ninterface-automatic: yes\n\n# Outgoing interfaces to be used\noutgoing-interface: X.X.X.X\n\n# DNS Rebinding\n# For DNS Rebinding prevention\nprivate-address: 127.0.0.0/8\nprivate-address: 10.0.0.0/8\nprivate-address: ::ffff:a00:0/104\nprivate-address: 172.16.0.0/12\nprivate-address: ::ffff:ac10:0/108\nprivate-address: 169.254.0.0/16\nprivate-address: ::ffff:a9fe:0/112\nprivate-address: 192.168.0.0/16\nprivate-address: ::ffff:c0a8:0/112\nprivate-address: fd00::/8\nprivate-address: fe80::/10\n\n\n\n# Access lists\ninclude: /var/unbound/access_lists.conf\n\n# Static host entries\ninclude: /var/unbound/host_entries.conf\n\n# dhcp lease entries\ninclude: /var/unbound/dhcpleases_entries.conf\n\n\n\n# Domain overrides\ninclude: /var/unbound/domainoverrides.conf\n# Forwarding\nforward-zone:\n        name: \".\"\n        forward-addr: 1.1.1.1\n        forward-addr: 8.8.8.8\n        forward-addr: X.X.X.188\n\n\n# Unbound custom options\nserver:include: /var/unbound/pfb_dnsbl.*conf\n\n\n###\n# Remote Control Config\n###\ninclude: /var/unbound/remotecontrol.conf\n</code></pre>"},{"location":"fundamentals/operating-systems/linux/config/extend-lvm-proxmox/","title":"Extend LVM","text":""},{"location":"fundamentals/operating-systems/linux/config/extend-lvm-proxmox/#resizingextending-logical-volumes-lvm-in-proxmox","title":"Resizing/Extending Logical Volumes (LVM) in Proxmox","text":"<p>What is LVM?</p> <p>LVM stands for Logical Volume Management. It is a system of managing logical volumes, or filesystems, that is much more advanced and flexible than the traditional method of partitioning a disk into one or more segments and formatting that partition with a filesystem. - https://wiki.ubuntu.com/Lvm</p> <p>Warning</p> <p>Below are the steps I took when I replaced my Proxmox Backup Server 30GB SSD with a 400GB and cloned the installation from the old card to the new one. Do not attempt these steps without first having a backup as there is a high risk of data loss if the partition changes are unsuccessful</p>"},{"location":"fundamentals/operating-systems/linux/config/extend-lvm-proxmox/#extending-a-lvm-volume","title":"Extending a LVM Volume","text":"<p>Log into the device using LVM, in this example I'll be extending the pbs-root and data volumes in Proxmox Backup Server Run the following commands in terminal</p> <pre><code># login as root if needed (not needed for proxmox)\nsudo su\n# list disks and partitions\nfdisk -l\n# list volume groups\nvgdisplay\n# list logical volumes\nlvdisplay\n# edit partitions with fdisk, change device id as needed\nfdisk /dev/sda\n# print the partition table\np\n# delete a partition\nd\n# enter the lvm partition number\n3\n# create a new partition\nn\n# enter the new partition number, same as the number deleted\n3\n# press enter to accept the default first sector\n# press enter to accept the default last sector\n# when prompted about removing the LVM signature, enter N\nn\n# set the partition type\nt\n# enter the partition number\n3\n# set the type to Linux LVM\n43\n# write the changes\nw\n# list disks and partitions, noting the size increase\nfdisk -l\n# extend the existing physical volume\npvresize /dev/sda3\n# extend the pbs-root logical volume to 100% available free space\nlvextend -l +100%FREE /dev/pbs/root\n# extend the underlying file system\nresize2fs /dev/mapper/pbs-root\n# list logical volumes\nlvdisplay\n</code></pre>"},{"location":"fundamentals/operating-systems/linux/config/ufw/","title":"UFW","text":"<p>UFW, or Uncomplicated Firewall, is a user-friendly interface designed to manage netfilter firewall rules on Linux systems. It provides a much simpler way to configure the firewall, making it accessible for users who may not be as experienced with firewall concepts. Here\u2019s a guide to understanding and using UFW, including setting up basic rules and troubleshooting common issues. Understanding UFW</p> <p>UFW operates by allowing or blocking traffic based on the rules you define. These rules can specify which services are allowed to communicate in and out of your system. For instance, you might allow HTTP traffic on port 80 for a web server or SSH traffic on port 22 for remote administration. Setting Up UFW</p> <ul> <li> <p>Installation: UFW is included with most Linux distributions. If it\u2019s not already installed, you can usually add it using your distribution's package manager.</p> </li> <li> <p>Enabling UFW: To activate UFW, use the command sudo ufw enable. This command starts the firewall and ensures it starts automatically at boot.</p> </li> <li> <p>Defining Rules: Rules can be added based on applications or specifically by port number. For example, sudo ufw allow http or sudo ufw allow 22/tcp. The first command allows traffic on the HTTP port (80), and the second allows TCP traffic on port 22 (SSH).</p> </li> <li> <p>Checking the Status: You can view the current rules and status of UFW by typing sudo ufw status. This command lists all active rules and indicates whether the firewall is active.</p> </li> </ul> <p>Advanced UFW Usage</p> <ul> <li>Denying Traffic: If you wish to block traffic, you can use sudo ufw deny followed by the service or port number.</li> <li>Deleting Rules: To remove a rule, you can use sudo ufw delete followed by the rule specification, either by number (after listing rules with sudo ufw status numbered) or by the rule itself (e.g., sudo ufw delete allow http).</li> </ul> <p>Troubleshooting with UFW</p> <ul> <li>Firewall Logs: UFW logs activities which can be helpful for troubleshooting. Logs are typically found in /var/log/ufw.log. Viewing these logs can provide insights into which packets were blocked or allowed, and why.</li> <li>Resetting UFW: If you encounter issues or wish to start over, you can reset UFW to its default settings with sudo ufw reset. This action clears all active rules and disables the firewall, allowing you to rebuild your rule set from scratch.</li> <li>Disabling UFW: If you need to temporarily disable the firewall for troubleshooting or setup purposes, you can do so with sudo ufw disable. Remember to enable it again with sudo ufw enable once you're done.</li> </ul> <p>Best Practices</p> <ul> <li>Minimum Necessary Access: Only allow traffic necessary for your applications to function. Limiting access reduces the potential for unauthorized access.</li> <li>Regularly Review Rules: Over time, the needs of your system may change. Regularly review and update your firewall rules to ensure they reflect your current requirements.</li> <li>Use Application Profiles: UFW supports application profiles which can simplify rule management. These profiles allow you to define rules based on the application rather than specific port numbers.</li> </ul> <p>By following this guide, you can effectively manage your system\u2019s firewall with UFW, enhancing your Linux system's security through a straightforward and accessible interface. Whether you're running a home server or managing enterprise systems, UFW provides the tools needed to protect your network.</p>"},{"location":"fundamentals/operating-systems/linux/install/ubuntu-cloud-init/","title":"Cloud-init Installation Guide","text":"<p>Cloud-init is a powerful tool for automating the cloud instance initialization process. It's essential for configuring instances with user data upon their first boot, streamlining deployments in cloud environments like AWS, Azure, and Google Cloud Platform.</p>"},{"location":"fundamentals/operating-systems/linux/install/ubuntu-cloud-init/#understanding-cloud-init","title":"Understanding Cloud-init","text":"<p>Cloud-init automates the system initialization process in cloud instances by reading user data from the instance's metadata and executing specified initialization modules. This includes tasks like setting up users, installing packages, writing files, and configuring network settings.</p>"},{"location":"fundamentals/operating-systems/linux/install/ubuntu-cloud-init/#installation","title":"Installation","text":""},{"location":"fundamentals/operating-systems/linux/install/ubuntu-cloud-init/#ubuntu-or-debian-based-systems","title":"Ubuntu or Debian-based Systems","text":"<pre><code># Update package lists\nsudo apt-get update\n\n# Install cloud-init\nsudo apt-get install cloud-init\n</code></pre>"},{"location":"fundamentals/operating-systems/linux/install/ubuntu-cloud-init/#centos-rhel-or-fedora-systems","title":"CentOS, RHEL, or Fedora Systems","text":"<pre><code># Update your system\nsudo yum update\n\n# Install cloud-init\nsudo yum install cloud-init\n</code></pre>"},{"location":"fundamentals/operating-systems/linux/install/ubuntu-cloud-init/#configuration","title":"Configuration","text":"<p>Cloud-init configurations are primarily located in <code>/etc/cloud/cloud.cfg</code> and <code>/etc/cloud/cloud.cfg.d/</code>. Customize these files to configure default user setups, network configurations, and more.</p>"},{"location":"fundamentals/operating-systems/linux/install/ubuntu-cloud-init/#usage","title":"Usage","text":"<p>To utilize cloud-init, provide user data to your cloud instance through the cloud provider's management console. User data can be shell scripts or cloud-init directives.</p>"},{"location":"fundamentals/operating-systems/linux/install/ubuntu-cloud-init/#example-user-data-script","title":"Example User Data Script","text":"<pre><code>#cloud-config\npackages:\n  - nginx\n\nwrite_files:\n  - path: /var/www/html/index.html\n    content: |\n      Welcome to my cloud instance!\n\nruncmd:\n  - systemctl start nginx\n  - systemctl enable nginx\n</code></pre> <p>This script installs nginx, creates a custom <code>index.html</code> file, and starts the nginx service.</p>"},{"location":"fundamentals/operating-systems/linux/install/ubuntu-cloud-init/#testing-cloud-init","title":"Testing Cloud-init","text":"<p>After setting up cloud-init and providing user data, launch your cloud instance. Verify the initialization process by checking <code>/var/log/cloud-init.log</code>.</p>"},{"location":"fundamentals/operating-systems/linux/install/ubuntu-cloud-init/#troubleshooting","title":"Troubleshooting","text":"<p>For any issues, refer to the cloud-init log at <code>/var/log/cloud-init.log</code>, which provides detailed execution logs and errors.</p> <p>Cloud-init simplifies the process of managing cloud instances by automating their setup, ensuring they're ready for use immediately after launch.</p>"},{"location":"fundamentals/operating-systems/linux/tshoot/grub/","title":"GRUB recovery","text":"<p>Warning</p> <p>When facing issues with GRUB not initiating your operating system, it usually traces back to interference from another OS's bootloader, especially in dual-boot     configurations, or from accidentally deleted GRUB configuration files. These conflicts can lead to GRUB failing to load the system, resulting in errors like \"no such     partition\" or \"unknown filesystem,\" and presenting you with the GRUB Rescue prompt or just the GRUB prompt. Understanding GRUB Rescue Commands</p> <p>In the event of a boot issue, the GRUB Rescue prompt becomes a crucial tool for troubleshooting and fixing boot problems. Here's a concise guide to some essential GRUB Rescue commands you might need:</p> <ul> <li>boot: Initiates the boot process. This command doesn't take any arguments.</li> <li>cat: Outputs the content of a specified file.</li> <li>configfile: Loads a configuration file, allowing GRUB to read its settings.</li> <li>initrd: Loads the initial ramdisk image.</li> <li>insmod: Inserts a module into the GRUB boot process.</li> <li>loopback: Attaches an image file as a loop device.</li> <li>ls: Lists contents of a directory or shows available partitions.</li> <li>lsmod: Lists all loaded GRUB modules.</li> <li>normal: Switches to normal mode from rescue mode.</li> <li>search: Searches for a specific device, file, label, or filesystem UUID.</li> <li>set: Assigns or shows environment variables.</li> </ul>"},{"location":"fundamentals/operating-systems/linux/tshoot/grub/#resolving-grub-boot-failures","title":"Resolving GRUB Boot Failures","text":"<p>To troubleshoot and fix GRUB boot failures, you can use the GRUB Rescue prompt or employ a Boot Repair tool. Here's how you can attempt to fix these issues through the GRUB Rescue prompt:</p> <ol> <li> <p>Identify Environment Variables: Use set to list current environment settings. This helps in identifying which partition GRUB expects to boot from.</p> </li> <li> <p>List Partitions: Utilize ls to display partitions available on the disk, aiding in locating your system's boot partition.</p> </li> <li> <p>Find Boot Directory: With ls [partition-name], navigate to find which partition contains the /boot directory.</p> </li> <li> <p>Define Boot Partition: Once identified, use set root=(hd0,msdos1) to specify the boot partition.</p> </li> <li> <p>Load Normal Module: Employ insmod normal to load the normal boot module.</p> </li> <li> <p>Enter Normal Boot Mode: By executing normal, you transition into a mode where more complex commands can be issued.</p> </li> <li> <p>Load the Kernel: Use the linux command to specify the kernel to boot, along with any necessary parameters.</p> </li> <li> <p>Boot the System: Finally, the boot command initiates the system boot with the provided settings.</p> </li> </ol> <p>Through these steps, you're not just troubleshooting but also gaining a deeper understanding of how GRUB manages the boot process, providing you with the knowledge to resolve similar issues in the future. This method is straightforward and doesn't require external tools, making it an efficient solution to common GRUB boot problems.</p>"},{"location":"fundamentals/operating-systems/linux/tshoot/root-password-recovery/","title":"Root password recovery","text":"<p>Resetting a Linux password can be crucial if you've forgotten it or need to access a system without the current user's credentials. There are two primary methods to accomplish this: through the GRUB bootloader or using a Live CD/USB. Here's a comprehensive guide on how to execute both procedures, ensuring you can regain access to your Linux system.</p>"},{"location":"fundamentals/operating-systems/linux/tshoot/root-password-recovery/#via-grub-bootloader","title":"Via GRUB Bootloader","text":"<p>The GRUB (Grand Unified Bootloader) provides a way to regain access by booting into a special mode where you can reset a user's password.</p> <ol> <li> <p>Access GRUB Menu: Start or restart your computer. Immediately press the Shift (or Esc in some cases) key to open the GRUB menu.</p> </li> <li> <p>Edit Boot Parameters: Navigate to the default boot entry using the arrow keys and press 'E' to edit it.     </p> </li> <li> <p>Boot into Single-User Mode: Look for the line beginning with linux or linux16. Change ro quiet to rw and append single or init=/bin/bash. Confirm and boot with these parameters by pressing Ctrl + X or F10.</p> </li> <li> <p>Enable Write Permissions: To modify the password, the filesystem must be writable. Execute mount -n -o remount,rw /.</p> </li> <li> <p>Change the Password: Use passwd , substituting  with the actual user's name. Follow the prompts to set a new password. <li> <p>Reboot the System: Ensure changes are written and reboot with sync followed by reboot -f.</p> </li>"},{"location":"fundamentals/operating-systems/linux/tshoot/root-password-recovery/#using-a-live-cdusb","title":"Using a Live CD/USB","text":"<p>If the GRUB method is not an option, a Live CD/USB provides an alternative approach.</p> <ol> <li> <p>Boot from Live Media: Insert the Live CD/USB and select it as the boot device during startup.</p> </li> <li> <p>Access a Terminal: In the live environment, open a terminal window.</p> </li> <li> <p>Locate the Root Partition: Use sudo fdisk -l or sudo lsblk -f to find the partition that contains the Linux system.</p> </li> <li> <p>Mount the Root Partition: Create a directory for mounting, e.g., sudo mkdir /mnt/myroot, and mount the partition (sudo mount /dev/sdXY /mnt/myroot).</p> </li> <li> <p>Change Root Directory: Enter into your system's environment with sudo chroot /mnt/myroot.</p> </li> <li> <p>Reset User Password: Use passwd , replacing  with the name of the user or root to change the root password. <li> <p>Cleanup: Exit the chroot environment with exit and unmount the partition with sudo umount /mnt/myroot.</p> </li> <li> <p>Restart: Remove the Live media and reboot. You should now be able to log in with the new password.</p> </li> <p>By following these detailed steps, you can reset a Linux user's password using either the GRUB method for quick fixes or the Live CD/USB method for more complex situations or when GRUB access is restricted. Both approaches are effective for regaining access to your Linux system.</p>"},{"location":"fundamentals/operating-systems/windows/config/active-directory/","title":"Windows Server 2019 Active Directory Configuration Guide","text":"<p>This guide outlines the steps for setting up and managing Active Directory in Windows Server 2019, a crucial component for network resource management in an enterprise setting.</p>"},{"location":"fundamentals/operating-systems/windows/config/active-directory/#installation-of-windows-server-2019","title":"Installation of Windows Server 2019","text":""},{"location":"fundamentals/operating-systems/windows/config/active-directory/#step-1-install-the-server-software","title":"Step 1: Install the Server Software","text":"<p>Begin by deploying Windows Server 2019 on your server hardware. Follow Microsoft's guided installation process to ensure a smooth setup.</p>"},{"location":"fundamentals/operating-systems/windows/config/active-directory/#promoting-server-to-domain-controller","title":"Promoting Server to Domain Controller","text":""},{"location":"fundamentals/operating-systems/windows/config/active-directory/#step-2-activate-domain-controller-role","title":"Step 2: Activate Domain Controller Role","text":"<p>After the OS installation, proceed to integrate Active Directory Domain Services (AD DS) to transform the server into a domain controller.</p> <ul> <li>Launch Server Manager from the Start menu, navigating to Manage &gt; Add Roles and Features.</li> <li>Proceed with the wizard, selecting Active Directory Domain Services for installation.</li> <li>Upon completion, specify your domain details and Directory Service Restore Mode (DSRM) password.</li> <li>Restart the server as prompted to finalize the role addition.</li> </ul>"},{"location":"fundamentals/operating-systems/windows/config/active-directory/#active-directory-setup","title":"Active Directory Setup","text":""},{"location":"fundamentals/operating-systems/windows/config/active-directory/#step-3-configuring-the-directory","title":"Step 3: Configuring the Directory","text":"<p>With the server now a domain controller, it's time to configure Active Directory for your network's needs.</p> <ul> <li>Access Active Directory Administrative Center via the Start menu.</li> <li>For a new setup, choose Create a new domain in a new forest and input your desired forest name.</li> <li>Adjust the forest and domain functional levels as necessary.</li> <li>Confirm DNS settings and proceed with the domain controller options.</li> <li>Review and initiate the configuration, following the wizard's prompts.</li> </ul>"},{"location":"fundamentals/operating-systems/windows/config/active-directory/#administration-and-management","title":"Administration and Management","text":""},{"location":"fundamentals/operating-systems/windows/config/active-directory/#step-4-managing-network-resources","title":"Step 4: Managing Network Resources","text":"<p>With Active Directory fully operational, the focus shifts to managing network users, groups, and resources.</p>"},{"location":"fundamentals/operating-systems/windows/config/active-directory/#user-and-group-management","title":"User and Group Management","text":"<ul> <li>Utilize the Active Directory Users and Computers console to add or modify user accounts and groups, adjusting permissions and security settings as needed.</li> </ul>"},{"location":"fundamentals/operating-systems/windows/config/active-directory/#policy-administration","title":"Policy Administration","text":"<ul> <li>Apply Group Policy Management for setting up policies that govern user and computer behaviors across the network.</li> </ul>"},{"location":"fundamentals/operating-systems/windows/config/active-directory/#resource-sharing","title":"Resource Sharing","text":"<ul> <li>Leverage Active Directory Users and Computers to set up shared network resources, ensuring proper access control and resource allocation.</li> </ul>"},{"location":"fundamentals/operating-systems/windows/config/active-directory/#conclusion","title":"Conclusion","text":"<p>By following these structured steps, Active Directory on Windows Server 2019 will be ready to serve your organization's needs, offering a robust framework for managing users, groups, and resources efficiently. Explore further to take full advantage of the comprehensive features available in Active Directory.</p>"},{"location":"fundamentals/operating-systems/windows/install/wserver2019/","title":"Windows Server 2019","text":""},{"location":"fundamentals/operating-systems/windows/install/wserver2019/#quick-installation-guide-for-windows-server-2019","title":"Quick Installation Guide for Windows Server 2019","text":"<ol> <li> <p>Prepare Your Server:</p> <p>Check minimum hardware requirements: 1.4 GHz 64-bit CPU, 512 MB RAM (2 GB for Desktop Experience), 32 GB disk space. Have your Windows Server 2019 media ready (USB/DVD).</p> </li> </ol> <ul> <li> Windows Server 2019 download ISO</li> </ul> <ol> <li> <p>Install Windows Server 2019:</p> <p>Boot from the installation media. Select language, time, and keyboard, then click \"Next\" &gt; \"Install now\". Enter your product key or select your edition. Accept the license terms. Choose \u201cCustom: Install Windows only (advanced)\u201d for a clean install. Select the disk where you want to install and follow prompts to complete installation.</p> </li> <li> <p>Post-Installation Setup:</p> <p>Set a strong administrator password. Configure network settings (static IP recommended). Rename your server for easy identification. Activate Windows Server. Install necessary roles and features via Server Manager or PowerShell. Apply all critical Windows updates.</p> </li> </ol> <p>Best Practices After Installation</p> <ol> <li> <p>Security:</p> <p>Regularly update Windows Server. Enable Windows Defender or similar for malware protection. Configure Windows Firewall properly.</p> </li> <li> <p>Performance and Maintenance:</p> <p>Install only necessary roles/features to reduce vulnerabilities. Set up regular backups to prevent data loss. Monitor server performance using built-in tools like Performance Monitor.</p> </li> <li> <p>Remote Management:</p> <p>Enable Remote Desktop for easier management, but ensure it's secured properly.</p> </li> <li> <p>Documentation:</p> <p>Keep a record of your server setup and configurations for troubleshooting and auditing purposes.</p> </li> </ol> <p>This streamlined guide covers the essentials to get your Windows Server 2019 up and running securely and efficiently. Regular maintenance and monitoring are key to a stable server environment.</p>"},{"location":"fundamentals/operating-systems/windows/install/wserver2019/#for-virtualization-in-proxmox","title":"For virtualization in Proxmox","text":"<p>Install also virtio for hardware driver detection:</p> <ul> <li> Proxmox docs for VirtIO Drivers</li> <li> VirtIO Drivers  Download</li> </ul>"},{"location":"fundamentals/operating-systems/windows/tshoot/dns/","title":"Efficient DNS Troubleshooting in Active Directory","text":"<p>Active Directory (AD) and the Domain Name System (DNS) work hand in hand to ensure smooth network operations. Correct DNS functioning is essential for AD-related tasks like locating domain controllers, authentication, and replication. This guide focuses on technical troubleshooting techniques and essential commands for DNS issues in an AD environment.</p>"},{"location":"fundamentals/operating-systems/windows/tshoot/dns/#key-dns-troubleshooting-steps","title":"Key DNS Troubleshooting Steps","text":""},{"location":"fundamentals/operating-systems/windows/tshoot/dns/#verify-dns-records-and-configuration","title":"Verify DNS Records and Configuration","text":"<ul> <li> <p>DNS Settings Check: Ensure DNS servers in network settings point to AD-integrated DNS servers. Use <code>ipconfig /all</code> to review.</p> </li> <li> <p>SRV Records Validation: Confirm SRV records for AD are correctly registered with <code>nslookup -type=srv _ldap._tcp.dc._msdcs.&lt;YourDomain.com&gt;</code>.</p> </li> <li> <p>Dynamic Updates Status: Check if dynamic updates are enabled in your DNS zones, allowing automatic DNS record registration.</p> </li> </ul>"},{"location":"fundamentals/operating-systems/windows/tshoot/dns/#monitoring-and-logging","title":"Monitoring and Logging","text":"<ul> <li> <p>Enable Debug Logging: Activate DNS debug logging to track queries and responses, helping identify issues. Be mindful of the potential impact on performance.</p> </li> <li> <p>Performance Monitoring: Utilize performance counters to monitor DNS operations, focusing on query processing times and response rates.</p> </li> </ul>"},{"location":"fundamentals/operating-systems/windows/tshoot/dns/#zone-health-and-replication","title":"Zone Health and Replication","text":"<ul> <li> <p>Replication Checks: Use <code>repadmin /replsummary</code> and <code>dcdiag /test:dns</code> to assess AD replication and DNS health.</p> </li> <li> <p>Aging and Scavenging: Verify settings to ensure outdated records are cleaned up without affecting necessary ones.</p> </li> </ul>"},{"location":"fundamentals/operating-systems/windows/tshoot/dns/#practical-dns-maintenance-tips","title":"Practical DNS Maintenance Tips","text":"<ul> <li> <p>Regular DNS Records Review: Periodically audit DNS records for accuracy, removing or correcting as necessary.</p> </li> <li> <p>DNSLint Utility: Employ Microsoft's DNSLint for diagnosing DNS and AD issues.</p> </li> <li> <p>Best Practices Implementation: Maintain DNS server redundancy, separate AD from other services, and keep DNS server software updated.</p> </li> </ul>"},{"location":"fundamentals/operating-systems/windows/tshoot/dns/#advanced-dns-features-for-ad","title":"Advanced DNS Features for AD","text":"<ul> <li> <p>Conditional Forwarders: Implement for efficient resolution in multi-domain environments.</p> </li> <li> <p>DNSSEC: Consider DNS Security Extensions for added security, especially for internet-facing AD setups.</p> </li> <li> <p>GlobalNames Zone: Enable for simplified access to resources via single-label DNS names.</p> </li> </ul>"},{"location":"fundamentals/operating-systems/windows/tshoot/dns/#conclusion","title":"Conclusion","text":"<p>Proactive DNS management and troubleshooting are crucial for maintaining a robust AD environment. By applying these focused techniques and utilizing the recommended commands, administrators can ensure optimal DNS performance, enhancing overall network health and security.</p>"},{"location":"homelab/ansible/","title":"Ansible","text":"<p>In the realm of DevOps and automation, I've embarked on a journey exploring various configuration management and orchestration tools. Over the years, I've worked with Puppet and Salt for personal and professional projects, each offering unique insights into the automation landscape. Recently, my homelab has seen a new addition: Ansible.</p>"},{"location":"homelab/ansible/#embracing-ansible-in-my-homelab","title":"Embracing Ansible in My Homelab","text":"<p>Ansible has become my go-to tool for automating and managing my homelab infrastructure. Its simplicity and agentless architecture make it an appealing choice for quick wins in automation without the overhead of managing agents on nodes. Here, I'll share insights into two of the playbooks I've employed to streamline my setup.</p>"},{"location":"homelab/ansible/#a-glimpse-into-my-ansible-playbooks","title":"A Glimpse into My Ansible Playbooks<sup>1</sup>","text":"<p>Playbooks</p> LinuxWindowsVMs <p>For Debian/Ubuntu systems, it leverages the apt module to update the cache, upgrade all packages, and clean up. For CentOS/RHEL systems, it uses the yum module for similar tasks, ensuring my entire Linux fleet remains secure and efficient.</p> <p>The upgrade_windows.yml playbook is tailored specifically for Windows servers in my homelab, utilizing the win_updates module to fetch and install updates across various categories, including security and critical updates. It's designed to run on a schedule, ensuring my Windows environments are always running the latest updates without manual intervention.</p> <p>Venturing into virtualization, the create_vm_proxmox.yml playbook automates the creation of VMs in Proxmox VE. This playbook defines a new VM with specific resources, like CPU, memory, and storage, and attaches an Ubuntu ISO for installation. It exemplifies how Ansible can interact with virtualization environments to streamline VM deployment.</p>"},{"location":"homelab/ansible/#automation-with-crontab","title":"Automation with Crontab","text":"<p>To ensure these playbooks run regularly and keep my systems in peak condition, I've scheduled them with crontab entries. This setup automates weekly updates for both Linux and Windows servers, with output and errors redirected to specific files for easy monitoring: <pre><code>0 0 * * 0 /usr/local/bin/ansible-playbook /home/merox/playbooks/upgrade.yml -i /home/merox/playbooks/hosts.ini &gt;&gt; /home/merox/playbooks/last_update.txt 2&gt;&gt; /home/merox/playbooks/last_update.err\n0 0 * * 0 /usr/local/bin/ansible-playbook -i /home/merox/playbooks/hosts_windows.ini -T 60 /home/merox/playbooks/upgrade_windows.yml &gt;&gt; /home/merox/playbooks/last_windows_update.txt 2&gt;&gt; /home/merox/playbooks/last_windows_update.err\n</code></pre></p>"},{"location":"homelab/ansible/#continuous-learning-and-the-path-forward-with-ansible","title":"Continuous Learning and the Path Forward with Ansible","text":"<p>As I venture deeper into the world of DevOps with my homelab, I recognize that my journey with Ansible is just beginning. Despite the initial strides I've made, there's a vast landscape of knowledge and skills yet to be explored. Ansible, with its powerful capabilities and simplicity, has opened a new chapter in my automation endeavors, one where I'm both a learner and an explorer.</p>"},{"location":"homelab/ansible/#the-road-ahead","title":"The Road Ahead","text":"<p>As I continue to build, automate, and refine my homelab with Ansible, the journey is as much about acquiring new skills as it is about applying them. The beauty of Ansible lies not just in its technical prowess but in its community, resources, and the continuous evolution of its ecosystem.</p> <p>The exploration doesn't end with mastering Ansible. The DevOps landscape is ever-changing, and with tools like Terraform on the horizon, the integration of configuration management and infrastructure provisioning is an exciting prospect. The journey ahead promises a blend of challenges and opportunities, pushing the boundaries of what's possible in my homelab and beyond.</p>"},{"location":"homelab/ansible/#configs","title":"Configs","text":"<ul> <li> Linux playbook for automatic linux distros upgrade</li> <li> Windows playbook  for automatic windows upgrade</li> <li> Proxmox playbook for automatic VM creation</li> <li> <p> More playbooks ... soon</p> </li> </ul> <ol> <li> <p>Playbooks configuration\u00a0\u21a9</p> </li> </ol>"},{"location":"homelab/connectivity/","title":"Connectivity","text":""},{"location":"homelab/connectivity/#connectivity-infrastructure-overview","title":"Connectivity Infrastructure Overview","text":"<p>Gateway to the Internet : The setup begins with a Huawei Optical Network Terminal (ONT) from Orange, in bridge mode, forwarding a public IP address to the pfSense router, marking the gateway to the internet.</p>"},{"location":"homelab/connectivity/#core-network-services","title":"Core Network Services","text":""},{"location":"homelab/connectivity/#security-and-management-core","title":"Security and Management Core","text":"<ul> <li>Firewall: Filters traffic based on security rules.</li> <li>pfBlockerNG: Blocks ads and malicious sites \ud83d\udee1\ufe0f.</li> <li>DHCP &amp; DNS Services: Assigns IP addresses and resolves DNS queries, with Unbound as a secondary DNS resolver \ud83d\udd04.</li> <li>Intrusion Detection with Snort: Monitors for security threats.</li> <li>Remote Accessibility: Enabled through WoL and VPNs (IPsec &amp; OpenVPN), offering remote access \ud83c\udf0d.</li> </ul>"},{"location":"homelab/connectivity/#vlan-management","title":"VLAN Management","text":"<p>A TP-Link managed switch facilitates VLAN segmentation, with devices organized within VLAN57 for streamlined network management.</p>"},{"location":"homelab/connectivity/#kubernetes-and-traefik","title":"Kubernetes and Traefik","text":"<p>The Kubernetes Ingress Controller, using Traefik, is pivotal for external access management, simplifying service deployment and routing.</p> <p></p> <p>Danger</p> <p>A virtual machine running Kali Linux features for network scanning and vulnerability assessments, indicating a strong focus on security. This setup will be detailed further in the Security section.</p> <p>Light</p> <p>In summary, the network is designed for security, efficiency, and scalability. Integrating both traditional and modern technologies, such as Traefik, ensures the lab is prepared for current needs and future growth.</p>"},{"location":"homelab/connectivity/#configs","title":"Configs","text":"<p>See the network configurations of my homelab</p> <ul> <li> DNS for domain to IP translation</li> <li> DHCP  for automatic IP assignment</li> <li> Traefik for routing and load balancing</li> <li> <p> More configs ... soon</p> </li> </ul>"},{"location":"homelab/docker/","title":"Docker","text":"<ul> <li> <p>In the dynamic world of PulsarDC, Docker containers play a pivotal role in my virtual machine \"alto\", simplifying application deployment and management. To streamline these operations, I employ Portainer, an intuitive UI for Docker container management. Here's an overview of my active Docker containers and their functions:</p> MonitoringMediaOthers </li> </ul>"},{"location":"homelab/docker/#monitoring","title":"Monitoring","text":""},{"location":"homelab/docker/#alertmanager","title":"\ud83d\udea8 alertmanager","text":"<p>Manages and forwards alerts generated by monitoring tools, ensuring timely notifications for system administrators.</p>"},{"location":"homelab/docker/#cadvisor","title":"\ud83d\udcca cadvisor","text":"<p>Provides analytics and performance metrics for running containers, aiding in resource optimization and monitoring.</p>"},{"location":"homelab/docker/#grafana","title":"\ud83d\udcc8 grafana","text":"<p>A powerful analytics and monitoring solution that visualizes time series data, perfect for observing system performance metrics.</p>"},{"location":"homelab/docker/#prometheus","title":"\ud83d\udd0d prometheus","text":"<p>An open-source monitoring system with a dimensional data model, flexible query language, and alerting functionality.</p>"},{"location":"homelab/docker/#node-exporter","title":"\ud83d\udce1 node-exporter","text":"<p>Collects and exposes a wide range of hardware and OS metrics with Prometheus, crucial for system monitoring.</p>"},{"location":"homelab/docker/#media-server","title":"Media Server","text":""},{"location":"homelab/docker/#jellyfin","title":"\ud83c\udf7f jellyfin","text":"<p>An open-source media system that organizes, manages, and streams your digital media to various devices.</p>"},{"location":"homelab/docker/#radarr","title":"\ud83c\udfac radarr","text":"<p>An independent fork of Sonarr reworked for automatically downloading movies via Usenet and BitTorrent.</p>"},{"location":"homelab/docker/#sonarr","title":"\ud83d\udcfa sonarr","text":"<p>Automates the finding, downloading, and naming of TV shows, integrating with Usenet and BitTorrent sources.</p>"},{"location":"homelab/docker/#jackett","title":"\ud83c\udfa5 jackett","text":"<p>Acts as a proxy server, translating queries from other applications to search over 100 torrent sites, facilitating media acquisition.</p>"},{"location":"homelab/docker/#others","title":"Others","text":""},{"location":"homelab/docker/#cloudflare_tunnel","title":"\ud83c\udf10 cloudflare_tunnel","text":"<p>Secures and accelerates internet traffic to our services through Cloudflare's network, enhancing privacy and speed.</p>"},{"location":"homelab/docker/#influxdb","title":"\ud83c\udf0a influxdb","text":"<p>Tailored for storing data from pfSense into Grafana, enabling detailed network traffic analysis and visualization.</p>"},{"location":"homelab/docker/#configs","title":"Configs","text":"<p>See the docker-compose configurations from my homelab</p> <ul> <li> Docker for containerization and application packaging</li> </ul>"},{"location":"homelab/hypervisors/","title":"Hypervisors","text":"<p>Welcome to the core of my homelab's virtualization infrastructure at Merox Cloud! I'm thrilled to share with you the details of the powerful Proxmox hypervisors that keep my virtual machines (VMs) and containers (LXCs) operating efficiently. Join me as we explore PulsarDC, my Proxmox cluster, and uncover the technology driving my homelab network.</p> <p>Info</p> <p>Proxmox environment is bolstered by three robust servers, each housed in its own mini-PC, known as Citadel, Helix, and Nexus. These servers are the pillars of the PulsarDC cluster, bringing high availability and flexibility to our virtualized infrastructure.</p>"},{"location":"homelab/hypervisors/#cluster-overview","title":"Cluster Overview","text":"<p> Proxmox cluster</p>"},{"location":"homelab/hypervisors/#ha","title":"HA","text":"<ul> <li> <p>High Availability Proxmox Servers: Citadel, Helix, and Nexus form the resilient backbone of our PulsarDC cluster, ensuring our applications and services are always up and running.</p> </li> <li> <p>Dedicated Network Segmentation: Our Proxmox servers are part of a distinct network segment, isolated from other devices in our homelab. This setup enhances security and performance, ensuring that our virtualization infrastructure operates in an optimized environment.</p> </li> </ul> <p> High Availability in Proxmox cluster</p>"},{"location":"homelab/hypervisors/#storage","title":"Storage","text":"<ul> <li> <p>NVME Storage with LVM: Each server boasts an NVME drive for the operating system, utilizing Logical Volume Management (LVM). This setup offers fast boot times and efficient storage management, key for high-performance virtualization.</p> </li> <li> <p>ZFS Replication: For data integrity and disaster recovery, our servers utilize ZFS Replication. This technology ensures our data is mirrored across the cluster, providing an extra layer of protection against data loss.</p> </li> <li> <p>NFS Backup Storage on Synology NAS: A crucial aspect of our virtualization infrastructure is our backup strategy. We utilize an NFS-mounted storage location on our Synology NAS for all our backup needs. This setup allows us to store backups of VMs and containers according to our defined policies, ensuring data safety and quick recovery in case of any failure.</p> </li> </ul> <p> Proxmox storage</p>"},{"location":"homelab/hypervisors/#network","title":"Network","text":"<p>1 Gb/s Ethernet Connection: Connectivity is key in a homelab environment. Each of our Proxmox servers is equipped with a 1GB/s Ethernet connection, ensuring speedy and reliable network communication.</p> <p>VM and LXC Support: Our cluster hosts a variety of Virtual Machines (VMs) and Linux Containers (LXCs), catering to a wide range of applications and services. These are seamlessly integrated into our network through bridge connections, allowing direct IP allocation from our DHCP pool.</p> <p>Seamless pfSense Integration: Connection to our pfSense gateway is handled with ease, thanks to the bridge mode setup. This allows for straightforward management of network traffic and security, providing IPs directly from our DHCP pool to VMs and LXCs.</p> <p> Proxmox network</p>"},{"location":"homelab/hypervisors/#vm-lxc-configs","title":"VM &amp; LXC configs","text":"<p>VM &amp; LXC Configurations Overview</p> <p>In modern infrastructures, VMs and LXCs are essential for running services and applications. Below is a streamlined overview of the configurations for VMs and LXCs on different servers, focusing on Kubernetes (K3S) clusters and other services.</p> <p>Cluster nodes</p> CitadelHelixNexus <p>VMs</p> <p>K3S-01: A VM with 16GB RAM, 2 CPU cores, and 128GB SSD for running a K3S node, utilizing cloud-init for setup and emphasizing security with pre-configured SSH access.</p> <p>LXC</p> <p>K3S-master-01 &amp; K3S: Containers designated for Kubernetes master nodes, each with 4GB RAM and configured for optimal Kubernetes performance, including necessary    adjustments for logging and device management.</p> <p>VMs</p> <p>K3S-02: Similar to K3S-01, ensuring a distributed K3S cluster setup across servers.</p> <p>LXC</p> <p>K3S-master-02, Ansible, and Alto: The master node mirrors Citadel's setup, while Ansible focuses on automation with minimal resources, and Alto is optimized for Docker     with enhanced features like NFS mounts and nesting.</p> <p>VMs</p> <p>K3S-03: A K3S VM with slightly less RAM, maintaining the cluster's resilience and efficiency.</p> <p>Windows AD: A VM dedicated to running Windows Active Directory, serving as the backbone for network authentication and policy management across the infrastructure. This VM is crucial for integrating Windows-based services and managing user access in a secure manner.</p> <p>Warning</p> <p>Key Configuration Insights Both VMs and LXCs are fine-tuned for Kubernetes, including logging enhancements and network optimizations. Adjustments such as disabling AppArmor and modifying cgroup settings are crucial for Kubernetes compatibility. The configurations across servers demonstrate a balanced approach to virtualization, ensuring a scalable, secure, and efficient Kubernetes infrastructure.</p>"},{"location":"homelab/hypervisors/#configs","title":"Configs","text":"<p>See the VM and LXC configurations of my homelab</p> <ul> <li> VMs for full-system emulation and isolation</li> <li> LXCs  for process and environment isolation</li> </ul>"},{"location":"homelab/infrastructure/","title":"Infrastructure","text":"<p>Info 1</p> <p>This document outlines the infrastructure framework for a personal IT project, meticulously designed to ensure a blend of efficiency, scalability, and security. It represents the foundational architecture, encompassing network design, data management strategies, computational resources, and security protocols. </p> <p>Info 2</p> <p>Aimed at supporting the project's unique requirements, this infrastructure is crafted to facilitate optimal performance, reliability, and adaptability, reflecting a commitment to leveraging advanced technologies and methodologies for achieving project objectives.</p>"},{"location":"homelab/infrastructure/#topology","title":"Topology","text":"<ul> <li> <p>Below, you can see both a diagram of my homelab and a photo of it.</p> DiagramPhoto <p></p> <p></p> </li> </ul>"},{"location":"homelab/infrastructure/#network","title":"Network","text":"<ul> <li>Description: The pfSense stands as a pivotal security gateway in the homelab setup, providing robust network protection, traffic management, and connectivity options. Tailored for efficient performance and reliability, this device is essential for maintaining a secure and efficient network environment.</li> </ul>"},{"location":"homelab/infrastructure/#network-flow-diagram","title":"Network flow diagram","text":"<ul> <li>Below you can see how a internet request for this website it's done in my homelab:</li> </ul>"},{"location":"homelab/infrastructure/#technical-specs","title":"Technical Specs","text":"<ul> <li>router pfSense: XCY X44</li> <li>CPU: Dual-core, supporting 1 package(s) x 2 core(s) configuration</li> <li>RAM: 8GB, ensuring smooth operation of network services and applications</li> <li>Storage: 120GB ZFS, offering reliable and resilient data storage capabilities</li> <li>Network Interfaces: <pre><code>igb0: flags=8963&lt;UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; metric 0 mtu 1500\n        description: WAN\n        options=4e120bb&lt;RXCSUM,TXCSUM,VLAN_MTU,VLAN_HWTAGGING,JUMBO_MTU,VLAN_HWCSUM,WOL_MAGIC,VLAN_HWFILTER,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP&gt;\n        ether 00:b5:13:fe:12:39\n        inet X.X.X.X netmask 0xffffffc0 broadcast X.X.X.255\n        media: Ethernet autoselect (1000baseT &lt;full-duplex&gt;)\n        status: active\n        nd6 options=23&lt;PERFORMNUD,ACCEPT_RTADV,AUTO_LINKLOCAL&gt;\nigb1: flags=8863&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; metric 0 mtu 1500\n        description: LAN\n        options=4e120bb&lt;RXCSUM,TXCSUM,VLAN_MTU,VLAN_HWTAGGING,JUMBO_MTU,VLAN_HWCSUM,WOL_MAGIC,VLAN_HWFILTER,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP&gt;\n        ether 00:a5:27:e0:2f:ba\n        inet6 fe80::2a5:27ff:fee0:2fba%igb1 prefixlen 64 scopeid 0x2\n        inet6 fe80::1:1%igb1 prefixlen 64 scopeid 0x2\n        inet 192.168.57.1 netmask 0xffffff00 broadcast 192.168.57.255\n        media: Ethernet autoselect (1000baseT &lt;full-duplex&gt;)\n        status: active\n        nd6 options=21&lt;PERFORMNUD,AUTO_LINKLOCAL&gt;\nigb2: flags=8863&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; metric 0 mtu 1500\n        description: OPT1\n        options=4e120bb&lt;RXCSUM,TXCSUM,VLAN_MTU,VLAN_HWTAGGING,JUMBO_MTU,VLAN_HWCSUM,WOL_MAGIC,VLAN_HWFILTER,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP&gt;\n        ether 00:a5:27:e0:2f:bb\n        inet6 fe80::2a5:27ff:fee0:2fbb%igb2 prefixlen 64 scopeid 0x3\n        inet 192.168.97.1 netmask 0xffffff00 broadcast 192.168.97.255\n        media: Ethernet autoselect (1000baseT &lt;full-duplex&gt;)\n        status: active\n        nd6 options=21&lt;PERFORMNUD,AUTO_LINKLOCAL&gt;`\n</code></pre></li> </ul>"},{"location":"homelab/infrastructure/#storage","title":"Storage","text":"<ul> <li>The storage framework within this homelab is engineered for robustness, leveraging a mix of hardware and software to ensure data safety, performance, and scalability. Here's a concise overview of the storage configuration:</li> </ul> <p>More info</p> SynologyLonghornDisksBackup <p>NFS Share on Synology NAS: Utilizing two HDDs in RAID1, this setup provides reliable, redundant storage, central to the Proxmox hypervisors for VM and essential data   storage.</p> <p>Kubernetes Cluster Storage:     Combines the Synology NAS NFS share for persistent storage and Longhorn for dynamic, scalable storage within the Kubernetes environment.</p> <p>Proxmox Node Storage:     NVMe Drives (128GB): Host the Proxmox OS, ensuring quick system operations, with regular backups via Clonezilla for system recovery.     SSD Storage (512GB): Employs ZFS replication for high availability and data integrity, supporting critical VMs and containers.</p> <p>Backup Strategy: Implements HyperBackup to C2 cloud storage for off-site backups, alongside RAID1 and Clonezilla for local redundancy and recovery..</p>"},{"location":"homelab/infrastructure/#key-features","title":"Key Features","text":"<ul> <li>Model: Synology DS223</li> <li>RAID: RAID1 for data safety</li> <li>Function: Backup storage and NFS host for the homelab network.</li> </ul> <pre><code>admin@storage:~$ df -hTP\nFilesystem        Type      Size  Used Avail Use% Mounted on\n/dev/md0          ext4      7.9G  1.3G  6.5G  16% /\ndevtmpfs          devtmpfs  967M     0  967M   0% /dev\ntmpfs             tmpfs     987M   68K  987M   1% /dev/shm\ntmpfs             tmpfs     987M   19M  969M   2% /run\ntmpfs             tmpfs     987M     0  987M   0% /sys/fs/cgroup\ntmpfs             tmpfs     987M  1.2M  986M   1% /tmp\n/dev/vg1/volume_1 btrfs     1.8T  461G  1.3T  26% /volume1\n</code></pre>"},{"location":"homelab/infrastructure/#servers","title":"Servers","text":"<ul> <li>Proxmox servers, when clustered, form a dynamic virtualization platform crucial for homelabs and IT infrastructures. These servers enable seamless management and deployment of virtual machines and containers, offering flexibility, scalability, and high availability for diverse workloads.</li> </ul> <p>More info</p> MiniPCTower <ul> <li>Description: The DELL T7910 tower serves as the heart of the homelab, running a Windows 11-based Kubernetes test cluster before deployment to the production cluster on       the mini PCs. It's the powerhouse for virtualization and development.</li> </ul>"},{"location":"homelab/infrastructure/#specs-per-mini-pc","title":"Specs per Mini PC","text":"<ul> <li>DELL mini 3050</li> <li>CPU: Intel Core i5-6500T @ 2.50GHz</li> <li>RAM: 32GB DDR3</li> <li>Storage: 128GB SSD + 512GB NVME</li> <li>OS: Proxmox Hypervisor</li> <li>Cluster Composition: Three units operating in a Proxmox cluster for high availability and using ZFS replication for scalable storage.</li> </ul>"},{"location":"homelab/infrastructure/#roles-vms","title":"Roles &amp; VMs","text":"<ul> <li>citadel.merox.cloud: k3s-master-01, k3s-worker-01 <pre><code>root@citadel:/home/merox# qm list\n      VMID NAME                 STATUS     MEM(MB)    BOOTDISK(GB) PID\n       103 kasm                 stopped    6144              64.00 0\n       304 k3s-01               running    16384            125.20 179315\nroot@citadel:/home/merox# pct list\nVMID       Status     Lock         Name\n210        stopped                 lxc-k3s-ct-ready\n301        running                 k3s-master-01\n</code></pre></li> <li>helix.merox.cloud: k3s-master-02, k3s-worker-02, docker on alto.merox.cloud, Ansible LXC <pre><code>root@helix:~# qm list\n      VMID NAME                 STATUS     MEM(MB)    BOOTDISK(GB) PID\n       305 k3s-02               running    16384            125.20 376169\n       800 ubuntu-cloud         stopped    4096              25.20 0\nroot@helix:~# pct list\nVMID       Status     Lock         Name\n100        running                 ansible\n101        stopped                 cirrus\n102        running                 alto\n104        stopped                 cirrus.merox\n302        running                 k3s-master-02\n2000       stopped                 rke2-admin-template\n</code></pre></li> <li>nexus.merox.cloud: k3s-master-03, k3s-worker-03, Windows Server 2019 <pre><code>root@nexus:~# qm list\n      VMID NAME                 STATUS     MEM(MB)    BOOTDISK(GB) PID\n       105 winserver            running    4096              32.00 1218\n       306 k3s-03               running    14336            125.20 1891\nroot@nexus:~# pct list\nVMID       Status     Lock         Name\n300        running                 k3s-admin\n303        running                 k3s-master-03\n</code></pre></li> </ul>"},{"location":"homelab/infrastructure/#technical-specs_1","title":"Technical Specs","text":"<ul> <li>DELL T7910</li> <li>CPU: 2 x Intel Xeon E5-2667 v3 @ 3.20GHz</li> <li>RAM: 64GB DDR3</li> <li>Storage: 4TB SSD (2 x 2TB), 4TB HDD (2 x 2TB)</li> <li>GPU: Nvidia Quadro K2000</li> <li>OS: Windows 11</li> </ul>"},{"location":"homelab/infrastructure/#monitoring","title":"Monitoring","text":"<p>\u2022 Expand the Use of Grafana: You already utilize Grafana; consider integrating it further with Loki for a centralized logging solution. This combination allows for powerful visualization of logs alongside your metrics.</p> <p>\u2022 Leverage Loki for Comprehensive Logging: Since Loki is in use, ensure it captures logs from all critical systems and services. Set up structured logging where possible to make searching and debugging more efficient.</p> <p>\u2022 Alerting and Anomaly Detection: Utilize Prometheus' alerting rules to monitor for anomalies or specific events in your system metrics. For logging, configure Loki to alert on critical log patterns that could indicate issues or security concerns.</p> <p>\u2022 Netdata ( K3S cluster) for Real-Time Performance Monitoring \ud83d\udcc9: Utilize Netdata alongside existing tools for granular, real-time system monitoring.</p> <p> </p>"},{"location":"homelab/infrastructure/#security","title":"Security","text":"<p>\u2022 Firewall and IDS/IPS Enhancements: With pfSense as your firewall, consider reviewing and updating your ruleset regularly to adapt to new threats. Enhance IDS/IPS capabilities by incorporating additional rule sets or integrating with external threat intelligence feeds for more dynamic protection.</p> <p>\u2022 Secure Configuration Management: Use Ansible for automated deployment of security updates and configuration changes. This ensures that all devices are consistently configured according to best security practices.</p> <p>\u2022 VPN Security: Ensure your VPN setup on pfSense uses strong encryption standards and regularly rotate keys to maintain secure remote access.</p> <p> </p>"},{"location":"homelab/kubernetes/","title":"Kubernetes","text":"<p>In the interconnected ecosystem of PulsarDC, Kubernetes orchestrates the containerized applications, bringing scalability and automation to the forefront of my homelab. Utilizing K3s for its lightweight footprint, I've established a resilient and efficient Kubernetes cluster that seamlessly manages a diverse array of workloads. Dive into the architecture, setup, and the essential role Kubernetes plays in optimizing application deployments and enhancing system resilience in my setup. </p> <p>Continue reading through this page to learn how to set up your own cluster, storage solutions, and automation with ArgoCD, Ingress controllers, and more, in no time.</p>"},{"location":"homelab/kubernetes/#overview","title":"Overview","text":"<p>Info</p> <p>My cluster is designed with a focus on reliability and performance: 3 master nodes ensure high availability and cluster management, tagged with: node-role.kubernetes.i.o/master=true:NoSchedule. 3 worker nodes, tagged with: longhorn=true worker=true, handle the workload and storage, optimizing the environment for distributed applications.</p> <pre><code>merox@k3s-admin:~$ kubectl get nodes -o wide\nNAME            STATUS   ROLES                       AGE   VERSION          OS-IMAGE             KERNEL-VERSION    CONTAINER-RUNTIME\nk3s-01          Ready    &lt;none&gt;                      14d   v1.29.1+k3s2     Ubuntu 22.04.3 LTS   5.15.0-1049-kvm   containerd://1.7.11-k3s2\nk3s-02          Ready    &lt;none&gt;                      14d   v1.29.1+k3s2     Ubuntu 22.04.3 LTS   5.15.0-1049-kvm   containerd://1.7.11-k3s2\nk3s-03          Ready    &lt;none&gt;                      14d   v1.29.1+k3s2     Ubuntu 22.04.3 LTS   5.15.0-1049-kvm   containerd://1.7.11-k3s2\nk3s-master-01   Ready    control-plane,etcd,master   14d   v1.29.1+k3s2     Ubuntu 22.04.3 LTS   6.5.11-7-pve      containerd://1.7.11-k3s2\nk3s-master-02   Ready    control-plane,etcd,master   14d   v1.29.1+k3s2     Ubuntu 22.04.3 LTS   6.5.11-7-pve      containerd://1.7.11-k3s2\nk3s-master-03   Ready    control-plane,etcd,master   14d   v1.29.1+k3s2     Ubuntu 22.04.3 LTS   6.5.11-7-pve      containerd://1.7.11-k3s2\n</code></pre> Rancher NODES screenshot <p></p>"},{"location":"homelab/kubernetes/#advanced-tooling-for-an-optimized-experience","title":"Advanced Tooling for an Optimized Experience","text":"<p>Leveraging cutting-edge tools enhances my cluster's capabilities:</p>"},{"location":"homelab/kubernetes/#metallb-and-kubevip","title":"MetalLB and KubeVIP","text":"<ul> <li>MetalLB and KubeVIP offer reliable load balancing.     Check the footer <sup>1</sup> to view config files.</li> </ul>"},{"location":"homelab/kubernetes/#longhorn","title":"Longhorn","text":"<ul> <li>Longhorn provides resilient and scalable storage.</li> </ul> Longhorn screenshot"},{"location":"homelab/kubernetes/#rancher","title":"Rancher","text":"<ul> <li>Rancher simplifies cluster management with its intuitive UI.</li> </ul> Rancher screenshot"},{"location":"homelab/kubernetes/#traefik","title":"Traefik","text":"<ul> <li>Traefik serves as the ingress controller, directing traffic efficiently.</li> </ul> Traefik screenshott"},{"location":"homelab/kubernetes/#monitoring","title":"Monitoring","text":"<ul> <li>Kube-Prometheus and Loki deliver comprehensive monitoring and logging.</li> </ul>"},{"location":"homelab/kubernetes/#cicd","title":"CI/CD","text":"<ul> <li>ArgoCD, integrated with GitHub, automates deployment, keeping my cluster in sync with the latest configurations and applications.</li> </ul> ArgoCD screenshot"},{"location":"homelab/kubernetes/#installationconfigs","title":"Installation/Configs","text":"<p>Success</p> <p>See how to deploy and configure your own K3S cluster in no time:  Check configs DEPLOY YOUR OWN K3S CLUSTER </p> <ol> <li> <p>Deploy K3S in no time\u00a0\u21a9</p> </li> </ol>"},{"location":"homelab/monitoring/","title":"System Monitoring","text":"<p>Monitoring is a cornerstone in ensuring the optimal performance, security, and reliability of my IT infrastructure. I employ two distinct monitoring setups: one tailored specifically for the K3S cluster and another for the broader infrastructure outside of the K3S cluster. Each setup is meticulously chosen to provide comprehensive insights into the health and performance of the systems.</p>"},{"location":"homelab/monitoring/#monitoring-inside-k3s-cluster","title":"Monitoring Inside K3S Cluster","text":"<p>Info</p> <p>Within the K3S cluster, I leverage a specialized monitoring stack that integrates seamlessly with Kubernetes. This setup offers detailed insights into the health, performance, and logging of the cluster.</p>"},{"location":"homelab/monitoring/#components","title":"Components","text":""},{"location":"homelab/monitoring/#grafana","title":"Grafana","text":"<p>Used for real-time monitoring and visualization within the K3S cluster. It allows me to create tailored dashboards displaying metrics and logs for easy analysis and troubleshooting.</p>"},{"location":"homelab/monitoring/#prometheus","title":"Prometheus","text":"<p>Part of the kube-prometheus stack, serving as the primary monitoring database to collect metrics from various components of the K3S cluster.</p>"},{"location":"homelab/monitoring/#alertmanager","title":"Alertmanager","text":"<p>Configured to work alongside Prometheus, managing alerts within the K3S environment to ensure I'm promptly notified about potential issues.</p>"},{"location":"homelab/monitoring/#loki","title":"Loki","text":"<p>Utilized for aggregating and querying logs from across the K3S cluster, facilitating efficient log management alongside metrics in Grafana.</p>"},{"location":"homelab/monitoring/#netdata","title":"Netdata","text":"<p>An additional tool deployed for real-time insights into the performance of individual nodes within the K3S cluster.</p> <p>This tailored setup ensures a comprehensive monitoring solution for the unique needs of Kubernetes environments, offering visibility into every aspect of the cluster's operations.</p>"},{"location":"homelab/monitoring/#monitoring-outside-k3s-cluster","title":"Monitoring Outside K3S Cluster","text":"<p>Info</p> <p>For the infrastructure outside the K3S cluster, including virtual machines, network components, and the Proxmox cluster, I utilize a robust set of monitoring tools running in Docker containers.</p>"},{"location":"homelab/monitoring/#components_1","title":"Components","text":""},{"location":"homelab/monitoring/#grafana_1","title":"Grafana","text":"<p>Acts as the central visualization tool, offering customizable dashboards for monitoring data related to virtual machines, networking, and the Proxmox cluster.</p>"},{"location":"homelab/monitoring/#prometheus_1","title":"Prometheus","text":"<p>The core data repository, collecting metrics from Node-exporter, cAdvisor, and Proxmox Exporter, among others, to monitor physical servers, virtual machines, and containers.</p>"},{"location":"homelab/monitoring/#alertmanager_1","title":"Alertmanager","text":"<p>Manages alerts for the broader infrastructure, integrated with Prometheus to ensure a quick response to critical issues.</p>"},{"location":"homelab/monitoring/#node-exporter-and-proxmox-exporter","title":"Node-exporter and Proxmox Exporter","text":"<p>These provide detailed metrics on virtual machines and the Proxmox hypervisor, feeding valuable data into Prometheus for a comprehensive monitoring view.</p>"},{"location":"homelab/monitoring/#cadvisor","title":"cAdvisor","text":"<p>Offers insights into Docker container performance, enhancing the overall visibility of the infrastructure's health.</p>"},{"location":"homelab/monitoring/#influxdb","title":"InfluxDB","text":"<p>Utilized for monitoring data from my FreeBSD router running pfSense, aiding in network and security analysis.</p> <p>This setup is meticulously designed to ensure the health, performance, and security of the entire infrastructure, offering a holistic view of the systems outside the K3S cluster.</p>"},{"location":"homelab/monitoring/#architecture","title":"Architecture","text":"<p>By segregating my monitoring strategies into inside and outside the K3S cluster, I achieve targeted and effective oversight of my entire infrastructure. Each component within my stacks runs in Docker containers for isolation, scalability, and ease of maintenance. This approach allows me to tailor the monitoring tools and practices to the specific needs and characteristics of different parts of the infrastructure, ensuring comprehensive coverage and proactive management.</p>"},{"location":"homelab/monitoring/#conclusion","title":"Conclusion","text":"<p>My dual monitoring strategy ensures high performance, security, and reliability across all parts of the infrastructure. Adopting a proactive and segmented approach to monitoring allows me to detect and address issues promptly, maintaining the integrity and efficiency of my services.</p> <p>For further details on configuring and utilizing the monitoring tools within each setup, please refer to the specific sections of this documentation.</p>"},{"location":"homelab/monitoring/#configs","title":"Configs","text":"<ul> <li> Docker deploy monitoring</li> <li> K8S  deploy monitoring</li> </ul>"},{"location":"homelab/security/","title":"Protection","text":"<p>Welcome to the core of my digital fortress. With over two years of professional experience as a cybersecurity engineer and a profound journey in system administration focusing on Linux, networking, and security, I've honed my skills to craft a homelab that's not just a testbed for technology but a bulwark against digital threats.</p>"},{"location":"homelab/security/#restricted-access","title":"Restricted Access:","text":"<p>Access Control: Leveraging biometric verification and secure device placement, ensuring that only I can navigate the sanctum of my technological domain.</p> <p>Network Moats: Configuring smart port guardians on my routers and switches to permit solely known allies (devices), repelling any unidentified invaders.</p>"},{"location":"homelab/security/#firewall-rules","title":"Firewall rules","text":"<p>Here are some basic examples of my firewall rules from my homelab.</p>"},{"location":"homelab/security/#command","title":"Command","text":"<pre><code>iptables -A INPUT -d LAN_ADDRESS -p tcp --dport 8443 -j ACCEPT #Anti Lockout rule\niptables -A FORWARD -d DESTINATION_IP -j ACCEPT #pfB_PRI1_v4 auto rule\niptables -A FORWARD -s OPT1_NET -d X.X.X.X -p tcp -j ACCEPT\n</code></pre>"},{"location":"homelab/security/#essential-service-management","title":"Essential Service Management","text":"<p>Service Disablement: Non-essential services across devices are disabled to minimize vulnerabilities.         Example: Disabling unused services via systemctl disable  on Linux         servers. <p>Warning</p> <p>Always check your running services. ON ALL SYSTEMS, LINUX IS JUST AN EXAMPLE HERE</p> <pre><code>  root@alto ~# systemctl list-units --type=service --state=active\n  UNIT                                 LOAD   ACTIVE SUB     DESCRIPTION\n  blk-availability.service             loaded active exited  Availability of block devices\n  console-getty.service                loaded active running Console Getty\n  container-getty@1.service            loaded active running Container Getty on /dev/tty1\n  container-getty@2.service            loaded active running Container Getty on /dev/tty2\n  containerd.service                   loaded active running containerd container runtime\n  cron.service                         loaded active running Regular background program processing daemon\n  dbus.service                         loaded active running D-Bus System Message Bus\n  docker.service                       loaded active running Docker Application Container Engine\n  fail2ban.service                     loaded active running Fail2Ban Service\n</code></pre>"},{"location":"homelab/security/#security-assessment-tools","title":"Security Assessment Tools","text":"<p>Nessus, Nmap, Wireshark Employed for deep network insights. Nmap Command:  <pre><code>    nmap -sV -O -p- 192.168.1.0/24 #for comprehensive network scanning.\n</code></pre> Burp Suite  Utilized for rigorous web application testing. Demo: Setting up Burp Suite as a proxy for HTTP/HTTPS traffic to inspect web application requests and responses.</p>"},{"location":"homelab/security/#internet-exposure-and-protection","title":"Internet Exposure and Protection","text":"<p>HTTPS Traffic  Managed by Traefik, ensuring secure web exposure. Traefik Config: Enforce HTTPS using middleware redirections in Traefik.</p> <p>SSL/TLS: Harnessing LetsEncrypt and Cloudflare for encrypted connections. Cloudflare Setting: Enabling \"WAF\" in the Cloudflare dashboard. </p>"},{"location":"homelab/security/#firewall-and-intrusion-detection","title":"Firewall and Intrusion Detection","text":"<p>pfBlockerNG &amp; Snort: Integral for intrusion detection. Snort Rule: <pre><code> alert icmp any any -&gt; $HOME_NET any (msg:\"ICMP test\"; sid:1000001;) #for ICMP traffic monitoring.\n</code></pre></p>"},{"location":"homelab/security/#monitoring-with-grafana","title":"Monitoring with Grafana","text":"<p>Grafana Dashboard: Set up a dashboard to monitor real-time network traffic and alerts from Snort. </p>"},{"location":"homelab/security/#password-management","title":"Password Management","text":"<p>Password Manager: Essential for secure credential storage.         Best Practice: Use of complex passwords and enabling two-factor authentication where possible.</p>"},{"location":"homelab/security/#penetration-testing","title":"Penetration Testing","text":"<p>Kali Linux Machine: Dedicated for security testing and penetration exercises.         Metasploit Example: msfconsole to launch Metasploit for vulnerability exploitation and testing. <pre><code>msfconsole\nuse exploit/multi/handler\nset PAYLOAD &lt;payload_name&gt;\nset LHOST &lt;local_host&gt;\nexploit\n</code></pre></p> <p>Info</p> <p>If you want to learn more about Cybersec, I recommend you to check this dedicated platforms: Metasploitable2 TryHackMe HTB</p>"},{"location":"homelab/security/#tools","title":"Tools","text":"<ul> <li> Cybersec pentesting tools</li> </ul>"},{"location":"homelab/virtual-machines/","title":"Virtual Machines","text":"<p>Virtual machines (VMs) simulate physical computers, enabling the running of different operating systems and applications on a single hardware host. In a Proxmox cluster, VMs host Kubernetes to orchestrate containerized applications, providing efficient resource use and scalability. This configuration supports workload isolation and simplifies application deployment and management.</p>"},{"location":"homelab/virtual-machines/#virtual-machines-features","title":"Virtual Machines Features","text":""},{"location":"homelab/virtual-machines/#why-proxmox-and-qemu","title":"Why Proxmox and QEMU?","text":"<p>I chose Proxmox and QEMU for their robust performance and open-source nature. Proxmox provides a user-friendly interface on top of QEMU, making complex virtualization tasks manageable. Coupled with KVM, QEMU offers near-native performance, essential for the resource-intensive tasks my VMs perform.</p>"},{"location":"homelab/virtual-machines/#virtual-machines-features_1","title":"Virtual Machines Features","text":"<ul> <li>QEMU Integration: Utilizing QEMU with KVM enhances performance, making it an ideal choice for my Kubernetes orchestration needs.</li> <li>Multi-OS Support: Running Windows, Linux, and BSD, ensures flexibility across various projects and tasks.</li> <li>Advanced Management: QEMU Guest Agent and VNC/Spice Client access simplify management and monitoring.</li> <li>Optimization: Virtio devices and hotplug capabilities allow for efficient resource use and dynamic scaling.</li> <li>Hardware Utilization: Host CPU and GPU passthrough ensure optimal performance for specific applications.</li> </ul>"},{"location":"homelab/virtual-machines/#virtual-machines-cluster-overview","title":"Virtual Machines Cluster Overview","text":""},{"location":"homelab/virtual-machines/#ubuntu-vms-for-kubernetes","title":"Ubuntu VMs for Kubernetes","text":"<ul> <li>Specs: 3 VMs, each with 16GB RAM, 4 CPU cores, and 100GB SSD.</li> <li>Configuration: Cloud-init for automation and Longhorn for Kubernetes storage solutions.</li> </ul>"},{"location":"homelab/virtual-machines/#kasm-for-secure-browsing","title":"KASM for Secure Browsing","text":"<ul> <li>Specs: 6GB RAM, 2 CPU cores, 64GB SSD.</li> <li>Purpose: Isolates web browsing to protect against online threats.</li> </ul>"},{"location":"homelab/virtual-machines/#windows-server-2019-for-active-directory","title":"Windows Server 2019 for Active Directory","text":"<ul> <li>Specs: 6GB RAM, 4 CPU cores, 32GB SSD.</li> <li>Role: Manages network access, identities, and policies through AD and DNS.</li> </ul>"},{"location":"homelab/virtual-machines/#enhancing-network-and-security","title":"Enhancing Network and Security","text":"<ul> <li>DHCP Management: Handled by pfSense for efficient IP allocation.</li> <li>Backup Strategy: Bi-weekly backups to a Synology DS223, ensuring data integrity and quick recovery.</li> </ul>"},{"location":"homelab/virtual-machines/#monitoring-and-alerts","title":"Monitoring and Alerts","text":"<ul> <li>Real-time Metrics: Utilizing prometheus-node-exporter.</li> <li>Alert System: Email notifications via Alertmanager for operational anomalies.</li> </ul> <p>Info</p> id type maxdisk maxmem mem name node qemu/103 qemu 64.00 GiB 6.00 GiB 0.00 B kasm citadel qemu/105 qemu 32.00 GiB 4.00 GiB 2.02 GiB winserver nexus qemu/304 qemu 125.20 GiB 16.00 GiB 5.18 GiB k3s-01 citadel qemu/305 qemu 125.20 GiB 16.00 GiB 6.58 GiB k3s-02 helix qemu/306 qemu 125.20 GiB 14.00 GiB 8.83 GiB k3s-03 nexus qemu/800 qemu 25.20 GiB 4.00 GiB 0.00 B ubuntu helix"},{"location":"operations/intro/","title":"Operations","text":"<p>The Operations section is a candid look into the day-to-day management and optimization of IT infrastructure from my perspective. It is crafted for those entrenched in the operational side of technology\u2014system administrators, IT professionals, and tech enthusiasts who appreciate a practical approach to problem-solving and innovation.</p>"},{"location":"operations/intro/#delve-into-operational-experiences","title":"Delve into Operational Experiences","text":"<p>Discover a collection of content that sheds light on managing complex IT environments:</p>"},{"location":"operations/intro/#how-i-do-it","title":"How I Do It","text":"<p>Access step-by-step insights on tackling server deployments, network configurations, and more, reflecting my personal approach and lessons learned.</p>"},{"location":"operations/intro/#troubleshooting-chronicles","title":"Troubleshooting Chronicles","text":"<p>Explore the challenges I've faced and the solutions I've discovered, offering a real-world glimpse into IT problem-solving.</p>"},{"location":"operations/intro/#optimization-tips","title":"Optimization Tips","text":"<p>Share in the strategies I've employed to enhance system efficiency and security, drawn from firsthand experience.</p>"},{"location":"operations/intro/#why-venture-into-operations","title":"Why Venture into Operations?","text":"<ul> <li> <p>Real-World Application: Provides a glimpse into the practical aspects of IT operations, underscoring the realities of managing tech environments.</p> </li> <li> <p>Personal Journey: Chronicles my own experiences and growth in the field, presenting an opportunity for readers to find resonance or divergence in their paths.</p> </li> <li> <p>Community Insight: Encourages a sharing of knowledge and experiences, contributing to a broader understanding of IT operational challenges and successes.</p> </li> </ul> <p>Join me in exploring the operational facets of IT, where each challenge is a story and every solution a lesson learned.</p>"},{"location":"operations/Cybersecurity/pentesting/tools/","title":"Pentesting Tools Overview","text":"<p>Welcome to the comprehensive guide on essential pentesting tools and resources, meticulously curated for security professionals and enthusiasts exploring the realm of penetration testing.</p>"},{"location":"operations/Cybersecurity/pentesting/tools/#parrotos","title":"ParrotOS","text":"<p>Parrot Security offers an extensive collection of tools, utilities, and libraries for IT and security professionals to assess their assets' security comprehensively. From information gathering to generating final reports, Parrot ensures a flexible environment for reliable, compliant, and reproducible testing.</p> <ul> <li>Edition: Choose the Security Edition for optimal features.</li> <li>Website: Parrot Security</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#kali-linux","title":"Kali Linux","text":"<p>Kali Linux is a Debian-based distribution tailored for various information security tasks, including Penetration Testing, Security Research, Computer Forensics, and Reverse Engineering.</p> <ul> <li>Installation: Opt for the Kali Linux VMware or VirtualBox image. Default credentials are \"kali/kali\".</li> <li>Website: Kali Linux</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#essential-tools-for-pentesting","title":"Essential Tools for Pentesting","text":""},{"location":"operations/Cybersecurity/pentesting/tools/#nmap","title":"nmap","text":"<p>A versatile tool for network discovery and security auditing. Useful for scanning large networks or single hosts.</p> <ul> <li>Install: <code>apt -y install nmap</code></li> <li>Command: <code>nmap -v -p- -sC -sV -oA &lt;basename&gt; 10.12.10.123</code></li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#netcat","title":"netcat","text":"<p>Often abbreviated to nc, netcat is a networking utility for reading from and writing to network connections using TCP or UDP.</p> <ul> <li>Install: <code>apt -y install netcat</code></li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#wappalyzer","title":"Wappalyzer","text":"<p>Discover the technology stack of any website. Useful for reconnaissance.</p> <ul> <li>Website: Wappalyzer</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#gobuster","title":"gobuster","text":"<p>A tool written in Go for brute-forcing URIs (directories and files) in web sites, DNS subdomains, and Virtual Host names.</p> <ul> <li>Install:</li> <li><code>sudo apt install golang-go</code></li> <li><code>go install github.com/OJ/gobuster/v3@latest</code></li> <li>GitHub: gobuster</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#ffuf-fuzz-faster-u-fool","title":"Ffuf (Fuzz Faster U Fool)","text":"<p>A fast web fuzzer written in Go, ideal for web penetration testing.</p> <ul> <li>Install:</li> <li><code>sudo apt install golang-go</code></li> <li><code>go install github.com/ffuf/ffuf@latest</code></li> <li>GitHub: ffuf</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#johntheripper","title":"JohnTheRipper","text":"<p>An advanced offline password cracker supporting numerous hash and cipher types.</p> <ul> <li>Install: <code>apt -y install john</code></li> <li>Tutorials: John the Ripper Tutorials</li> <li>GitHub: JohnTheRipper</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#responder","title":"Responder","text":"<p>A tool for poisoning LLMNR, NBT-NS, and MDNS, with a built-in rogue authentication server.</p> <ul> <li>GitHub: Responder</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#burp-suite","title":"Burp Suite","text":"<p>A Java application for web application security testing, including tools like a proxy server, web spider, intruder, and scanner.</p> <ul> <li>Website: Burp Suite Community Edition</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#zap-zed-attack-proxy","title":"ZAP - Zed Attack Proxy","text":"<p>An OWASP-maintained tool for penetration testing web applications, acting as a man-in-the-middle proxy.</p> <ul> <li>Website: ZAP</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#impacket","title":"Impacket","text":"<p>Python classes for working with network protocols, providing low-level access to packets and protocol implementations.</p> <ul> <li>Install: <code>python3 -m pip install .</code></li> <li>GitHub: Impacket</li> <li>Website: Impacket by SecureAuth</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#revshellgen","title":"Revshellgen","text":"<p>Generates reverse shells easily, automating common setup tasks.</p> <ul> <li>Download: <code>git clone https://github.com/t0thkr1s/revshellgen</code></li> <li>Run: <code>python3 revshellgen.py</code></li> <li>GitHub: Revshellgen</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#evil-winrm","title":"Evil-WinRM","text":"<p>The ultimate WinRM shell for hacking/pentesting Windows environments.</p> <ul> <li>GitHub: Evil-WinRM</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#watson","title":"Watson","text":"<p>Enumerates missing KBs and suggests exploits for Privilege Escalation vulnerabilities.</p> <ul> <li>Supported Versions: Windows 10, Server 2016 &amp; 2019</li> <li>GitHub: Watson</li> </ul>"},{"location":"operations/Cybersecurity/pentesting/tools/#webshells","title":"Webshells","text":""},{"location":"operations/Cybersecurity/pentesting/tools/#blackarchs-webshells","title":"Blackarch's Webshells","text":"<p>A collection of various webshells for ASP, ASPX, CFM, JSP, Perl, PHP.</p> <ul> <li>Usage Warning: Review the source code for safety before use.</li> <li>GitHub: BlackArch Webshells</li> </ul> <p>Dive deeper into each tool and harness their power effectively for your pentesting needs.</p>"},{"location":"operations/DevOps/ansible/playbooks/linux-upgrade/","title":"Linux upgrades","text":""},{"location":"operations/DevOps/ansible/playbooks/linux-upgrade/#linux-automatically-upgrade","title":"Linux automatically upgrade","text":"<pre><code>---\n\n- hosts: all\n  tasks:\n  - name: install all updates\n    apt:\n      upgrade: dist\n      update_cache: yes\n      autoremove: yes\n      autoclean: yes\n    register: result\n  - name: List installed and updated packages\n    shell: grep -E \"^$(date +%Y-%m-%d).+ (install|upgrade) \" /var/log/dpkg.log |cut -d \" \" -f 3-5\n    register: result\n  - name: Show Output\n    debug: msg=\"{{ result.stdout_lines }}\"\n- hosts: centos\n  tasks:\n  - name: install all updates\n    yum:\n      name: '*'\n      update_cache: yes\n      state: latest\n  - name: List updated packages\n    shell: rpm -qa --last | grep \"$(date +%a\\ %d\\ %b\\ %Y)\" |cut -f 1 -d \" \"\n    register: result\n    args:\n      warn: no\n  - name: Updates packages\n    debug: msg=\"{{ result.stdout_lines }}\"\n</code></pre>"},{"location":"operations/DevOps/ansible/playbooks/proxmox-vm/","title":"Proxmox VM","text":""},{"location":"operations/DevOps/ansible/playbooks/proxmox-vm/#vm-creation-in-proxmox","title":"VM creation in Proxmox","text":"<pre><code>---\n- name: Create VM in Proxmox\n  hosts: localhost\n  gather_facts: no\n  tasks:\n    - name: Create a new VM\n      community.general.proxmox_kvm:\n        api_user: root@pam\n        api_password: YOUR_PASSWORD\n        api_host: YOUR_PROXMOX_HOST\n        validate_certs: no\n        vmid: \"100\"\n        name: \"ansible-vm\"\n        node: \"pve\"\n        memory: 2048\n        cores: 2\n        sockets: 1\n        cpu: \"host\"\n        net: '{\"net0\":\"virtio,bridge=vmbr0\"}'\n        disk: '{\"size\":\"30G\"}'\n        ostype: l26\n        iso: \"local:iso/ubuntu-22.04.1-live-server-amd64.iso\"\n        ide2: \"local:iso/ubuntu-22.04.1-live-server-amd64.iso,media=cdrom\"\n        bootdisk: \"virtio0\"\n        boot: \"cdn\"\n        description: \"Created by Ansible\"\n        state: \"present\"\n</code></pre>"},{"location":"operations/DevOps/ansible/playbooks/windows-upgrade/","title":"Windows upgrades","text":""},{"location":"operations/DevOps/ansible/playbooks/windows-upgrade/#windows-automatically-upgrade","title":"Windows automatically upgrade","text":"<pre><code>- name:  Windows Update\n  hosts: wserver.merox.cloud\n  gather_facts: false\n  tasks:\n    - name: Running Windows Update\n      win_updates:\n        category_names: ['SecurityUpdates','CriticalUpdates','UpdateRollups', 'Updates', 'DefinitionUpdates']\n        reboot: yes\n</code></pre>"},{"location":"operations/containerization/kubernetes/","title":"Quick links for Kubernetes and K3S","text":""},{"location":"operations/containerization/kubernetes/#kubernetes","title":"Kubernetes","text":"<p>Official Site:  kubernetes.io</p> <p>Cheat Sheet:  kubernetes.io/docs/reference/kubectl/quick-reference/</p> <p>Kubectl Cheat sheet:  kubernetes.io/docs/reference/generated/kubectl/kubectl-commands</p>"},{"location":"operations/containerization/kubernetes/#k3s","title":"K3S","text":"<p>Official site:  k3s.io</p> <p>Upgrade:  docs.k3s.io/upgrades</p>"},{"location":"operations/containerization/LXCs/proxmox/","title":"Proxmox LXCs","text":"<p>Containers are a lightweight alternative to fully virtualized machines (VMs). They use the kernel of the host system that they run on, instead of emulating a full operating system (OS). This means that containers can access resources on the host system directly.</p> <p>The runtime costs for containers is low, usually negligible. However, there are some drawbacks that need be considered:</p> <p>Only Linux distributions can be run in Proxmox Containers. It is not possible to run other operating systems like, for example, FreeBSD or Microsoft Windows inside a container.</p> <p>Danger</p> <p>For security reasons, access to host resources needs to be restricted. Therefore, containers run in their own separate namespaces. Additionally some syscalls (user space requests to the Linux kernel) are not allowed within containers.</p> <p>LXCs configurations on 3 nodes Proxmox Cluster</p> CitadelHelixNexus"},{"location":"operations/containerization/LXCs/proxmox/#k3s-master-01","title":"K3S-master-01","text":"<pre><code>root@citadel:/home/merox# cat /etc/pve/nodes/citadel/lxc/301.conf\n</code></pre> <pre><code>arch: amd64\ncores: 4\nfeatures: fuse=1\nhostname: k3s-master-01\nmemory: 4096\nnet0: name=eth0,bridge=vmbr0,firewall=1,hwaddr=FC:14:11:E0:A7:02,ip=dhcp,type=veth\nonboot: 1\nostype: ubuntu\nrootfs: Storage:subvol-301-disk-1,size=50G\nswap: 0\nlxc.apparmor.profile: unconfined\nlxc.cgroup.devices.allow: a\nlxc.cap.drop: \nlxc.mount.auto: \"proc:rw sys:rw\"\n</code></pre>"},{"location":"operations/containerization/LXCs/proxmox/#k3s-master-02","title":"K3S-master-02","text":"<p><pre><code>root@helix:/etc/pve/nodes/helix/lxc# cat /etc/pve/nodes/helix/lxc/302.conf \n</code></pre> <pre><code>arch: amd64\ncores: 4\nfeatures: fuse=1\nhostname: k3s-master-02\nmemory: 4096\nnet0: name=eth0,bridge=vmbr0,firewall=1,hwaddr=BC:24:11:3A:A1:2A,ip=dhcp,type=veth\nonboot: 1\nostype: ubuntu\nrootfs: Storage:subvol-302-disk-1,size=50G\nswap: 0\nlxc.apparmor.profile: unconfined\nlxc.cgroup.devices.allow: a\nlxc.cap.drop: \nlxc.mount.auto: \"proc:rw sys:rw\"\n</code></pre></p>"},{"location":"operations/containerization/LXCs/proxmox/#ansible","title":"Ansible","text":"<p><pre><code>root@helix:/etc/pve/nodes/helix/lxc# cat 100.conf \n</code></pre> <pre><code>arch: amd64\ncores: 2\nfeatures: nesting=1\nhostname: ansible\nmemory: 1024\nnameserver: 10.57.57.1\nnet0: name=eth0,bridge=vmbr0,firewall=1,gw=10.57.57.1,hwaddr=92:9C:BD:89:57:E1,ip=10.57.57.113/24,type=veth\nonboot: 1\nostype: debian\nrootfs: Storage:subvol-100-disk-0,size=32G\nsearchdomain: merox.cloud\nswap: 1024\ntags: docker;intern;linux\n</code></pre></p>"},{"location":"operations/containerization/LXCs/proxmox/#alto-docker","title":"Alto ( docker )","text":"<p><pre><code>root@helix:/etc/pve/nodes/helix/lxc# cat 102.conf \n</code></pre> <pre><code>#For cloning\narch: amd64\ncores: 2\nfeatures: mount=nfs,nesting=1\nhostname: alto\nmemory: 3072\nnameserver: 10.57.57.1\nnet0: name=eth0,bridge=vmbr0,firewall=1,gw=10.57.57.1,hwaddr=E6:B3:3E:64:D2:CA,ip=10.57.57.56/24,type=veth\nonboot: 1\nostype: debian\nrootfs: Storage:subvol-102-disk-1,size=50G\nswap: 512\ntags: container;intern;linux\nlxc.cap.drop: \n</code></pre></p>"},{"location":"operations/containerization/LXCs/proxmox/#k3s-master-03","title":"K3S-master-03","text":"<p><pre><code>root@nexus:~# cat /etc/pve/nodes/nexus/lxc/303.conf \n</code></pre> <pre><code>arch: amd64\ncores: 4\nfeatures: fuse=1\nhostname: k3s-master-03\nmemory: 4096\nnet0: name=eth0,bridge=vmbr0,firewall=1,hwaddr=FC:14:11:21:50:3B,ip=dhcp,type=veth\nonboot: 1\nostype: ubuntu\nrootfs: Storage:subvol-303-disk-0,size=50G\nswap: 0\nlxc.apparmor.profile: unconfined\nlxc.cgroup.devices.allow: a\nlxc.cap.drop: \nlxc.mount.auto: \"proc:rw sys:rw\"\n</code></pre></p>"},{"location":"operations/containerization/LXCs/proxmox/#k3s-admin","title":"K3S-admin","text":"<p><pre><code>root@nexus:~# cat /etc/pve/nodes/nexus/lxc/300.conf \n</code></pre> <pre><code>arch: amd64\ncores: 4\nfeatures: fuse=1\nhostname: k3s-admin\nmemory: 4096\nnet0: name=eth0,bridge=vmbr0,firewall=1,hwaddr=E4:14:11:68:C2:D6,ip=dhcp,type=veth\nonboot: 1\nostype: ubuntu\nrootfs: Storage:subvol-300-disk-1,size=50G\nswap: 0\nlxc.apparmor.profile: unconfined\nlxc.cgroup.devices.allow: a\nlxc.cap.drop: \nlxc.mount.auto: \"proc:rw sys:rw\"\n</code></pre></p>"},{"location":"operations/containerization/LXCs/proxmox/#configuration-for-k3s","title":"Configuration for K3S","text":"<p>I spent many hours grappling with the challenge of getting K3S to work on LXCs, and at this point, I'd like to extend my heartfelt thanks to Garrett Mills. His insights were invaluable. Now, let's dive into how you can successfully run K3S on LXC.</p> <p>Warning</p> <ol> <li>Make sure you add above highlighted lines to your LXC config.</li> <li>Note: It's important that the container is stopped when you try to edit the file, otherwise Proxmox's network filesystem will prevent you from saving it.</li> <li>In order, these options (1) disable AppArmor, (2) allow the container's cgroup to access all devices, (3) prevent dropping any capabilities for the container, and (4) mount /proc and /sys as read-write in the container.</li> <li>Next, we need to publish the kernel boot configuration into the container. Normally, this isn't needed by the container since it runs using the host's kernel, but the Kubelet uses the configuration to determine various settings for the runtime, so we need to copy it into the container. To do this, first start the container using the Proxmox web UI, then run the following command on the Proxmox host:</li> <li>Execute: <pre><code>pct push &lt;container id&gt; /boot/config-$(uname -r) /boot/config-$(uname -r)\n</code></pre></li> <li>Finally, in each of the containers, we need to make sure that /dev/kmsg exists. Kubelet uses this for some logging functions, and it doesn't exist in the containers by default. For our purposes, we'll just alias it to /dev/console. In each container, create the file /usr/local/bin/conf-kmsg.sh with the following contents:</li> </ol> <pre><code>#!/bin/sh -e\nif [ ! -e /dev/kmsg ]; then\n    ln -s /dev/console /dev/kmsg\nfi\nmount --make-rshared /\n</code></pre> <p>7.This script symlinks /dev/console as /dev/kmsg if the latter does not exist. Finally, we will configure it to run when the container starts with a SystemD one-shot service. Create the file /etc/systemd/system/conf-kmsg.service with the following contents:</p> <pre><code>[Unit]\nDescription=Make sure /dev/kmsg exists\n\n[Service]\nType=simple\nRemainAfterExit=yes\nExecStart=/usr/local/bin/conf-kmsg.sh\nTimeoutStartSec=0\n\n[Install]\nWantedBy=default.target\n</code></pre> <p>8.Finally, enable the service by running the following: <pre><code>chmod +x /usr/local/bin/conf-kmsg.sh\nsystemctl daemon-reload\nsystemctl enable --now conf-kmsg\n</code></pre></p>"},{"location":"operations/containerization/LXCs/proxmox/#lxc-k3s","title":"LXC K3S","text":"<pre><code>arch: amd64\ncores: 4\nfeatures: fuse=1\nhostname: lxc-k3s-ct-ready\nmemory: 4096\nnet0: name=eth0,bridge=vmbr0,firewall=1,hwaddr=BC:24:11:78:33:4B,ip=dhcp,type=veth\nostype: ubuntu\nrootfs: Storage:basevol-210-disk-0,size=50G\nswap: 0\ntemplate: 1\nlxc.apparmor.profile: unconfined\nlxc.cgroup.devices.allow: a\nlxc.cap.drop: \nlxc.mount.auto: \"proc:rw sys:rw\"\n</code></pre>"},{"location":"operations/containerization/docker/","title":"Docker","text":"<p>Docker offers advantages over virtual machines by being more resource-efficient and faster due to container technology, which shares the host's kernel. It ensures consistent environments across different stages, simplifying deployment and reducing compatibility issues. Docker's efficient resource use and deployment agility make it a cost-effective solution for application management compared to VMs.</p>"},{"location":"operations/containerization/docker/#install","title":"Install","text":"<p>+++ Docker install</p> <p>Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.</p> <ol> <li>Set up Docker's apt repository.</li> </ol> <p>```bash #   # Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc</p>"},{"location":"operations/containerization/docker/#add-the-repository-to-apt-sources","title":"Add the repository to Apt sources:","text":"<p>echo \\   \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\   \\((. /etc/os-release &amp;&amp; echo \"\\)VERSION_CODENAME\") stable\" | \\   sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null sudo apt-get update   <code>2. Install the Docker packages.</code>bash # sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin <pre><code>!!!warning DISTRO\n If you use an Ubuntu derivative distro, such as Linux Mint, you may need to use UBUNTU_CODENAME instead of VERSION_CODENAME.\n!!!\n\n3. Verify that the Docker Engine installation is successful by running the hello-world image.\n```bash #\nsudo docker run hello-world\n</code></pre> !button icon=\"\" text=\"View source\"</p> <p>+++ Portainer install</p> <ol> <li>First, create the volume that Portainer Server will use to store its database: ```bash # docker volume create portainer_data <pre><code>2. Then, download and install the Portainer Server container:\n```bash #\ndocker run -d -p 8000:8000 -p 9443:9443 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:latest\n</code></pre></li> </ol> <p>Info</p> <p>By default, Portainer generates and uses a self-signed SSL certificate to secure port 9443   !!!</p> <p>!button icon=\"\" text=\"View source\"</p> <p>+++</p>"},{"location":"operations/containerization/docker/docker-compose/cloudflare/","title":"Cloudflare Tunnel","text":"<p>Protect your web servers from direct attack</p> <p>From the moment an application is deployed, developers and IT spend time locking it down \u2014 configuring ACLs, rotating IP addresses, and using clunky solutions like GRE tunnels.</p> <pre><code>version: \"3.9\"\nservices:\n  wordpress:\n    container_name: wordpress\n    image: wordpress:latest\n    restart: unless-stopped\n    volumes:\n      - ./wordpress:/app/data\n\n  tunnel:\n    container_name: cloudflared-tunnel\n    image: cloudflare/cloudflared\n    restart: unless-stopped\n    command: tunnel run\n    environment:\n      - TUNNEL_TOKEN=mytoken\n\nnetworks:\n  default:\n    external:\n      name: wordpress\n</code></pre>"},{"location":"operations/containerization/docker/docker-compose/docker-compose-options/","title":"Docker Compose","text":"<p>Docker Compose is a tool for defining and running multi-container applications. It is the key to unlocking a streamlined and efficient development and deployment experience.</p> <p>Compose simplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single, comprehensible YAML configuration file. Then, with a single command, you create and start all the services from your configuration file.</p>"},{"location":"operations/containerization/docker/docker-compose/docker-compose-options/#docker-compose-options","title":"Docker Compose Options","text":""},{"location":"operations/containerization/docker/docker-compose/docker-compose-options/#loging","title":"Loging","text":"<p>Reduce container size of logs (support compose &amp; swarm) ```yaml #</p> <p>logging:      driver: \"json-file\"      options:        max-size: \"500m\"        max-file: \"10\"        compress: \"true\"</p> <pre><code>### DNS\n```yaml #\n\n   dns:\n     - \"10.0.0.2\"\n     - \"8.8.8.8\"\n</code></pre>"},{"location":"operations/containerization/docker/docker-compose/docker-compose-options/#ports","title":"PORTS","text":"<p>```yaml #</p> <p>ports:      - \"8080:8080 <pre><code>### Extra Hosts\n```yaml #\n\n   extra_hosts:\n     - \"sub.somedomain.com:10.0.10.30\"\n</code></pre></p>"},{"location":"operations/containerization/docker/docker-compose/docker-compose-options/#labels","title":"Labels","text":"<p>```yaml #</p> <pre><code>labels: [app=reporting]\n</code></pre> <pre><code>### Container name\n\n```yaml #\n\n    container_name: somename\n</code></pre>"},{"location":"operations/containerization/docker/docker-compose/docker-compose-options/#limit-resources","title":"Limit resources","text":"<pre><code>      resources:\n        reservations:\n          memory: 2048M\n          cpus: '0.0001'\n        limits:\n          memory: 4096M\n          cpus: '0.5'\n</code></pre>"},{"location":"operations/containerization/docker/docker-compose/influxdb/","title":"InfluxDB","text":"<p>Real-time insights from any time series data with a single, purpose-built database. Run at any scale in any environment in the cloud, on-premises, or at the edge.</p> <pre><code>version: '3'\nservices:\n  influxdb:\n    image: influxdb:1.8\n    container_name: influxdb\n    ports:\n      - \"8086:8086\"  \n    volumes:\n      - /docker/influxdb:/var/lib/influxdb  \n    restart: always  # \n</code></pre>"},{"location":"operations/containerization/docker/docker-compose/media-stack/","title":"Media Stack","text":"<p>The media stack with Jellyfin, Radarr, Sonarr, qBittorrent, and Jackett creates an efficient personal media center. Jellyfin streams media, Radarr and Sonarr automate media organization, qBittorrent manages torrents, and Jackett enhances search across torrent providers. This setup streamlines media management and streaming.</p> <pre><code>version: '3'\nservices:\n  jellyfin:\n    image: jellyfin/jellyfin\n    container_name: jellyfin\n    restart: unless-stopped\n    volumes:\n      - /media-config/jellyfin:/config\n      - /media:/media\n      - /media/movies:/movies\n      - /media/shows:/TVs\n    ports:\n      - 8096:8096\n    environment:\n      - PUID=1057\n      - PGID=1056\n      - TZ=Europe/Bucharest\n\n  radarr:\n    container_name: radarr\n    restart: unless-stopped\n    ports:\n      - 7878:7878\n    volumes:\n      - /media-config/radarr:/config\n      - /media/qbittorrent:/downloads\n      - /media/movies:/movies\n    environment:\n      - PUID=1057\n      - PGID=1056\n      - TZ=Europe/Bucharest\n    image: linuxserver/radarr\n\n  sonarr:\n    container_name: sonarr\n    restart: unless-stopped\n    ports:\n      - 8989:8989\n    volumes:\n      - /media-config/sonarr:/config\n      - /media/qbittorrent:/downloads\n      - /media/shows:/tv\n    environment:\n      - PUID=1057\n      - PGID=1056\n      - TZ=Europe/Bucharest\n    image: linuxserver/sonarr\n\n  jackett:\n    container_name: jackett\n    restart: unless-stopped\n    ports:\n      - 9117:9117\n    volumes:\n      - /media-config/jackett:/config\n    environment:\n      - PUID=1057\n      - PGID=1056\n      - TZ=Europe/Bucharest\n    image: linuxserver/jackett\n\n  qbittorrent:\n    image: linuxserver/qbittorrent\n    container_name: qbittorrent\n    environment:\n      - PUID=1057\n      - PGID=1056\n      - TZ=Europe/Bucharest\n      - WEBUI_PORT=8091\n    volumes:\n      - /media-config/qbittorrent:/config\n      - /media/qbittorrent:/downloads\n    ports:\n      - 6881:6881\n      - 6881:6881/udp\n      - 8091:8091\n    restart: unless-stopped\n</code></pre>"},{"location":"operations/containerization/docker/docker-compose/monitoring-stack/","title":"Monitoring stack","text":"<p>The monitoring stack with Grafana, Prometheus, and Alertmanager delivers efficient system and application insights. Grafana visualizes metrics, Prometheus monitors and alerts on time series data, and Alertmanager handles alert notifications. This setup ensures proactive infrastructure management.</p> <pre><code>version: '3.3'\nnetworks:\n  monitoring:\n    driver: bridge\nvolumes:\n  prometheus_data: {}\nservices:\n  node-exporter:\n    image: prom/node-exporter:latest\n    container_name: node-exporter\n    restart: unless-stopped\n    volumes:\n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /:/rootfs:ro\n    command:\n      - '--path.procfs=/host/proc'\n      - '--path.rootfs=/rootfs'\n      - '--path.sysfs=/host/sys'\n      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'\n    ports:\n      - 9100:9100\n    networks:\n      - monitoring\n\n  prometheus:\n    image: prom/prometheus:latest\n    user: \"1001\"\n    environment:\n      - PUID=1001\n      - PGID=1001\n    container_name: prometheus\n    restart: unless-stopped\n    volumes:\n      - /home/1001/promgrafnode/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml\n      - /home/1001/promgrafnode/prometheus/alert.rules.yml:/etc/prometheus/alert.rules.yml\n      - /home/1001/promgrafnode/prometheus:/prometheus\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--web.console.libraries=/etc/prometheus/console_libraries'\n      - '--web.console.templates=/etc/prometheus/consoles'\n      - '--web.enable-lifecycle'\n    ports:\n      - 9090:9090\n    networks:\n      - monitoring\n\n  grafana:\n    image: grafana/grafana:latest\n    user: \"1001\"\n    container_name: grafana\n    ports:\n      - 3000:3000\n    restart: unless-stopped\n    volumes:\n      - /home/1001/promgrafnode/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources\n      - /home/1001/promgrafnode/grafana:/var/lib/grafana\n    networks:\n      - monitoring\n\n  alertmanager:\n    image: prom/alertmanager:latest\n    command:\n      - '--config.file=/etc/alertmanager/config.yml'\n    ports:\n      - '9093:9093'\n    volumes:\n      - '/home/alertmanager/:/etc/alertmanager'\n    networks:\n      - alertmanager-net\n\nnetworks:\n  alertmanager-net:\n  cadvisor:\n    image: gcr.io/cadvisor/cadvisor:latest\n    container_name: cadvisor\n    ports:\n      - 8081:8080\n    networks:\n      - monitoring\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:rw\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n    depends_on:\n      - redis\n  redis:\n    image: redis:latest\n    container_name: redis\n    ports:\n      - 6379:6379\n    networks:\n      - monitoring\n ```\n\n ## prometheus.yml\n\n ```yaml linenums=\"1\"\nglobal:\n  scrape_interval: 1m\n\nrule_files:\n  - alert.rules.yml\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets: ['alto.merox.cloud:9093']\n\n\nscrape_configs:\n  - job_name: \"prometheus\"\n    scrape_interval: 1m\n    static_configs:\n    - targets: [\"localhost:9090\"]\n\n  - job_name: \"AltoVM\"\n    static_configs:\n    - targets: [\"node-exporter:9100\"]\n\n  - job_name: \"CirrusVM\"\n    static_configs:\n    - targets: [\"cirrus.merox.cloud:9100\"]\n\n  - job_name: \"cadvisor_alto\"\n    scrape_interval: 5s\n    static_configs:\n    - targets: [\"alto.merox.cloud:8081\"]\n\n  - job_name: \"cadvisor_cirrus\"\n    scrape_interval: 5s\n    static_configs:\n    - targets: [\"cirrus.merox.cloud:8081\"]\n\n#  - job_name: \"cadvisor_tower\"\n#    scrape_interval: 5s\n#    static_configs:\n#    - targets: [\"media.merox.cloud:8081\"]\n\n#Windows machines\n  - job_name: \"Tower\"\n    scrape_interval: 5s\n    static_configs:\n    - targets: [\"tower.merox.cloud:9182\"]\n\n  - job_name: \"Wserver\"\n    scrape_interval: 5s\n    static_configs:\n    - targets: [\"wserver.merox.cloud:9182\"]\n#Proxmox\n  - job_name: 'pve-exporter'\n    static_configs:\n      - targets:\n        - nexus.merox.cloud:9221\n    metrics_path: /pve\n    params:\n      module: [default]\n</code></pre>"},{"location":"operations/containerization/docker/docker-compose/monitoring-stack/#alertrulesyml","title":"alert.rules.yml","text":"<pre><code>groups:\n- name: alert.rules\n  rules:\n  - alert: InstanceDown\n    expr: up == 0\n    for: 1m\n    labels:\n      severity: \"critical\"\n    annotations:\n      summary: \"Endpoint {{ $labels.instance }} down\"\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minutes.\"\n\n  - alert: HostOutOfMemory\n    expr: node_memory_MemAvailable / node_memory_MemTotal * 100 &lt; 25\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Host out of memory (instance {{ $labels.instance }})\"\n      description: \"Node memory is filling up (&lt; 25% left)\\n  VALUE = {{ $value }}\\n  LABELS: {{ $labels }}\"\n\n\n  - alert: HostOutOfDiskSpace\n    expr: (node_filesystem_avail{mountpoint=\"/\"}  * 100) / node_filesystem_size{mountpoint=\"/\"} &lt; 50\n    for: 1s\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Host out of disk space (instance {{ $labels.instance }})\"\n      description: \"Disk is almost full (&lt; 50% left)\\n  VALUE = {{ $value }}\\n  LABELS: {{ $labels }}\"\n\n\n  - alert: HostHighCpuLoad\n    expr: (sum by (instance) (irate(node_cpu{job=\"node_exporter_metrics\",mode=\"idle\"}[5m]))) &gt; 80\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Host high CPU load (instance {{ $labels.instance }})\"\n      description: \"CPU load is &gt; 80%\\n  VALUE = {{ $value }}\\n  LABELS: {{ $labels }}\"\n\n  - alert: HighHTTPRequestRate\n    expr: rate(http_requests_total[1m]) &gt; 1000\n    for: 1m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Possible DDoS Attack Detected\"\n      description: \"High rate of HTTP requests detected. Possible DDoS attack.\"\n</code></pre> <pre><code>docker-compose -f /path/to/your/docker-compose-file.yml up -d\n</code></pre>"},{"location":"operations/containerization/docker/docker-compose/uptime-kuma/","title":"Uptime KUMA","text":"<p>Uptime Kuma is an easy-to-use self-hosted monitoring tool. https://github.com/louislam/uptime-kuma</p> <pre><code>version: '3.3'\n\nservices:\n  uptime-kuma:\n    image: louislam/uptime-kuma:1\n    container_name: uptime-kuma\n    volumes:\n      - ./uptime-kuma-data:/app/data\n    ports:\n      - 3001:3001  \n    restart: always\n</code></pre>"},{"location":"operations/containerization/docker/docker-compose/wordpress/","title":"WordPress","text":"<p>WordPress, launched in 2003, is a leading content management system (CMS) for creating websites, known for its ease of use, extensive plugins, and themes. It caters to both beginners and developers, enabling the development of everything from blogs to complex sites, making it highly popular across the web.</p> <pre><code>version: '2'\n\nservices:\n   db:\n     image: mysql:5.7\n     volumes:\n       - db_data:/var/lib/mysql\n     restart: always\n     environment:\n       MYSQL_ROOT_PASSWORD: ${MYSQL_DATABASE_PASSWORD}\n       MYSQL_DATABASE: wordpress\n       MYSQL_USER: wordpress\n       MYSQL_PASSWORD: wordpress\n\n   wordpress:\n     image: wordpress:latest\n     ports:\n       - 80\n     restart: always\n     environment:\n       WORDPRESS_DB_HOST: db:3306\n       WORDPRESS_DB_USER: wordpress\n       WORDPRESS_DB_PASSWORD: wordpress\n\nvolumes:\n    db_data:\n</code></pre> <pre><code>docker-compose -f /path/to/your/docker-compose-file.yml up -d\n</code></pre>"},{"location":"operations/containerization/k3s/installation/","title":"Kubernetes","text":""},{"location":"operations/containerization/k3s/installation/#deploy-k3s-in-no-time","title":"Deploy K3S in no time","text":"<p>Special thanks to:</p> <p>@Jims-Garage @TechnoTim</p> <p>don't be script kidde</p> <p>Please read, check, and adapt the script to your needs!</p> <p>Warning</p> <p>All resources: cluster-deployment</p>"},{"location":"operations/containerization/k3s/installation/#k3s-deploysh","title":"k3s-deploy.sh","text":"<pre><code>#!/bin/bash\n\n\n\necho -e \" \\033[33;5m  _           _                     _                   _   _       _   __  __                    \\033[0m\"\necho -e \" \\033[33;5m | |         | |                   | |                 | | | |     (_) |  \\/  |                   \\033[0m\"\necho -e \" \\033[33;5m | |     __ _| |__   ___  _ __ __ _| |_ ___  _ __ _   _| | | |_   _ _  | \\  / | ___ _ __ _____  __\\033[0m\"\necho -e \" \\033[33;5m | |    / _\\` | '_ \\ / _ \\| '__/ _\\` | __/ _ \\| '__| | | | | | | | | | | | |\\/| |/ _ \\ '__/ _ \\ \\/ \\033[0m\"\necho -e \" \\033[33;5m | |___| (_| | |_) | (_) | | | (_| | || (_) | |  | |_| | | | | |_| | | | |  | |  __/ | | (_) &gt;  &lt; \\033[0m\"\necho -e \" \\033[33;5m |______\\__,_|_.__/ \\___/|_|  \\__,_|\\__\\___/|_|   \\__,_|_| |_|\\__,_|_| |_|  |_|\\___|_|  \\___/_/\\_\\033[0m\"\necho -e \"                                                                                                    \\033[0m\"\necho -e \"                                                                                                   \\033[0m\"\n\n\n#############################################\n# Thanks Jims Garage #\n#############################################\n\n# Version of Kube-VIP to deploy\nKVVERSION=\"v0.6.3\"\n\n# K3S Version\nk3sVersion=\"v1.26.10+k3s2\"\n\n# Set the IP addresses of the master and work nodes\nmaster1=X.X.X.X\nmaster2=X.X.X.X\nmaster3=X.X.X.X\nworker1=X.X.X.X\nworker2=X.X.X.X\nworker3=X.X.X.X\n\n# User of remote machines\nuser=merox\n\n# Interface used on remotes\ninterface=eth0\n\n# Set the virtual IP address (VIP)\nvip=X.X.X.X\n\n# Array of master nodes\nmasters=($master2 $master3)\n\n# Array of worker nodes\nworkers=($worker1 $worker2 $worker3)\n\n# Array of all\nall=($master1 $master2 $master3 $worker1 $worker2 $worker3)\n\n# Array of all minus master\nallnomaster1=($master2 $master3 $worker1 $worker2 $worker3)\n\n#Loadbalancer IP range\nlbrange=X.X.X.X-X.X.X.Y\n\n#ssh certificate name variable\ncertName=id_rsa\n\n#############################################\n#            DO NOT EDIT BELOW              #\n#############################################\n# For testing purposes - in case time is wrong due to VM snapshots\nsudo timedatectl set-ntp off\nsudo timedatectl set-ntp on\n\n# Move SSH certs to ~/.ssh and change permissions\ncp /home/$user/{$certName,$certName.pub} /home/$user/.ssh\nchmod 600 /home/$user/.ssh/$certName \nchmod 644 /home/$user/.ssh/$certName.pub\n\n# Install k3sup to local machine if not already present\nif ! command -v k3sup version &amp;&gt; /dev/null\nthen\n    echo -e \" \\033[31;5mk3sup not found, installing\\033[0m\"\n    curl -sLS https://get.k3sup.dev | sh\n    sudo install k3sup /usr/local/bin/\nelse\n    echo -e \" \\033[32;5mk3sup already installed\\033[0m\"\nfi\n\n# Install Kubectl if not already present\nif ! command -v kubectl version &amp;&gt; /dev/null\nthen\n    echo -e \" \\033[31;5mKubectl not found, installing\\033[0m\"\n    curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n    sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\nelse\n    echo -e \" \\033[32;5mKubectl already installed\\033[0m\"\nfi\n\n# Create SSH Config file to ignore checking (don't use in production!)\necho \"StrictHostKeyChecking no\" &gt; ~/.ssh/config\n\n#add ssh keys for all nodes\nfor node in \"${all[@]}\"; do\n  ssh-copy-id $user@$node\ndone\n\n# Install policycoreutils for each node\nfor newnode in \"${all[@]}\"; do\n  ssh $user@$newnode -i ~/.ssh/$certName sudo su &lt;&lt;EOF\n  NEEDRESTART_MODE=a apt install policycoreutils -y\n  exit\nEOF\n  echo -e \" \\033[32;5mPolicyCoreUtils installed!\\033[0m\"\ndone\n\n# Step 1: Bootstrap First k3s Node\nmkdir ~/.kube\nk3sup install \\\n  --ip $master1 \\\n  --user $user \\\n  --tls-san $vip \\\n  --cluster \\\n  --k3s-version $k3sVersion \\\n  --k3s-extra-args \"--disable traefik --disable servicelb --flannel-iface=$interface --node-ip=$master1 --node-taint node-role.kubernetes.io/master=true:NoSchedule\" \\\n  --merge \\\n  --sudo \\\n  --local-path $HOME/.kube/config \\\n  --ssh-key $HOME/.ssh/$certName \\\n  --context k3s-ha\necho -e \" \\033[32;5mFirst Node bootstrapped successfully!\\033[0m\"\n\n# Step 2: Install Kube-VIP for HA\nkubectl apply -f https://kube-vip.io/manifests/rbac.yaml\n\n# Step 3: Download kube-vip\ncurl -sO https://raw.githubusercontent.com/mer0x/merox.docs/k3s/K3S/cluster-deployment/kube-vip\ncat kube-vip | sed 's/$interface/'$interface'/g; s/$vip/'$vip'/g' &gt; $HOME/kube-vip.yaml\n\n# Step 4: Copy kube-vip.yaml to master1\nscp -i ~/.ssh/$certName $HOME/kube-vip.yaml $user@$master1:~/kube-vip.yaml\n\n\n# Step 5: Connect to Master1 and move kube-vip.yaml\nssh $user@$master1 -i ~/.ssh/$certName &lt;&lt;- EOF\n  sudo mkdir -p /var/lib/rancher/k3s/server/manifests\n  sudo mv kube-vip.yaml /var/lib/rancher/k3s/server/manifests/kube-vip.yaml\nEOF\n\n# Step 6: Add new master nodes (servers) &amp; workers\nfor newnode in \"${masters[@]}\"; do\n  k3sup join \\\n    --ip $newnode \\\n    --user $user \\\n    --sudo \\\n    --k3s-version $k3sVersion \\\n    --server \\\n    --server-ip $master1 \\\n    --ssh-key $HOME/.ssh/$certName \\\n    --k3s-extra-args \"--disable traefik --disable servicelb --flannel-iface=$interface --node-ip=$newnode --node-taint node-role.kubernetes.io/master=true:NoSchedule\" \\\n    --server-user $user\n  echo -e \" \\033[32;5mMaster node joined successfully!\\033[0m\"\ndone\n\n# add workers\nfor newagent in \"${workers[@]}\"; do\n  k3sup join \\\n    --ip $newagent \\\n    --user $user \\\n    --sudo \\\n    --k3s-version $k3sVersion \\\n    --server-ip $master1 \\\n    --ssh-key $HOME/.ssh/$certName \\\n    --k3s-extra-args \"--node-label \\\"longhorn=true\\\" --node-label \\\"worker=true\\\"\"\n  echo -e \" \\033[32;5mAgent node joined successfully!\\033[0m\"\ndone\n\n# Step 7: Install kube-vip as network LoadBalancer - Install the kube-vip Cloud Provider\nkubectl apply -f https://raw.githubusercontent.com/kube-vip/kube-vip-cloud-provider/main/manifest/kube-vip-cloud-controller.yaml\n\n# Step 8: Install Metallb\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml\n# Download ipAddressPool and configure using lbrange above\ncurl -sO https://raw.githubusercontent.com/mer0x/merox.docs/k3s/K3S/cluster-deployment/ipAddressPool\ncat ipAddressPool | sed 's/$lbrange/'$lbrange'/g' &gt; $HOME/ipAddressPool.yaml\n\n# Step 9: Test with Nginx\nkubectl apply -f https://raw.githubusercontent.com/inlets/inlets-operator/master/contrib/nginx-sample-deployment.yaml -n default\nkubectl expose deployment nginx-1 --port=80 --type=LoadBalancer -n default\n\necho -e \" \\033[32;5mWaiting for K3S to sync and LoadBalancer to come online\\033[0m\"\n\nwhile [[ $(kubectl get pods -l app=nginx -o 'jsonpath={..status.conditions[?(@.type==\"Ready\")].status}') != \"True\" ]]; do\n   sleep 1\ndone\n\n# Step 10: Deploy IP Pools and l2Advertisement\nkubectl wait --namespace metallb-system \\\n                --for=condition=ready pod \\\n                --selector=component=controller \\\n                --timeout=120s\nkubectl apply -f ipAddressPool.yaml\nkubectl apply -f https://raw.githubusercontent.com/mer0x/merox.docs/master/K3S/cluster-deployment/l2Advertisement.yaml\n\n# Step 11: Install Rancher (Optional - Delete if not required)\n#Install Helm\necho -e \" \\033[32;5mInstalling Helm\\033[0m\"\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n\n# Add Rancher Helm Repo &amp; create namespace\nhelm repo add rancher-latest https://releases.rancher.com/server-charts/latest\nkubectl create namespace cattle-system\n\n# Install Cert-Manager\necho -e \" \\033[32;5mDeploying Cert-Manager\\033[0m\"\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.2/cert-manager.crds.yaml\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install cert-manager jetstack/cert-manager \\\n--namespace cert-manager \\\n--create-namespace \\\n--version v1.13.2\nkubectl get pods --namespace cert-manager\n\n# Install Rancher\necho -e \" \\033[32;5mDeploying Rancher\\033[0m\"\nhelm install rancher rancher-latest/rancher \\\n --namespace cattle-system \\\n --set hostname=rancher.my.org \\\n --set bootstrapPassword=admin\nkubectl -n cattle-system rollout status deploy/rancher\nkubectl -n cattle-system get deploy rancher\n\n# Add Rancher LoadBalancer\nkubectl get svc -n cattle-system\nkubectl expose deployment rancher --name=rancher-lb --port=443 --type=LoadBalancer -n cattle-system\nwhile [[ $(kubectl get svc -n cattle-system 'jsonpath={..status.conditions[?(@.type==\"Pending\")].status}') = \"True\" ]]; do\n   sleep 5\n   echo -e \" \\033[32;5mWaiting for LoadBalancer to come online\\033[0m\" \ndone\n\n\n# Step 12: Install Longhorn (using modified Official to pin to Longhorn Nodes)\nkubectl apply -f https://raw.githubusercontent.com/mer0x/merox.docs/k3s/K3S/cluster-deployment/longhorn.yaml\nkubectl get pods \\\n--namespace longhorn-system \\\n--watch\n\nkubectl get nodes\nkubectl get svc\nkubectl get pods --all-namespaces -o wide\n\necho -e \" \\033[32;5mHappy Kubing! Access Nginx at EXTERNAL-IP above\\033[0m\"\n</code></pre>"},{"location":"operations/containerization/k3s/upgrade/","title":"Upgrade","text":""},{"location":"operations/containerization/k3s/upgrade/#how-to-upgrade-k3s-cluster","title":"How to upgrade K3S cluster","text":"<pre><code>#Upgrade master1\ncurl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh -s - server \\\n    --cluster-init \\\n    --tls-san x.x.x.x.88 \\\n    --disable traefik \\\n    --disable servicelb \\\n    --flannel-iface eth0 \\\n    --node-ip x.x.x.x.81 \\\n    --node-taint \"node-role.kubernetes.io/master=true:NoSchedule\"\n#Upgrade master2\n\ncurl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh -s - server \\\n    --server https://x.x.x.x.81:6443 \\\n    --disable traefik \\\n    --disable servicelb \\\n    --flannel-iface eth0 \\\n    --node-ip x.x.x.x.82 \\\n    --node-taint \"node-role.kubernetes.io/master=true:NoSchedule\"\n#Upgrade master3\n\ncurl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh -s - server \\\n    --server https://x.x.x.x.81:6443 \\\n    --disable traefik \\\n    --disable servicelb \\\n    --flannel-iface eth0 \\\n    --node-ip x.x.x.x.83 \\\n    --node-taint \"node-role.kubernetes.io/master=true:NoSchedule\"\n\n#Upgrade workers\n\ncurl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest K3S_URL=https://x.x.x.x.81:6443 K3S_TOKEN=/var/lib/rancher/k3s/server/node-token sh -s - agent \\\n    --node-label 'longhorn=true' \\\n    --node-label 'worker=true'\n</code></pre>"},{"location":"operations/containerization/k3s/upgrade/#how-to-upgrade-rancher","title":"How to upgrade Rancher","text":"<pre><code>helm upgrade rancher rancher-latest/rancher \\\n --namespace cattle-system \\\n --set hostname=rancher.my.org \\\n</code></pre>"},{"location":"operations/containerization/k3s/upgrade/#how-to-upgrade-longhorn","title":"How to upgrade Longhorn","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.3/deploy/longhorn.yaml\n</code></pre>"},{"location":"operations/containerization/k3s/upgrade/#how-to-upgrade-metallb","title":"How to upgrade Metallb","text":"<p>Warning</p> <p>Change version on the delete command to the version you are currently running (e.g., v0.13.11) Change version on the apply to the new version (e.g., v0.13.12) Ensure your Lbrange is still the one you want (check ipAddressPool.yaml)</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/metallb/metallb/v0.13.11/config/manifests/metallb-native.yaml\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml\nkubectl apply -f ipAddressPool.yaml\nkubectl apply -f https://raw.githubusercontent.com/mer0x/merox.cloud/k3s/K3S/cluster-deployment/l2Advertisement.yaml\n</code></pre>"},{"location":"operations/containerization/k3s/upgrade/#how-to-upgrade-kubevip","title":"How to upgrade KubeVIP","text":"<p>Warning</p> <p>Delete the daemonset in Rancher or use kubectl delete Redeploy the daemonset with updated values (check kube-vip file)</p> <pre><code>kubectl delete -f kube-vip\nkubectl apply -f kube-vip\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/argocd/","title":"ArgoCD","text":"<p>Install and configure ArgoCD</p> <ol> <li>Create a new namespace argocd and deploy ArgoCD with the web UI included. <pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre></li> <li>Log in to the ArgoCD web interface</li> </ol> <p>Log in to the ArgoCD web interface https://[your-dns-record/] by using the default username admin and the password, collected by the following command.</p> <pre><code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/cert-manager/","title":"Cert Manager","text":""},{"location":"operations/containerization/k3s/manifests/cert-manager/#production","title":"Production","text":""},{"location":"operations/containerization/k3s/manifests/cert-manager/#merox-productionyaml","title":"merox-production.yaml","text":"<pre><code>---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: merox.cloud # change to your domain\n  namespace: traefik # add to traefik namespace so it can use it \nspec:\n  secretName: X-tls # change to your secretname\n  issuerRef:\n    name: letsencrypt-production\n    kind: ClusterIssuer\n  commonName: \"*.merox.cloud\" # change to your domain\n  dnsNames:\n  - \"*.merox.cloud\" # change to your domain\n  - merox.cloud # change to your domain\n  - \"*.robertmelcher.ro\" # adauga al doilea domeniu\n  - robertmelcher.ro # adauga al doilea domeniu\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/cert-manager/#issuers","title":"Issuers","text":""},{"location":"operations/containerization/k3s/manifests/cert-manager/#secret-cf-tokenyaml","title":"secret-cf-token.yaml","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: cloudflare-token-secret\n  namespace: cert-manager\ntype: Opaque\nstringData:\n  cloudflare-token: ZzZZzZZzZzzZzZzZZwZ0 #  https://cert-manager.io/docs/configuration/acme/dns01/cloudflare/#api-tokens\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/cert-manager/#issuers_1","title":"Issuers","text":""},{"location":"operations/containerization/k3s/manifests/cert-manager/#letsencrypt-productionyaml","title":"letsencrypt-production.yaml","text":"<pre><code>---\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-production\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: e@mail.com # add your emaill\n    privateKeySecretRef:\n      name: letsencrypt-production\n    solvers:\n      - dns01:\n          cloudflare:\n            email: e@mail.com # add your email to your cloudflare account\n            apiTokenSecretRef:\n              name: homelab\n              key: key-homelab\n        selector:\n          dnsZones:\n            - \"merox.cloud\" # change to your zone on CloudFlare\n            - \"robertmelcher.ro\" # change to your zone on CloudFlare\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/cert-manager/#valuesyaml","title":"values.yaml","text":"<pre><code>installCRDs: false\nreplicaCount: 3 # change to number of masternodes\nextraArgs: # required for querying for certificate\n  - --dns01-recursive-nameservers=1.1.1.1:53,8.8.4.4:53\n  - --dns01-recursive-nameservers-only\npodDnsPolicy: None\npodDnsConfig:\n  nameservers:\n    - 1.1.1.1\n    - 8.8.4.4\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/dashboard/","title":"Homepage","text":""},{"location":"operations/containerization/k3s/manifests/dashboard/#dashboard","title":"Dashboard","text":"<ul> <li>If you're like me and enjoy having a dashboard in your homelab for all the services you run, for me, Homepage was the perfect solution. So, if you like what you see in the image below, let me tell you how to set it up.</li> </ul> <p>Warning</p> <p>Complete YAML on Github</p>"},{"location":"operations/containerization/k3s/manifests/dashboard/#resourcesyaml","title":"resources.yaml","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: merox-dashboard\n  namespace: default\n  labels:\n    app.kubernetes.io/service: merox-dashboard\n    app.kubernetes.io/instance: merox-dashboard\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: merox-dashboard\n    app.kubernetes.io/version: v0.6.10\n    helm.sh/chart: homepage-1.2.3\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: http\n      protocol: TCP\n      name: http\n  selector:\n    app.kubernetes.io/instance: merox-dashboard\n    app.kubernetes.io/name: merox-dashboard\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: merox-dashboard-homepage-logs\n  namespace: default\n  labels:\n    app.kubernetes.io/name: homepage\n    app.kubernetes.io/instance: merox-dashboard\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 2Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: merox-dashboard\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: merox-dashboard\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: merox-dashboard\n    app.kubernetes.io/version: v0.6.10\n    helm.sh/chart: homepage-1.2.3\nspec:\n  revisionHistoryLimit: 3\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: merox-dashboard\n      app.kubernetes.io/instance: merox-dashboard\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: merox-dashboard\n        app.kubernetes.io/instance: merox-dashboard\n    spec:\n      serviceAccountName: homepage\n      automountServiceAccountToken: true\n      dnsPolicy: ClusterFirst\n      enableServiceLinks: true\n      containers:\n        - name: merox-dashboard\n          image: \"ghcr.io/benphelps/homepage:v0.6.10\"\n          imagePullPolicy: IfNotPresent\n          ports:\n            - name: http\n              containerPort: 3000\n              protocol: TCP\n          volumeMounts:\n            - name: homepage-config\n              subPath: bookmarks.yaml\n              mountPath: /app/config/bookmarks.yaml\n            - name: homepage-config\n              subPath: docker.yaml\n              mountPath: /app/config/docker.yaml\n            - name: homepage-config\n              subPath: kubernetes.yaml\n              mountPath: /app/config/kubernetes.yaml\n            - name: homepage-config\n              subPath: services.yaml\n              mountPath: /app/config/services.yaml\n            - name: homepage-config\n              subPath: settings.yaml\n              mountPath: /app/config/settings.yaml\n            - name: homepage-config\n              subPath: widgets.yaml\n              mountPath: /app/config/widgets.yaml\n            - name: logs\n              mountPath: /app/config/logs\n      volumes:\n        - name: homepage-config\n          configMap:\n            name: merox-dashboard-homepage\n        - name: logs\n          persistentVolumeClaim:\n            claimName: merox-dashboard-homepage-logs\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/expose-service/","title":"Expose Service","text":"<p>Below, I will provide some YAML examples of how to expose both internal or external Kubernetes services through Traefik.</p>"},{"location":"operations/containerization/k3s/manifests/expose-service/#internal-service","title":"Internal service","text":""},{"location":"operations/containerization/k3s/manifests/expose-service/#wordpress-example","title":"WordPress example","text":""},{"location":"operations/containerization/k3s/manifests/expose-service/#robertmelcherro_wordpress_ingressyaml","title":"robertmelcherro_wordpress_ingress.yaml","text":"<pre><code>---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: wordpress-robertmelcher\n  namespace: external-websites\n  annotations:\n    kubernetes.io/ingress.class: traefik-external\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`www.robertmelcher.ro`) || Host(`robertmelcher.ro`)\n      kind: Rule\n      services:\n        - name: wordpress\n          port: 80\n  tls:\n    secretName: X-tls\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/expose-service/#external-service","title":"External service","text":""},{"location":"operations/containerization/k3s/manifests/expose-service/#external_serviceyaml","title":"external_service.yaml","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: synology\n  namespace: external-websites\nspec:\n  externalName: X.X.X.X\n  type: ExternalName\n  ports:\n  - name: websecure\n    port: 10003\n    targetPort: 10003\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/expose-service/#storage_ingressyaml","title":"storage_ingress.yaml","text":"<pre><code>---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: synology\n  namespace: external-websites\n  annotations:\n    kubernetes.io/ingress.class: traefik-external\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`www.sv1.merox.cloud`)\n      kind: Rule\n      services:\n        - name: synology\n          port: 10003\n          scheme: https\n          passHostHeader: true\n    - match: Host(`sv1.merox.cloud`)\n      kind: Rule\n      services:\n        - name: synology\n          port: 10003\n          scheme: https\n          passHostHeader: true\n      middlewares:\n        - name: default-headers\n  tls:\n    secretName: X-tls\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/loki/","title":"Loki","text":""},{"location":"operations/containerization/k3s/manifests/loki/#installing-loki-stack","title":"Installing Loki Stack","text":"<ol> <li>First add Loki\u2019s chart repository to helm <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\n</code></pre></li> <li>Then update the chart repository <pre><code>helm repo update\n</code></pre></li> </ol> <p>This command will:</p> <ul> <li>install grafana</li> <li>install prometheus</li> <li>install loki</li> <li>enable persistence for your stack and create a PVC <pre><code>helm upgrade --install loki grafana/loki-stack  --set grafana.enabled=true,prometheus.enabled=true,prometheus.alertmanager.persistentVolume.enabled=false,prometheus.server.persistentVolume.enabled=false,loki.persistence.enabled=true,loki.persistence.storageClassName=longhorn,loki.persistence.size=5Gi\n</code></pre> <p>To configure your StorageClass, you should specify loki.persistence.storageClassName=longhorn. In this instance, I am utilizing longhorn as the Kubernetes loghorn Provisioner.</p> </li> </ul>"},{"location":"operations/containerization/k3s/manifests/media-stack/","title":"Deploying Media Stack on Kubernetes with Longhorn and NFS Storage","text":""},{"location":"operations/containerization/k3s/manifests/media-stack/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster</li> <li>kubectl configured for your cluster</li> <li>Longhorn installed on your Kubernetes cluster</li> <li>An NFS server configured and accessible</li> <li>Traefik Ingress controller installed in your Kubernetes cluster</li> </ul>"},{"location":"operations/containerization/k3s/manifests/media-stack/#pv-pvc","title":"PV &amp; PVC","text":"<p><pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: qbitt-download\nspec:\n  capacity:\n    storage: 400Gi\n  accessModes:\n    - ReadWriteOnce\n  nfs:\n    path: /volume1/Server/Data/alto/media_nas/qbittorrent/\n    server: storage.merox.cloud\n  persistentVolumeReclaimPolicy: Retain\n  mountOptions:\n    - hard\n    - nfsvers=3\n  storageClassName: \"\"\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: qbitt-download\n  namespace: media\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 400Gi\n  volumeName: qbitt-download\n  storageClassName: \"\"\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: jellyfin-videos\nspec:\n  capacity:\n    storage: 400Gi\n  accessModes:\n    - ReadWriteOnce\n  nfs:\n    path: /volume1/Server/Data/alto/media_nas\n    server: storage.merox.cloud\n  persistentVolumeReclaimPolicy: Retain\n  mountOptions:\n    - hard\n    - nfsvers=3\n  storageClassName: \"\"\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: jellyfin-videos\n  namespace: media\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 400Gi\n  volumeName: jellyfin-videos\n  storageClassName: \"\"\n</code></pre></p>"},{"location":"operations/containerization/k3s/manifests/media-stack/#jellyfin","title":"Jellyfin","text":"<p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jellyfin\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jellyfin\n  template:\n    metadata:\n      labels:\n        app: jellyfin\n    spec:\n      containers:\n      - name: jellyfin\n        image: jellyfin/jellyfin\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: videos\n          mountPath: /data/videos\n        ports:\n        - containerPort: 8096\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: jellyfin-config\n      - name: videos\n        persistentVolumeClaim:\n          claimName: jellyfin-videos\n</code></pre> <pre><code>---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: jellyfin\n  namespace: media\n  annotations:\n    kubernetes.io/ingress.class: traefik-external\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`www.jellyfin.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: jellyfin\n          port: 80\n    - match: Host(`jellyfin.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: jellyfin\n          port: 80\n      middlewares:\n        - name: default-headers-jellyfin\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: jellyfin-config\n  namespace: media\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: jellyfin\n  namespace: media\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 8096\n  selector:\n    app: jellyfin\n</code></pre></p>"},{"location":"operations/containerization/k3s/manifests/media-stack/#sonarr","title":"Sonarr","text":"<p><pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: sonarr-config\n  namespace: media\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sonarr\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sonarr\n  template:\n    metadata:\n      labels:\n        app: sonarr\n    spec:\n      containers:\n      - name: sonarr\n        image: linuxserver/sonarr\n        env:\n        - name: PUID\n          value: \"1057\"\n        - name: PGID\n          value: \"1056\"\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: videos\n          mountPath: /tv\n        - name: downloads\n          mountPath: /downloads\n        ports:\n        - containerPort: 8989\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: sonarr-config\n      - name: videos\n        persistentVolumeClaim:\n          claimName: jellyfin-videos\n      - name: downloads\n        persistentVolumeClaim:\n          claimName: qbitt-download  \n</code></pre> <pre><code>---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: sonarr\n  namespace: media\n  annotations:\n    kubernetes.io/ingress.class: traefik-external\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`www.tv.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: sonarr\n          port: 80\n    - match: Host(`tv.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: sonarr\n          port: 80\n      middlewares:\n        - name: default-headers-jellyfin\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: sonarr\n  namespace: media\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 8989\n  selector:\n    app: sonarr\n</code></pre></p>"},{"location":"operations/containerization/k3s/manifests/media-stack/#radarr","title":"Radarr","text":"<p><pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: radarr-config\n  namespace: media\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: radarr\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: radarr\n  template:\n    metadata:\n      labels:\n        app: radarr\n    spec:\n#      initContainers:\n#      - name: set-perms\n#        image: alpine\n#        command: ['sh', '-c', 'chown -R 1057:1056 /movies']\n      containers:\n      - name: radarr\n        image: linuxserver/radarr\n        env:\n        - name: PUID\n          value: \"1057\"  \n        - name: PGID\n          value: \"1056\"  \n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: videos\n          mountPath: /movies\n        - name: downloads\n          mountPath: /downloads\n        ports:\n        - containerPort: 7878\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: radarr-config\n      - name: videos\n        persistentVolumeClaim:\n          claimName: jellyfin-videos\n      - name: downloads\n        persistentVolumeClaim:\n          claimName: qbitt-download  \n``` yaml linenums=\"1\"\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: radarr\n  namespace: media\n  annotations:\n    kubernetes.io/ingress.class: traefik-external\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`movies.tv.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: radarr\n          port: 80\n    - match: Host(`movies.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: radarr\n          port: 80\n      middlewares:\n        - name: default-headers-jellyfin\n\n``` yaml linenums=\"1\"\napiVersion: v1\nkind: Service\nmetadata:\n  name: radarr\n  namespace: media\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 7878\n  selector:\n    app: radarr\n</code></pre></p>"},{"location":"operations/containerization/k3s/manifests/media-stack/#jackett","title":"Jackett","text":"<p><pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: jackett-config\n  namespace: media\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jackett\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jackett\n  template:\n    metadata:\n      labels:\n        app: jackett\n    spec:\n      containers:\n      - name: jackett\n        image: linuxserver/jackett\n        env:\n        - name: PUID\n          value: \"1057\"  # Ajusteaz\u0103 aceast\u0103 valoare conform nevoilor tale\n        - name: PGID\n          value: \"1056\"  # Ajusteaz\u0103 aceast\u0103 valoare conform nevoilor tale\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        ports:\n        - containerPort: 9117\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: jackett-config\n</code></pre> <pre><code>---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: jackett\n  namespace: media\n  annotations:\n    kubernetes.io/ingress.class: traefik-external\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`www.jackett.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: jackett\n          port: 80\n    - match: Host(`jackett.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: jackett\n          port: 80\n      middlewares:\n        - name: default-headers-jellyfin\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: jackett\n  namespace: media\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 9117\n  selector:\n    app: jackett\n</code></pre></p>"},{"location":"operations/containerization/k3s/manifests/media-stack/#qbittorrent","title":"qBittorrent","text":"<p><pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: qbitt-config\n  namespace: media\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: qbittorrent\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: qbittorrent\n  template:\n    metadata:\n      labels:\n        app: qbittorrent\n    spec:\n      containers:\n      - name: qbittorrent\n        image: linuxserver/qbittorrent\n        resources:\n          limits:\n            memory: \"2Gi\"\n          requests:\n            memory: \"512Mi\"\n        env:\n        - name: PUID\n          value: \"1057\"  \n        - name: PGID\n          value: \"1056\" \n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: downloads\n          mountPath: /downloads\n        ports:\n        - containerPort: 8080\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: qbitt-config\n      - name: downloads\n        persistentVolumeClaim:\n          claimName: qbitt-download \n</code></pre> <pre><code>---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: qbittorrent\n  namespace: media\n  annotations:\n    kubernetes.io/ingress.class: traefik-external\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`www.qbitt.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: qbittorrent\n          port: 80\n    - match: Host(`qbitt.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: qbittorrent\n          port: 80\n      middlewares:\n        - name: default-headers-jellyfin\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: qbittorrent\n  namespace: media\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 8080\n  selector:\n    app: qbittorrent\n</code></pre></p>"},{"location":"operations/containerization/k3s/manifests/media-stack/#qbittorrent-with-gluetun","title":"qBittorrent with Gluetun","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: qbittorrent\n  namespace: media\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: qbittorrent\n  template:\n    metadata:\n      labels:\n        app: qbittorrent\n    spec:\n      containers:\n        - name: qbittorrent\n          image: linuxserver/qbittorrent\n          resources:\n            limits:\n              memory: \"2Gi\"\n            requests:\n              memory: \"512Mi\"\n          env:\n           - name: PUID\n             value: \"1057\"\n           - name: PGID\n             value: \"1056\"\n          volumeMounts:\n            - name: config\n              mountPath: /config\n            - name: downloads\n              mountPath: /downloads\n          ports:\n            - containerPort: 8080\n\n        - name: gluetun\n          image: qmcgaw/gluetun\n          env:\n            - name: VPNSP\n              value: \"protonvpn\"\n            - name: OPENVPN_USER\n              valueFrom:\n                secretKeyRef:\n                  name: protonvpn-secrets\n                  key: PROTONVPN_USER\n            - name: OPENVPN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: protonvpn-secrets\n                  key: PROTONVPN_PASSWORD\n            - name: COUNTRY\n              value: \"Germany\" \n          securityContext:\n            capabilities:\n              add:\n                - NET_ADMIN\n          volumeMounts:\n            - name: gluetun-config\n              mountPath: /gluetun\n\n      volumes:\n        - name: config\n          persistentVolumeClaim:\n            claimName: qbitt-config\n        - name: downloads\n          persistentVolumeClaim:\n            claimName: qbitt-download\n        - name: gluetun-config\n          persistentVolumeClaim:\n            claimName: gluetun-config\n</code></pre> <p>Example</p> <p>I've chosen to use ProtonVPN due to their security policy and because they do not collect/store data, but also because of the speeds and diverse settings, all at a very good price</p>"},{"location":"operations/containerization/k3s/manifests/monitor-stack/","title":"Monitor Stack","text":"<p>Thanks TechnoTim</p>"},{"location":"operations/containerization/k3s/manifests/monitor-stack/#create-a-kubernetes-namespace","title":"Create a Kubernetes Namespace","text":"<pre><code>kubectl create namespace monitoring\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/monitor-stack/#username-and-password","title":"username and password","text":"<pre><code>echo -n 'adminuser' &gt; ./admin-user\necho -n 'p@ssword!' &gt; ./admin-password\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/monitor-stack/#create-a-kubernetes-secret","title":"Create a Kubernetes Secret","text":"<pre><code> kubectl create secret generic grafana-admin-credentials --from-file=./admin-user --from-file=admin-password -n monitoring\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/monitor-stack/#verify-your-secret","title":"Verify your secret","text":"<pre><code>kubectl describe secret -n monitoring grafana-admin-credentials\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/monitor-stack/#you-should-see","title":"You should see","text":"<pre><code>Name:         grafana-admin-credentials\nNamespace:    monitoring\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\n\nType:  Opaque\n\nData\n====\nadmin-password:  9 bytes\nadmin-user:      9 bytes\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/monitor-stack/#create-a-values-file","title":"Create a values file","text":""},{"location":"operations/containerization/k3s/manifests/monitor-stack/#copy-values","title":"Copy values","text":"<p>Github <pre><code>nano values.yaml\n</code></pre></p>"},{"location":"operations/containerization/k3s/manifests/monitor-stack/#install-helm-chart","title":"Install helm chart","text":"<pre><code>helm install -n monitoring prometheus prometheus-community/kube-prometheus-stack -f values.yaml\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/netdata/","title":"Netdata","text":""},{"location":"operations/containerization/k3s/manifests/netdata/#install-netdata-via-the-helm-install-command","title":"Install Netdata via the helm install command","text":"<ol> <li> <p>Add the Netdata Helm chart repository by running: <pre><code>helm repo add netdata https://netdata.github.io/helmchart/\n</code></pre></p> </li> <li> <p>To install Netdata using the helm install command, run: <pre><code>helm install netdata netdata/netdata\n</code></pre></p> </li> <li>If you want to deploy with specific parameters: <pre><code>helm install netdata netdata/netdata --set service.type=LoadBalancer,service.loadBalancerIP=\"X.X.X.X\",service.port=19900,parent.database.storageclass=longhorn\n</code></pre> More info: netdata docs</li> </ol>"},{"location":"operations/containerization/k3s/manifests/nfs-share/","title":"Adding NFS Server as Storage Class on Kubernetes","text":"<p>This guide walks you through the process of adding a Network File System (NFS) server as a Storage Class in Kubernetes, enabling you to use NFS for persistent storage.</p>"},{"location":"operations/containerization/k3s/manifests/nfs-share/#prerequisites","title":"Prerequisites","text":"<ul> <li>An NFS server setup and configured.</li> <li>A Kubernetes cluster with administrative access.</li> </ul>"},{"location":"operations/containerization/k3s/manifests/nfs-share/#step-1-installing-and-configuring-the-nfs-server","title":"Step 1: Installing and Configuring the NFS Server","text":"<p>Ensure your NFS server is correctly set up and that the directory you wish to share is configured in the <code>/etc/exports</code> file on your NFS server.</p>"},{"location":"operations/containerization/k3s/manifests/nfs-share/#step-2-creating-a-persistent-volume-on-kubernetes","title":"Step 2: Creating a Persistent Volume on Kubernetes","text":"<p>Create a Persistent Volume (PV) that references your NFS server and the path to the shared directory. Here is an example YAML configuration:</p> <p><pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-nfs-kubedata-nginx-1  # &lt; Name of the persistent volume\n  namespace: default    \nspec:\n  storageClassName: \"\"\n  capacity:\n    storage: 1Gi # &lt; Maximum storage size you want to reserve\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  nfs:\n    server: xxx.xxx.xxx.xxx  # &lt; The ip adress of your NAS (NFS Server)\n    path: \"/volume1/kubedata/nginx-1\"  # &lt; The NFS volumename \n    readOnly: false\n</code></pre> Replace nfs-server.your-domain.com with the IP or domain name of your NFS server and /var/nfs/general with the path to your shared directory. Step 3: Creating a Storage Class for NFS</p> <p>Since NFS does not support dynamic provisioning directly through Kubernetes without an external provisioner, you might need to use an external provisioner like NFS Subdir External Provisioner for dynamic storage provisioning. Step 4: Creating a Persistent Volume Claim</p> <p>Applications will use a Persistent Volume Claim (PVC) to request storage. Below is an example PVC configuration:</p> <p><pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n  volumeName: nfs-pv\n</code></pre> Step 5: Using the PVC in Kubernetes Pods</p> <p>You can now reference the PVC in your pod definitions to mount the NFS volume. Here's how:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  containers:\n  - name: example-container\n    image: nginx\n    volumeMounts:\n    - mountPath: \"/usr/share/nginx/html\"\n      name: nfs-volume\n  volumes:\n  - name: nfs-volume\n    persistentVolumeClaim:\n      claimName: nfs-pvc\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/traefik/","title":"Traefik","text":"<p>The Cloud Native Application Proxy Simplify networking complexity while designing, deploying, and operating applications.</p>"},{"location":"operations/containerization/k3s/manifests/traefik/#valuesyaml","title":"values.yaml","text":"<pre><code>globalArguments:\n  - \"--global.sendanonymoususage=false\"\n  - \"--global.checknewversion=false\"\n\nadditionalArguments:\n  - \"--serversTransport.insecureSkipVerify=true\"\n  - \"--log.level=INFO\"\n\ndeployment:\n  enabled: true\n  replicas: 3 # match with number of workers\n  annotations: {}\n  podAnnotations: {}\n  additionalContainers: []\n  initContainers: []\n\nnodeSelector: \n  worker: \"true\" # add these labels to your worker nodes before running\n\nports:\n  web:\n    redirectTo:\n      port: websecure\n      priority: 10\n  websecure:\n    tls:\n      enabled: true\n\ningressRoute:\n  dashboard:\n    enabled: false\n\nproviders:\n  kubernetesCRD:\n    enabled: true\n    ingressClass: traefik-external\n    allowExternalNameServices: true\n  kubernetesIngress:\n    enabled: true\n    allowExternalNameServices: true\n    publishedService:\n      enabled: false\n\nrbac:\n  enabled: true\n\nservice:\n  enabled: true\n  type: LoadBalancer\n  annotations: {}\n  labels: {}\n  spec:\n    loadBalancerIP: X.X.X.100 # this should be an IP in the Kube-VIP range\n  loadBalancerSourceRanges: []\n  externalIPs: []\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/traefik/#middlewareyaml","title":"middleware.yaml","text":"<pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: default-headers\n  namespace: traefik\nspec:\n  headers:\n    browserXssFilter: true\n    contentTypeNosniff: true\n    forceSTSHeader: true\n    stsIncludeSubdomains: true\n    stsPreload: true\n    stsSeconds: 15552000\n    customFrameOptionsValue: SAMEORIGIN\n    customRequestHeaders:\n      X-Forwarded-Proto: https\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/traefik/#traefik-dashboard","title":"Traefik Dashboard","text":""},{"location":"operations/containerization/k3s/manifests/traefik/#traefik_ingressyaml","title":"traefik_ingress.yaml","text":"<pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard\n  namespace: traefik\n  annotations: \n    kubernetes.io/ingress.class: traefik-external\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`traefik.merox.cloud`) \n      kind: Rule\n      middlewares:\n        - name: traefik-dashboard-basicauth\n          namespace: traefik\n      services:\n        - name: api@internal\n          kind: TraefikService\n  tls:\n    secretName: X-tls \n</code></pre>"},{"location":"operations/containerization/k3s/manifests/traefik/#secret-dashboardyaml","title":"secret-dashboard.yaml","text":"<pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: traefik-dashboard-auth\n  namespace: traefik\ntype: Opaque\ndata:\n  users: 2YW3sSRtaW4nOIfbenjenwfewipo732tdSadUuRU0wRzZSLg==\n\n  ```\n\n### middleware.yaml\n``` yaml linenums=\"1\"\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: traefik-dashboard-basicauth\n  namespace: traefik\nspec:\n  basicAuth:\n    secret: traefik-dashboard-auth\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/wikijs/","title":"WikiJS","text":""},{"location":"operations/containerization/k3s/manifests/wikijs/#wikijs-configyaml","title":"wikijs-config.yaml","text":"<pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mariadb\n  namespace: wikijs\nspec:\n  selector:\n    app: mariadb\n  ports:\n  - name: mariadb\n    protocol: TCP\n    port: 3306\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mariadb\n  namespace: wikijs\n  labels:\n    app: mariadb\nspec:\n  selector:\n    matchLabels:\n      app: mariadb\n  template:\n    metadata:\n      labels:\n        app: mariadb\n    spec:\n      containers:\n      - name: mariadb\n        image: mariadb:latest\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-secret\n              key: ROOT_PASSWORD\n        - name: MYSQL_DATABASE\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-secret\n              key: DATABASE\n        - name: MYSQL_USER\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-secret\n              key: USER\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-secret\n              key: PASSWORD\n        - name: MARIADB_ROOT_HOST\n          value: \"%\"\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: wikijs-db\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: wikijs-db\n        persistentVolumeClaim:\n          claimName: wikijs-pv-claim\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/wikijs/#wikijs-deploymentyaml","title":"wikijs-deployment.yaml","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wikijs\n  namespace: wikijs\n  labels:\n    app: wikijs\nspec:\n  selector:\n    matchLabels:\n      app: wikijs\n  template:\n    metadata:\n      labels:\n        app: wikijs\n    spec:\n      containers:\n      - name: wikijs\n        image: requarks/wiki:latest\n        imagePullPolicy: Always\n        env:\n        - name: DB_TYPE\n          value: \"mariadb\"\n        - name: DB_HOST\n          value: \"mariadb\"\n        - name: DB_PORT\n          value: \"3306\"\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-secret\n              key: DATABASE\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-secret\n              key: USER\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: mariadb-secret\n              key: PASSWORD\n        ports:\n        - containerPort: 3000\n          name: http\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/wikijs/#wikijs-ingressyaml","title":"wikijs-ingress.yaml","text":"<pre><code>---\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: wikijs\n  namespace: wikijs\n  annotations: \n    kubernetes.io/ingress.class: traefik-external\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`www.merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: wikijs\n          port: 3000\n    - match: Host(`merox.cloud`) # change to your domain\n      kind: Rule\n      services:\n        - name: wikijs\n          port: 3000\n      middlewares:\n        - name: wikijs-middleware\n  tls:\n    secretName: mer0x39-tls # change to your cert name\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/wikijs/#wikijs-middlewareyaml","title":"wikijs-middleware.yaml","text":"<pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: wikijs-middleware\n  namespace: wikijs\nspec:\n  headers:\n    browserXssFilter: true\n    contentTypeNosniff: true\n    forceSTSHeader: true\n    stsIncludeSubdomains: true\n    stsPreload: true\n    stsSeconds: 15552000\n    customFrameOptionsValue: SAMEORIGIN\n    customRequestHeaders:\n      X-Forwarded-Proto: https\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/wikijs/#wikijs-pvcyaml","title":"wikijs-pvc.yaml","text":"<pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: wikijs-pv-claim\n  namespace: wikijs\nspec:\n  storageClassName: longhorn\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/wikijs/#wikijs-secretyaml","title":"wikijs-secret.yaml","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mariadb-secret\n  namespace: wikijs\ntype: Opaque\ndata:\n  ROOT: cm9vdA==\n  ROOT_PASSWORD: VGhpdqw#@423yISE1Nw==\n  DATABASE: zdzd22lrsaWpz\n  USER: zdzd22lrsaWpz\n  PASSWORD: VGhpdqw#@423yISE1Nw==\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/wikijs/#wikijs-serviceyaml","title":"wikijs-service.yaml","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: \"wikijs\"\n  namespace: wikijs\nspec:\n  type: ClusterIP\n  ports:\n    - name: http\n      port: 3000\n  selector:\n    app: \"wikijs\"\n</code></pre>"},{"location":"operations/containerization/k3s/manifests/wordpress/","title":"WordPress","text":""},{"location":"operations/containerization/k3s/manifests/wordpress/#introduction","title":"Introduction","text":"<ol> <li>This chart bootstraps a WordPress deployment on a Kubernetes cluster using the Helm package manager.</li> </ol> <p>It also packages the Bitnami MariaDB chart which is required for bootstrapping a MariaDB deployment for the database requirements of the WordPress application, and the Bitnami Memcached chart that can be used to cache database queries.</p> <p>Bitnami charts can be used with Kubeapps for deployment and management of Helm Charts in clusters.</p> <p>Prerequisites:</p> <p>Kubernetes 1.23+    Helm 3.8.0+    PV provisioner support in the underlying infrastructure    ReadWriteMany volumes for deployment scaling</p> <ol> <li> <p>Complete YAML on Github</p> </li> <li> <p>P.S: Interesting tutorial for Horizontally Scalable</p> </li> </ol> <pre><code>affinity: {}\nallowEmptyPassword: true\nallowOverrideNone: false\napacheConfiguration: ''\nargs: []\nautomountServiceAccountToken: false\nautoscaling:\n  enabled: false\n  maxReplicas: 11\n  minReplicas: 1\n  targetCPU: 50\n  targetMemory: 50\nclusterDomain: cluster.local\ncommand: []\ncommonAnnotations: {}\ncommonLabels: {}\ncontainerPorts:\n  http: 8080\n  https: 8443\ncontainerSecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n      - ALL\n  enabled: true\n  privileged: false\n  readOnlyRootFilesystem: false\n  runAsNonRoot: true\n  runAsUser: 1001\n  seLinuxOptions: {}\n  seccompProfile:\n    type: RuntimeDefault\ncustomHTAccessCM: ''\ncustomLivenessProbe: {}\ncustomPostInitScripts: {}\ncustomReadinessProbe: {}\ncustomStartupProbe: {}\ndiagnosticMode:\n  args:\n    - infinity\n  command:\n    - sleep\n  enabled: false\nexistingApacheConfigurationConfigMap: ''\nexistingSecret: ''\nexistingWordPressConfigurationSecret: ''\nexternalCache:\n  host: localhost\n  port: 11211\nexternalDatabase:\n  database: bitnami_wordpress\n  existingSecret: ''\n  host: localhost\n  password: 'defaultOne'\n  port: 3306\n  user: bn_wordpress\nextraContainerPorts: []\nextraDeploy: []\nextraEnvVars: []\nextraEnvVarsCM: ''\nextraEnvVarsSecret: ''\nextraVolumeMounts: []\nextraVolumes: []\nfullnameOverride: ''\nglobal:\n  imagePullSecrets: []\n  imageRegistry: ''\n  storageClass: 'longhorn'\nhostAliases:\n  - hostnames:\n      - status.localhost\n    ip: 127.0.0.1\nhtaccessPersistenceEnabled: false\nimage:\n  debug: false\n  digest: ''\n  pullPolicy: IfNotPresent\n  pullSecrets: []\n  registry: docker.io\n  repository: bitnami/wordpress\n  tag: 6.4.2-debian-11-r18\ningress:\n  annotations: {}\n  apiVersion: ''\n  enabled: false\n  extraHosts: []\n  extraPaths: []\n  extraRules: []\n  extraTls: []\n  hostname: wordpress.local\n  ingressClassName: ''\n  path: /\n  pathType: ImplementationSpecific\n  secrets: []\n  selfSigned: false\n  tls: false\n  tlsWwwPrefix: false\ninitContainers: []\nkubeVersion: ''\nlifecycleHooks: {}\nlivenessProbe:\n  enabled: true\n  failureThreshold: 6\n  httpGet:\n    httpHeaders: []\n    path: /wp-admin/install.php\n    port: '{{ .Values.wordpressScheme }}'\n    scheme: '{{ .Values.wordpressScheme | upper }}'\n  initialDelaySeconds: 120\n  periodSeconds: 10\n  successThreshold: 1\n  timeoutSeconds: 5\nmariadb:\n  architecture: standalone\n  auth:\n    database: bitnami_wordpress\n    password: 'defaultOne'\n    rootPassword: 'defaultOneRoot'\n    username: bn_wordpress\n  enabled: true\n  primary:\n    persistence:\n      accessModes:\n        - ReadWriteOnce\n      enabled: true\n      size: 8Gi\n      storageClass: ''\nmemcached:\n  auth:\n    enabled: false\n    existingPasswordSecret: ''\n    password: ''\n    username: ''\n  enabled: false\n  service:\n    port: 11211\nmetrics:\n  containerPorts:\n    metrics: 9117\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop:\n        - ALL\n    enabled: true\n    privileged: false\n    readOnlyRootFilesystem: false\n    runAsNonRoot: true\n    runAsUser: 1001\n    seLinuxOptions: {}\n    seccompProfile:\n      type: RuntimeDefault\n  customLivenessProbe: {}\n  customReadinessProbe: {}\n  customStartupProbe: {}\n  enabled: false\n  image:\n    digest: ''\n    pullPolicy: IfNotPresent\n    pullSecrets: []\n    registry: docker.io\n    repository: bitnami/apache-exporter\n    tag: 1.0.5-debian-11-r3\n  livenessProbe:\n    enabled: true\n    failureThreshold: 3\n    initialDelaySeconds: 15\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 5\n  readinessProbe:\n    enabled: true\n    failureThreshold: 3\n    initialDelaySeconds: 5\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 3\n  resources:\n    limits: {}\n    requests: {}\n  service:\n    annotations:\n      prometheus.io/port: '{{ .Values.metrics.containerPorts.metrics }}'\n      prometheus.io/scrape: 'true'\n    ports:\n      metrics: 9150\n  serviceMonitor:\n    enabled: false\n    honorLabels: false\n    interval: ''\n    jobLabel: ''\n    labels: {}\n    metricRelabelings: []\n    namespace: ''\n    relabelings: []\n    scrapeTimeout: ''\n    selector: {}\n  startupProbe:\n    enabled: false\n    failureThreshold: 15\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 1\nmultisite:\n  enable: false\n  enableNipIoRedirect: false\n  host: ''\n  networkType: subdomain\nnameOverride: ''\nnetworkPolicy:\n  egressRules:\n    customRules: {}\n    denyConnectionsToExternal: false\n  enabled: false\n  ingress:\n    enabled: false\n    namespaceSelector: {}\n    podSelector: {}\n  ingressRules:\n    accessOnlyFrom:\n      enabled: false\n      namespaceSelector: {}\n      podSelector: {}\n    backendOnlyAccessibleByFrontend: false\n    customBackendSelector: {}\n    customRules: {}\n  metrics:\n    enabled: false\n    namespaceSelector: {}\n    podSelector: {}\nnodeAffinityPreset:\n  key: ''\n  type: ''\n  values: []\nnodeSelector: {}\noverrideDatabaseSettings: false\npdb:\n  create: false\n  maxUnavailable: ''\n  minAvailable: 1\npersistence:\n  accessMode: ReadWriteOnce\n  accessModes:\n    - ReadWriteOnce\n  annotations: {}\n  dataSource: {}\n  enabled: true\n  existingClaim: ''\n  selector: {}\n  size: 10Gi\n  storageClass: ''\npodAffinityPreset: ''\npodAnnotations: {}\npodAntiAffinityPreset: soft\npodLabels: {}\npodSecurityContext:\n  enabled: true\n  fsGroup: 1001\n  fsGroupChangePolicy: Always\n  supplementalGroups: []\n  sysctls: []\npriorityClassName: ''\nreadinessProbe:\n  enabled: true\n  failureThreshold: 6\n  httpGet:\n    httpHeaders: []\n    path: /wp-login.php\n    port: '{{ .Values.wordpressScheme }}'\n    scheme: '{{ .Values.wordpressScheme | upper }}'\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  successThreshold: 1\n  timeoutSeconds: 5\nreplicaCount: 1\nresources:\n  limits: {}\n  requests:\n    cpu: 300m\n    memory: 512Mi\nschedulerName: ''\nservice:\n  annotations: {}\n  clusterIP: ''\n  externalTrafficPolicy: Cluster\n  extraPorts: []\n  httpsTargetPort: https\n  loadBalancerIP: ''\n  loadBalancerSourceRanges: []\n  nodePorts:\n    http: ''\n    https: ''\n  ports:\n    http: 80\n    https: 443\n  sessionAffinity: None\n  sessionAffinityConfig: {}\n  type: LoadBalancer\nserviceAccount:\n  annotations: {}\n  automountServiceAccountToken: false\n  create: true\n  name: ''\nsidecars: []\nsmtpExistingSecret: ''\nsmtpHost: ''\nsmtpPassword: ''\nsmtpPort: ''\nsmtpProtocol: ''\nsmtpUser: ''\nstartupProbe:\n  enabled: false\n  failureThreshold: 6\n  httpGet:\n    httpHeaders: []\n    path: /wp-login.php\n    port: '{{ .Values.wordpressScheme }}'\n    scheme: '{{ .Values.wordpressScheme | upper }}'\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  successThreshold: 1\n  timeoutSeconds: 5\nterminationGracePeriodSeconds: ''\ntolerations: []\ntopologySpreadConstraints: []\nupdateStrategy:\n  type: RollingUpdate\nvolumePermissions:\n  containerSecurityContext:\n    runAsUser: 0\n    seLinuxOptions: {}\n  enabled: false\n  image:\n    digest: ''\n    pullPolicy: IfNotPresent\n    pullSecrets: []\n    registry: docker.io\n    repository: bitnami/os-shell\n    tag: 11-debian-11-r95\n  resources:\n    limits: {}\n    requests: {}\nwordpressBlogName: Merox's Blog!\nwordpressConfiguration: ''\nwordpressConfigureCache: false\nwordpressEmail: melcher95@gmail.com\nwordpressExtraConfigContent: ''\nwordpressFirstName: Robert\nwordpressLastName: Melcher\nwordpressPassword: 'defaultOne2'\nwordpressPlugins: none\nwordpressScheme: http\nwordpressSkipInstall: false\nwordpressTablePrefix: wp_\nwordpressUsername: user\n</code></pre>"},{"location":"operations/monitoring/grafana/basics/","title":"Best Practices for Configuring Grafana Dashboards","text":"<p>Grafana is a powerful tool for visualizing and analyzing metrics from various data sources. This guide provides best practices for configuring and installing Grafana dashboards, ensuring you can effectively monitor your systems and applications.</p>"},{"location":"operations/monitoring/grafana/basics/#installation","title":"Installation","text":"<ol> <li>Download Grafana:</li> <li> <p>Visit the official Grafana download page and select the version compatible with your operating system.</p> </li> <li> <p>Installation Process:</p> </li> <li>For Debian/Ubuntu systems, use:      <pre><code>sudo apt-get install -y software-properties-common\nsudo add-apt-repository \"deb https://packages.grafana.com/oss/deb stable main\"\nsudo wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install grafana\n</code></pre></li> <li> <p>For RHEL/CentOS systems, use:      <pre><code>sudo yum install https://dl.grafana.com/oss/release/grafana-&lt;version&gt;.rpm\n</code></pre>    Replace <code>&lt;version&gt;</code> with the desired Grafana version number.</p> </li> <li> <p>Start Grafana:</p> </li> <li>Enable and start the Grafana service:      <pre><code>sudo systemctl enable --now grafana-server\n</code></pre></li> </ol>"},{"location":"operations/monitoring/grafana/basics/#configuring-data-sources","title":"Configuring Data Sources","text":"<ol> <li>Access Grafana Dashboard:</li> <li> <p>Open your web browser and navigate to <code>http://&lt;YourServerIP&gt;:3000</code>. The default login credentials are <code>admin</code> for both username and password.</p> </li> <li> <p>Add Data Source:</p> </li> <li>Click on the Gear icon in the sidebar &gt; Data Sources &gt; Add data source.</li> <li>Select your data source type (e.g., Prometheus, InfluxDB, MySQL).</li> <li>Configure the data source settings specific to your source (URL, database, user, password).</li> </ol>"},{"location":"operations/monitoring/grafana/basics/#creating-dashboards","title":"Creating Dashboards","text":"<ol> <li>Create a New Dashboard:</li> <li>Click the + icon in the sidebar &gt; Dashboard.</li> <li> <p>Choose Add new panel to start customizing your first panel.</p> </li> <li> <p>Configure the Panel:</p> </li> <li>Select the data source from the Query tab.</li> <li>Craft your metric query using the query editor.</li> <li> <p>Customize the panel's appearance and settings in the Panel and Visualization tabs.</p> </li> <li> <p>Save the Dashboard:</p> </li> <li>Click the Save icon in the top right corner, give your dashboard a name, and click Save.</li> </ol>"},{"location":"operations/monitoring/grafana/basics/#best-practices","title":"Best Practices","text":"<ul> <li>Organize with Folders: Use folders to organize your dashboards by project, data source, or environment.</li> <li>Template Variables: Use variables to create more dynamic and interactive dashboards.</li> <li>Alerts Configuration: Configure alerts to notify you of critical conditions within your data.</li> <li>Regular Backups: Regularly export and back up your dashboard configurations.</li> <li>Security: Ensure Grafana is behind a reverse proxy with SSL termination. Use firewall rules to restrict access to the Grafana server.</li> </ul>"},{"location":"operations/monitoring/grafana/basics/#conclusion","title":"Conclusion","text":"<p>Grafana dashboards are essential for monitoring the performance and health of your systems. By following these best practices for configuration and installation, you'll be able to set up powerful visualizations that help you make informed decisions based on your data.</p> <p>For more advanced configurations and detailed explanations, refer to the Grafana documentation.</p>"},{"location":"operations/monitoring/grafana/prometheus-alertmanager/","title":"Setting Up Prometheus and Alertmanager for Grafana Integration","text":"<p>Integrating Prometheus with Grafana provides a powerful monitoring solution, allowing you to visualize metrics and create alerts based on data collected by Prometheus. Alertmanager further enhances monitoring by managing and routing alerts. This guide covers the best practices for installing and configuring Prometheus, Alertmanager, and their integration with Grafana.</p>"},{"location":"operations/monitoring/grafana/prometheus-alertmanager/#installing-prometheus","title":"Installing Prometheus","text":"<ol> <li> <p>Download and Extract Prometheus:</p> <p>First, download the latest Prometheus version from the official Prometheus downloads page. Choose the version that matches your operating system and architecture.</p> <pre><code>tar xvfz prometheus-*.tar.gz\ncd prometheus-*\n</code></pre> </li> <li> <p>Configure Prometheus:</p> <p>Prometheus configurations are defined in <code>prometheus.yml</code>. A simple configuration to scrape metrics from a single instance might look like this:</p> <pre><code>global:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n</code></pre> <p>Adjust the <code>targets</code> to include the addresses of the systems you want Prometheus to scrape.</p> </li> <li> <p>Start Prometheus:</p> <p>Run Prometheus with your configuration file:</p> <pre><code>./prometheus --config.file=prometheus.yml\n</code></pre> </li> </ol>"},{"location":"operations/monitoring/grafana/prometheus-alertmanager/#installing-alertmanager","title":"Installing Alertmanager","text":"<ol> <li> <p>Download and Extract Alertmanager:</p> <p>Similar to Prometheus, download the latest Alertmanager version from the Alertmanager downloads page and extract it:</p> <pre><code>tar xvfz alertmanager-*.tar.gz\ncd alertmanager-*\n</code></pre> </li> <li> <p>Configure Alertmanager:</p> <p>Alertmanager's configuration is defined in <code>alertmanager.yml</code>. Here's an example configuration that integrates with an email system for alerts:</p> <pre><code>global:\n  smtp_smarthost: 'smtp.example.com:587'\n  smtp_from: 'alertmanager@example.com'\n  smtp_auth_username: 'alertmanager'\n  smtp_auth_password: 'password'\n\nroute:\n  receiver: 'email'\n\nreceivers:\n- name: 'email'\n  email_configs:\n  - to: 'ops-team@example.com'\n</code></pre> <p>This configuration should be adapted to fit your alerting needs and environment.</p> </li> <li> <p>Start Alertmanager:</p> <p>Run Alertmanager with your configuration file:</p> <pre><code>./alertmanager --config.file=alertmanager.yml\n</code></pre> </li> </ol>"},{"location":"operations/monitoring/grafana/prometheus-alertmanager/#integrating-prometheus-with-grafana","title":"Integrating Prometheus with Grafana","text":"<ol> <li> <p>Add Prometheus as a Data Source in Grafana:</p> <ul> <li>Navigate to Grafana's dashboard.</li> <li>Go to Configuration &gt; Data Sources.</li> <li>Click Add data source, and select Prometheus.</li> <li>In the HTTP settings, set the URL to the address of your Prometheus server (e.g., <code>http://localhost:9090</code>).</li> <li>Save and test the data source to ensure Grafana can connect to Prometheus.</li> </ul> </li> <li> <p>Creating Dashboards:</p> <p>With Prometheus configured as a data source, you can now create dashboards in Grafana to visualize your metrics. Grafana offers a wide range of visualization options, from simple graphs to complex histograms and heatmaps.</p> <ul> <li>Click + &gt; Dashboard and use the graphical editor to create queries and visualize the data from Prometheus.</li> </ul> </li> <li> <p>Setting Up Alerts:</p> <p>Grafana can use Prometheus as an alert source. Define alert rules within Grafana dashboards by setting thresholds and conditions based on Prometheus metrics.</p> <ul> <li>In the dashboard panel, click the Alert tab, and configure your alert conditions.</li> <li>Alerts can notify you via email, Slack, and other supported notification channels when conditions are met.</li> </ul> </li> </ol>"},{"location":"operations/monitoring/grafana/prometheus-alertmanager/#best-practices","title":"Best Practices","text":"<ul> <li>Security: Ensure your Prometheus and Alertmanager instances are secured, especially if they're exposed to the internet. Consider using reverse proxies, firewalls, and secure access mechanisms (like basic auth or OAuth).</li> <li>Scalability: Plan your Prometheus storage and retention policies according to your data volume. Prometheus can handle millions of metrics, but proper storage planning is crucial.</li> <li>Reliability: Use redundant Alertmanager instances to ensure high availability of your alerting system. Prometheus supports configuring multiple Alertmanager instances for failover purposes.</li> </ul> <p>By following these steps and best practices, you'll have a robust monitoring and alerting setup integrated with Grafana, providing deep insights into your system's health and performance.</p>"},{"location":"operations/virtualization/","title":"Index","text":"<p>The virtualization section primarily focuses on Proxmox, a comprehensive server management platform that enables the deployment and management of virtual machines (VMs) and containers. Proxmox offers an integrated solution for managing virtualized technologies using a robust and user-friendly web interface. This section delves into the nuances of setting up, configuring, and optimizing Proxmox environments, providing insights into leveraging its full potential for efficient virtual infrastructure management. </p> <p>Warning</p> <p>More informations: https://www.proxmox.com/en/ !!!</p>"},{"location":"operations/virtualization/VMs/proxmox/","title":"Proxmox VMs","text":"<p>Proxmox Virtual Environment is a complete, open-source server management platform for enterprise virtualization. It tightly integrates the KVM hypervisor and Linux Containers (LXC), software-defined storage and networking functionality, on a single platform. With the integrated web-based user interface you can manage VMs and containers, high availability for clusters, or the integrated disaster recovery tools with ease.</p> <p>VMs configurations on 3 nodes Proxmox Cluster</p> CitadelHelixNexus"},{"location":"operations/virtualization/VMs/proxmox/#k3s-01","title":"K3S-01","text":"<p><pre><code>root@citadel:/home/merox# cat /etc/pve/nodes/citadel/qemu-server/304.conf\n</code></pre> <pre><code>balloon: 0\nboot: c\nbootdisk: scsi0\ncipassword: $5$jkhUIiuewfe79877Ebvfeuewffew32dLrj1\nciupgrade: 0\nciuser: merox\ncores: 2\ncpu: host\nide2: Storage:vm-304-cloudinit,media=cdrom,size=4M\nipconfig0: ip=dhcp\nmemory: 16384\nmeta: creation-qemu=8.1.2,ctime=1706184272\nname: k3s-01\nnet0: virtio=FB:10:22:1C:2E:0B,bridge=vmbr0\nnuma: 0\nonboot: 1\nscsi0: Storage:vm-304-disk-0,size=128204M,ssd=1\nscsihw: virtio-scsi-pci\nserial0: socket\nsmbios1: uuid=2eee58c6-6212-47f4-b7a5-73143cf5a6cf\nsockets: 2\nsshkeys: ssh-rsa%\nvga: serial0\nvmgenid: b09d281c-3ee0-44dc-bc43-5b1afeba8a83\n</code></pre></p>"},{"location":"operations/virtualization/VMs/proxmox/#k3s-02","title":"K3S-02","text":"<p><pre><code>root@helix:~# cat /etc/pve/nodes/helix/qemu-server/305.conf \n</code></pre> <pre><code>balloon: 0\nboot: c\nbootdisk: scsi0\ncipassword: $2ATZCNhUIiuewfe79877Ebvfeuewffew32dLrj14jvg5WdLrj1\nciupgrade: 0\nciuser: merox\ncores: 2\ncpu: host\nide2: Storage:vm-305-cloudinit,media=cdrom,size=4M\nipconfig0: ip=dhcp\nmemory: 16384\nmeta: creation-qemu=8.1.2,ctime=1706184272\nname: k3s-02\nnet0: virtio=FE:21:11:35:72:3A,bridge=vmbr0\nnuma: 0\nonboot: 1\nscsi0: Storage:vm-305-disk-0,size=128204M,ssd=1\nscsihw: virtio-scsi-pci\nserial0: socket\nsmbios1: uuid=4a38edc1-641d-4c9a-bc50-d1b236fe6de0\nsockets: 2\nsshkeys: ssh-rsa%\nvga: serial0\nvmgenid: a643e4a7-4d57-4655-b0ab-d151991d724e\n</code></pre></p>"},{"location":"operations/virtualization/VMs/proxmox/#k3s-03","title":"K3S-03","text":"<p><pre><code>root@nexus:~# cat /etc/pve/nodes/nexus/qemu-server/306.conf \n</code></pre> <pre><code>balloon: 0\nboot: c\nbootdisk: scsi0\ncipassword: $2$ATjfo389y932hWDbjdwqNSNML8izsrq9I4jvg5WdLrj1\nciupgrade: 0\nciuser: merox\ncores: 2\ncpu: host\nide2: Storage:vm-306-cloudinit,media=cdrom,size=4M\nipconfig0: ip=dhcp\nmemory: 14336\nmeta: creation-qemu=8.1.2,ctime=1706184272\nname: k3s-03\nnet0: virtio=DF:14:12:DF:42:DA,bridge=vmbr0\nnuma: 0\nonboot: 1\nscsi0: Storage:vm-306-disk-0,size=128204M,ssd=1\nscsihw: virtio-scsi-pci\nserial0: socket\nsmbios1: uuid=1319e437-1cc7-4444-9d14-2444d0953a29\nsockets: 2\nsshkeys: ssh-rsa%\nvga: serial0\nvmgenid: 6551cc5e-630c-40d3-ad9e-c5a622b71711\n</code></pre></p>"},{"location":"operations/virtualization/VMs/proxmox/#windows-server-2019-addns","title":"Windows Server 2019 ( AD/DNS )","text":"<pre><code>root@nexus:~# cat /etc/pve/nodes/nexus/qemu-server/105.conf \n</code></pre> <pre><code>boot: order=ide0;net0\ncores: 2\ncpu: host\nide0: Storage:vm-105-disk-1,format=raw,size=32G\nmachine: pc-i440fx-7.2\nmemory: 4096\nmeta: creation-qemu=7.2.0,ctime=1687109663\nname: winserver\nnet0: e1000=D6:46:16:2C:CD:8C,bridge=vmbr0\nnuma: 0\nonboot: 1\nostype: win10\nscsihw: virtio-scsi-single\nsmbios1: uuid=3a978c5c-e1d0-482a-b12f-8fa3a5e14ce1\nsockets: 2\nstartup: order=1\ntags: windows\nunused0: Storage:vm-105-disk-0\nvmgenid: 14c4f1ad-59fb-4a73-981a-b97e89fab435\n</code></pre>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/kubernetes/","title":"kubernetes","text":""},{"location":"blog/category/installation/","title":"installation","text":""},{"location":"blog/category/networking/","title":"networking","text":""}]}